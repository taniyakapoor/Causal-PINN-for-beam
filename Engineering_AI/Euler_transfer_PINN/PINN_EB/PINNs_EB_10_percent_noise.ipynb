{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a6071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import pi\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69275903",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.tensor(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd5d8b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "# Define the exact solution\n",
    "def exact_solution(x, t):\n",
    "    return torch.sin(x)*torch.cos(pi*t)\n",
    "\n",
    "def initial_condition(x):\n",
    "    return torch.sin(x)*(1 + sigma*torch.randn(x.shape)).to(device)\n",
    "\n",
    "def initial_condition_t(x):\n",
    "    return 0*torch.cos(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d9104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning number of points\n",
    "initial_pts = 500\n",
    "left_boundary_pts = 500\n",
    "right_boundary_pts = 500\n",
    "residual_pts = 10000\n",
    "\n",
    "# Type of optimizer (ADAM or LBFGS)\n",
    "opt_type = \"LBFGS\"\n",
    "\n",
    "x_init = 8*pi*torch.rand((initial_pts,1)) # initial pts\n",
    "t_init = 0*x_init\n",
    "init = torch.cat([x_init, t_init],1).to(device)\n",
    "u_init = initial_condition(init[:,0]).reshape(-1, 1)\n",
    "u_init_t = 0*initial_condition(init[:,0]).reshape(-1, 1)\n",
    "\n",
    "xb_left = torch.zeros((left_boundary_pts, 1)) # left spatial boundary\n",
    "tb_left = torch.rand((left_boundary_pts, 1)) #\n",
    "b_left = torch.cat([xb_left, tb_left ],1).to(device)\n",
    "u_b_l = 0*torch.sin(tb_left)\n",
    "\n",
    "xb_right = 8*pi*torch.ones((right_boundary_pts, 1)) # right spatial boundary\n",
    "tb_right = torch.rand((right_boundary_pts, 1)) # right boundary pts\n",
    "b_right = torch.cat([xb_right, tb_right ],1).to(device)\n",
    "u_b_r = 0*torch.sin(2*pi - tb_right)\n",
    "\n",
    "x_interior = 8*pi*torch.rand((residual_pts, 1))\n",
    "t_interior = torch.rand((residual_pts, 1))\n",
    "interior = torch.cat([x_interior, t_interior],1).to(device)\n",
    "\n",
    "training_set = DataLoader(torch.utils.data.TensorDataset(init.to(device), u_init.to(device), u_init_t.to(device), b_left.to(device),  b_right.to(device), u_b_l.to(device), u_b_r.to(device)), batch_size=500, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b9872a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e5f5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension, output_dimension, n_hidden_layers, neurons):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer\n",
    "        self.neurons = neurons\n",
    "        # Number of hidden layers\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        # Activation function\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.neurons, self.neurons) for _ in range(n_hidden_layers)])\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear transformations defining the network\n",
    "        # (see equation above)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x = self.activation(l(x))\n",
    "        return self.output_layer(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f73441b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "my_network = NeuralNet(input_dimension = init.shape[1], output_dimension = u_init.shape[1], n_hidden_layers=4, neurons=200)\n",
    "model_state_dict = torch.load('PINN_EB.pth', map_location=torch.device('cpu'))\n",
    "my_network = my_network.to(device)\n",
    "\n",
    "# def init_xavier(model, retrain_seed):\n",
    "#     torch.manual_seed(retrain_seed)\n",
    "#     def init_weights(m):\n",
    "#         if type(m) == nn.Linear and m.weight.requires_grad and m.bias.requires_grad:\n",
    "#             g = nn.init.calculate_gain('tanh')\n",
    "#             torch.nn.init.xavier_uniform_(m.weight, gain=g)\n",
    "#             #torch.nn.init.xavier_normal_(m.weight, gain=g)\n",
    "#             m.bias.data.fill_(0)\n",
    "#     model.apply(init_weights)\n",
    "\n",
    "# # Random Seed for weight initialization\n",
    "# retrain = 128\n",
    "# # Xavier weight initialization\n",
    "# init_xavier(my_network, retrain)\n",
    "\n",
    "if opt_type == \"ADAM\":\n",
    "    optimizer_ = optim.Adam(my_network.parameters(), lr=0.001)\n",
    "elif opt_type == \"LBFGS\":\n",
    "    optimizer_ = optim.LBFGS(my_network.parameters(), lr=0.1, max_iter=1, max_eval=50000, tolerance_change=1.0 * np.finfo(float).eps)\n",
    "else:\n",
    "    raise ValueError(\"Optimizer not recognized\")\n",
    "\n",
    "\n",
    "def fit(model, training_set, interior, num_epochs, optimizer, p, verbose=True):\n",
    "    history = list()\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "\n",
    "        running_loss = list([0])\n",
    "\n",
    "        # Loop over batches\n",
    "        for j, (initial, u_initial, u_initial_t, bd_left, bd_right, ubl, ubr) in enumerate(training_set):\n",
    "            def closure():\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # for initial\n",
    "                initial.requires_grad = True\n",
    "                u_initial_pred_ = model(initial)\n",
    "                inputs = torch.ones(initial_pts, 1).to(device)\n",
    "                grad_u_init = torch.autograd.grad(u_initial_pred_, initial, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_init_t = grad_u_init[:, 1].reshape(-1, )\n",
    "\n",
    "                # for left boundary\n",
    "                bd_left.requires_grad = True\n",
    "                bd_left_pred_ = model(bd_left)\n",
    "                inputs = torch.ones(left_boundary_pts, 1).to(device)\n",
    "                grad_bd_left = torch.autograd.grad(bd_left_pred_, bd_left, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_x_left = grad_bd_left[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(left_boundary_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_bd_x_left = torch.autograd.grad(u_bd_x_left, bd_left, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_xx_left = grad_u_bd_x_left[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(left_boundary_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_bd_xx_left = torch.autograd.grad(u_bd_xx_left, bd_left, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_xxx_left = grad_u_bd_xx_left[:, 0].reshape(-1, )\n",
    "\n",
    "                # for right boundary\n",
    "                bd_right.requires_grad = True\n",
    "                bd_right_pred_ = model(bd_right)\n",
    "                inputs = torch.ones(right_boundary_pts, 1).to(device)\n",
    "                grad_bd_right = torch.autograd.grad(bd_right_pred_, bd_right, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_x_right = grad_bd_right[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(right_boundary_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_bd_x_right = torch.autograd.grad(u_bd_x_right, bd_right, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_xx_right = grad_u_bd_x_right[:, 0].reshape(-1, )\n",
    "\n",
    "                # residual calculation\n",
    "                interior.requires_grad = True\n",
    "                u_hat = model(interior)\n",
    "                inputs = torch.ones(residual_pts, 1).to(device)\n",
    "                grad_u_hat = torch.autograd.grad(u_hat, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "\n",
    "                u_x = grad_u_hat[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_x = torch.autograd.grad(u_x, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_xx = grad_u_x[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_xx = torch.autograd.grad(u_xx, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_xxx = grad_u_xx[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_xxx = torch.autograd.grad(u_xxx, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_xxxx = grad_u_xxx[:, 0].reshape(-1, )\n",
    "\n",
    "                u_t = grad_u_hat[:, 1]\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_t = torch.autograd.grad(u_t, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_tt = grad_u_t[:, 1].reshape(-1, )\n",
    "\n",
    "                # Item 1. below\n",
    "\n",
    "                loss_ic = torch.mean((u_initial_pred_.reshape(-1, ) - u_initial.reshape(-1, )) ** p) + \\\n",
    "                          torch.mean((u_init_t.reshape(-1, )) ** p)\n",
    "                loss_pde = torch.mean((u_tt.reshape(-1, ) + u_xxxx.reshape(-1, ) - (2-pi**2)*torch.sin(interior[:,0])*torch.cos(pi*interior[:,1])) ** p)\n",
    "                loss_left_b = torch.mean((u_bd_x_left.reshape(-1, ) - ubl.reshape(-1, )) ** p) + \\\n",
    "                              torch.mean((u_bd_xx_left.reshape(-1, ) - ubl.reshape(-1, )) ** p)\n",
    "                loss_right_b = torch.mean((bd_right_pred_.reshape(-1, ) - ubr.reshape(-1, )) ** p) + \\\n",
    "                               torch.mean((u_bd_xx_right.reshape(-1, ) - ubr.reshape(-1, )) ** p)\n",
    "\n",
    "                loss = loss_ic + loss_pde + loss_left_b + loss_right_b\n",
    "\n",
    "                # Item 2. below\n",
    "                loss.backward()\n",
    "                # Compute average training loss over batches for the current epoch\n",
    "                running_loss[0] += loss.item()\n",
    "                return loss\n",
    "\n",
    "            # Item 3. below\n",
    "            optimizer.step(closure=closure)\n",
    "\n",
    "        print('Loss: ', (running_loss[0] / len(training_set)))\n",
    "        history.append(running_loss[0])\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a242aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################  0  ################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/localhome/tkapoor/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:234: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  16.15862274169922\n",
      "################################  1  ################################\n",
      "Loss:  16.158414840698242\n",
      "################################  2  ################################\n",
      "Loss:  16.157577514648438\n",
      "################################  3  ################################\n",
      "Loss:  16.156471252441406\n",
      "################################  4  ################################\n",
      "Loss:  16.15506935119629\n",
      "################################  5  ################################\n",
      "Loss:  16.153337478637695\n",
      "################################  6  ################################\n",
      "Loss:  16.151254653930664\n",
      "################################  7  ################################\n",
      "Loss:  16.148792266845703\n",
      "################################  8  ################################\n",
      "Loss:  16.145896911621094\n",
      "################################  9  ################################\n",
      "Loss:  16.142431259155273\n",
      "################################  10  ################################\n",
      "Loss:  16.137996673583984\n",
      "################################  11  ################################\n",
      "Loss:  16.13143539428711\n",
      "################################  12  ################################\n",
      "Loss:  16.11846923828125\n",
      "################################  13  ################################\n",
      "Loss:  16.100080490112305\n",
      "################################  14  ################################\n",
      "Loss:  16.099651336669922\n",
      "################################  15  ################################\n",
      "Loss:  16.092086791992188\n",
      "################################  16  ################################\n",
      "Loss:  16.08602523803711\n",
      "################################  17  ################################\n",
      "Loss:  16.08147621154785\n",
      "################################  18  ################################\n",
      "Loss:  16.076648712158203\n",
      "################################  19  ################################\n",
      "Loss:  16.069713592529297\n",
      "################################  20  ################################\n",
      "Loss:  16.060699462890625\n",
      "################################  21  ################################\n",
      "Loss:  16.050451278686523\n",
      "################################  22  ################################\n",
      "Loss:  16.04014015197754\n",
      "################################  23  ################################\n",
      "Loss:  16.029233932495117\n",
      "################################  24  ################################\n",
      "Loss:  16.0172061920166\n",
      "################################  25  ################################\n",
      "Loss:  16.004592895507812\n",
      "################################  26  ################################\n",
      "Loss:  15.991780281066895\n",
      "################################  27  ################################\n",
      "Loss:  15.981147766113281\n",
      "################################  28  ################################\n",
      "Loss:  15.965163230895996\n",
      "################################  29  ################################\n",
      "Loss:  15.943650245666504\n",
      "################################  30  ################################\n",
      "Loss:  15.921561241149902\n",
      "################################  31  ################################\n",
      "Loss:  15.896472930908203\n",
      "################################  32  ################################\n",
      "Loss:  15.86220645904541\n",
      "################################  33  ################################\n",
      "Loss:  15.817967414855957\n",
      "################################  34  ################################\n",
      "Loss:  15.762191772460938\n",
      "################################  35  ################################\n",
      "Loss:  15.697633743286133\n",
      "################################  36  ################################\n",
      "Loss:  15.612092971801758\n",
      "################################  37  ################################\n",
      "Loss:  15.519575119018555\n",
      "################################  38  ################################\n",
      "Loss:  15.409197807312012\n",
      "################################  39  ################################\n",
      "Loss:  15.321956634521484\n",
      "################################  40  ################################\n",
      "Loss:  15.24479866027832\n",
      "################################  41  ################################\n",
      "Loss:  15.170341491699219\n",
      "################################  42  ################################\n",
      "Loss:  15.085299491882324\n",
      "################################  43  ################################\n",
      "Loss:  14.999336242675781\n",
      "################################  44  ################################\n",
      "Loss:  14.939020156860352\n",
      "################################  45  ################################\n",
      "Loss:  14.888522148132324\n",
      "################################  46  ################################\n",
      "Loss:  14.82548713684082\n",
      "################################  47  ################################\n",
      "Loss:  14.756327629089355\n",
      "################################  48  ################################\n",
      "Loss:  14.685262680053711\n",
      "################################  49  ################################\n",
      "Loss:  14.607250213623047\n",
      "################################  50  ################################\n",
      "Loss:  14.531052589416504\n",
      "################################  51  ################################\n",
      "Loss:  14.457147598266602\n",
      "################################  52  ################################\n",
      "Loss:  14.385132789611816\n",
      "################################  53  ################################\n",
      "Loss:  14.315760612487793\n",
      "################################  54  ################################\n",
      "Loss:  14.248343467712402\n",
      "################################  55  ################################\n",
      "Loss:  14.187332153320312\n",
      "################################  56  ################################\n",
      "Loss:  14.127333641052246\n",
      "################################  57  ################################\n",
      "Loss:  14.063881874084473\n",
      "################################  58  ################################\n",
      "Loss:  14.004510879516602\n",
      "################################  59  ################################\n",
      "Loss:  13.94989013671875\n",
      "################################  60  ################################\n",
      "Loss:  13.897201538085938\n",
      "################################  61  ################################\n",
      "Loss:  13.843570709228516\n",
      "################################  62  ################################\n",
      "Loss:  13.786052703857422\n",
      "################################  63  ################################\n",
      "Loss:  13.730389595031738\n",
      "################################  64  ################################\n",
      "Loss:  13.675150871276855\n",
      "################################  65  ################################\n",
      "Loss:  13.61577033996582\n",
      "################################  66  ################################\n",
      "Loss:  13.55545711517334\n",
      "################################  67  ################################\n",
      "Loss:  13.491291999816895\n",
      "################################  68  ################################\n",
      "Loss:  13.42507266998291\n",
      "################################  69  ################################\n",
      "Loss:  13.355010986328125\n",
      "################################  70  ################################\n",
      "Loss:  13.282750129699707\n",
      "################################  71  ################################\n",
      "Loss:  13.213988304138184\n",
      "################################  72  ################################\n",
      "Loss:  13.150660514831543\n",
      "################################  73  ################################\n",
      "Loss:  13.09840202331543\n",
      "################################  74  ################################\n",
      "Loss:  13.050460815429688\n",
      "################################  75  ################################\n",
      "Loss:  13.007152557373047\n",
      "################################  76  ################################\n",
      "Loss:  12.96552848815918\n",
      "################################  77  ################################\n",
      "Loss:  12.918184280395508\n",
      "################################  78  ################################\n",
      "Loss:  12.881959915161133\n",
      "################################  79  ################################\n",
      "Loss:  12.846696853637695\n",
      "################################  80  ################################\n",
      "Loss:  12.80876636505127\n",
      "################################  81  ################################\n",
      "Loss:  12.775762557983398\n",
      "################################  82  ################################\n",
      "Loss:  12.742918968200684\n",
      "################################  83  ################################\n",
      "Loss:  12.709049224853516\n",
      "################################  84  ################################\n",
      "Loss:  12.672094345092773\n",
      "################################  85  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  12.63329792022705\n",
      "################################  86  ################################\n",
      "Loss:  12.59334659576416\n",
      "################################  87  ################################\n",
      "Loss:  12.552593231201172\n",
      "################################  88  ################################\n",
      "Loss:  12.511739730834961\n",
      "################################  89  ################################\n",
      "Loss:  12.474047660827637\n",
      "################################  90  ################################\n",
      "Loss:  12.438776016235352\n",
      "################################  91  ################################\n",
      "Loss:  12.406279563903809\n",
      "################################  92  ################################\n",
      "Loss:  12.37635326385498\n",
      "################################  93  ################################\n",
      "Loss:  12.348550796508789\n",
      "################################  94  ################################\n",
      "Loss:  12.321192741394043\n",
      "################################  95  ################################\n",
      "Loss:  12.294119834899902\n",
      "################################  96  ################################\n",
      "Loss:  12.266490936279297\n",
      "################################  97  ################################\n",
      "Loss:  12.239974975585938\n",
      "################################  98  ################################\n",
      "Loss:  12.21389389038086\n",
      "################################  99  ################################\n",
      "Loss:  12.18773078918457\n",
      "################################  100  ################################\n",
      "Loss:  12.161032676696777\n",
      "################################  101  ################################\n",
      "Loss:  12.133777618408203\n",
      "################################  102  ################################\n",
      "Loss:  12.106531143188477\n",
      "################################  103  ################################\n",
      "Loss:  12.080122947692871\n",
      "################################  104  ################################\n",
      "Loss:  12.05482292175293\n",
      "################################  105  ################################\n",
      "Loss:  12.03237533569336\n",
      "################################  106  ################################\n",
      "Loss:  12.011245727539062\n",
      "################################  107  ################################\n",
      "Loss:  11.99005126953125\n",
      "################################  108  ################################\n",
      "Loss:  11.969268798828125\n",
      "################################  109  ################################\n",
      "Loss:  11.949572563171387\n",
      "################################  110  ################################\n",
      "Loss:  11.931095123291016\n",
      "################################  111  ################################\n",
      "Loss:  11.911115646362305\n",
      "################################  112  ################################\n",
      "Loss:  11.890070915222168\n",
      "################################  113  ################################\n",
      "Loss:  11.866378784179688\n",
      "################################  114  ################################\n",
      "Loss:  11.839020729064941\n",
      "################################  115  ################################\n",
      "Loss:  11.808762550354004\n",
      "################################  116  ################################\n",
      "Loss:  11.778764724731445\n",
      "################################  117  ################################\n",
      "Loss:  11.74935531616211\n",
      "################################  118  ################################\n",
      "Loss:  11.722594261169434\n",
      "################################  119  ################################\n",
      "Loss:  11.697982788085938\n",
      "################################  120  ################################\n",
      "Loss:  11.672496795654297\n",
      "################################  121  ################################\n",
      "Loss:  11.646575927734375\n",
      "################################  122  ################################\n",
      "Loss:  11.62772274017334\n",
      "################################  123  ################################\n",
      "Loss:  11.608796119689941\n",
      "################################  124  ################################\n",
      "Loss:  11.588915824890137\n",
      "################################  125  ################################\n",
      "Loss:  11.565197944641113\n",
      "################################  126  ################################\n",
      "Loss:  11.537698745727539\n",
      "################################  127  ################################\n",
      "Loss:  11.504741668701172\n",
      "################################  128  ################################\n",
      "Loss:  11.478778839111328\n",
      "################################  129  ################################\n",
      "Loss:  11.45522689819336\n",
      "################################  130  ################################\n",
      "Loss:  11.430543899536133\n",
      "################################  131  ################################\n",
      "Loss:  11.399904251098633\n",
      "################################  132  ################################\n",
      "Loss:  11.362982749938965\n",
      "################################  133  ################################\n",
      "Loss:  11.337495803833008\n",
      "################################  134  ################################\n",
      "Loss:  11.309551239013672\n",
      "################################  135  ################################\n",
      "Loss:  11.282868385314941\n",
      "################################  136  ################################\n",
      "Loss:  11.258201599121094\n",
      "################################  137  ################################\n",
      "Loss:  11.23469066619873\n",
      "################################  138  ################################\n",
      "Loss:  11.214493751525879\n",
      "################################  139  ################################\n",
      "Loss:  11.194705963134766\n",
      "################################  140  ################################\n",
      "Loss:  11.177701950073242\n",
      "################################  141  ################################\n",
      "Loss:  11.16026782989502\n",
      "################################  142  ################################\n",
      "Loss:  11.144207954406738\n",
      "################################  143  ################################\n",
      "Loss:  11.1271390914917\n",
      "################################  144  ################################\n",
      "Loss:  11.108452796936035\n",
      "################################  145  ################################\n",
      "Loss:  11.089316368103027\n",
      "################################  146  ################################\n",
      "Loss:  11.069148063659668\n",
      "################################  147  ################################\n",
      "Loss:  11.049042701721191\n",
      "################################  148  ################################\n",
      "Loss:  11.028632164001465\n",
      "################################  149  ################################\n",
      "Loss:  11.008299827575684\n",
      "################################  150  ################################\n",
      "Loss:  10.985288619995117\n",
      "################################  151  ################################\n",
      "Loss:  10.963004112243652\n",
      "################################  152  ################################\n",
      "Loss:  10.94111442565918\n",
      "################################  153  ################################\n",
      "Loss:  10.918081283569336\n",
      "################################  154  ################################\n",
      "Loss:  10.894295692443848\n",
      "################################  155  ################################\n",
      "Loss:  10.870129585266113\n",
      "################################  156  ################################\n",
      "Loss:  10.849514961242676\n",
      "################################  157  ################################\n",
      "Loss:  10.832512855529785\n",
      "################################  158  ################################\n",
      "Loss:  10.818948745727539\n",
      "################################  159  ################################\n",
      "Loss:  10.802794456481934\n",
      "################################  160  ################################\n",
      "Loss:  10.783869743347168\n",
      "################################  161  ################################\n",
      "Loss:  10.764573097229004\n",
      "################################  162  ################################\n",
      "Loss:  10.743866920471191\n",
      "################################  163  ################################\n",
      "Loss:  10.721498489379883\n",
      "################################  164  ################################\n",
      "Loss:  10.693595886230469\n",
      "################################  165  ################################\n",
      "Loss:  10.660059928894043\n",
      "################################  166  ################################\n",
      "Loss:  10.619763374328613\n",
      "################################  167  ################################\n",
      "Loss:  10.58031940460205\n",
      "################################  168  ################################\n",
      "Loss:  10.541108131408691\n",
      "################################  169  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  10.510775566101074\n",
      "################################  170  ################################\n",
      "Loss:  10.49143123626709\n",
      "################################  171  ################################\n",
      "Loss:  10.477712631225586\n",
      "################################  172  ################################\n",
      "Loss:  10.464714050292969\n",
      "################################  173  ################################\n",
      "Loss:  10.45249080657959\n",
      "################################  174  ################################\n",
      "Loss:  10.440155982971191\n",
      "################################  175  ################################\n",
      "Loss:  10.428500175476074\n",
      "################################  176  ################################\n",
      "Loss:  10.416438102722168\n",
      "################################  177  ################################\n",
      "Loss:  10.403836250305176\n",
      "################################  178  ################################\n",
      "Loss:  10.390334129333496\n",
      "################################  179  ################################\n",
      "Loss:  10.375744819641113\n",
      "################################  180  ################################\n",
      "Loss:  10.361689567565918\n",
      "################################  181  ################################\n",
      "Loss:  10.347830772399902\n",
      "################################  182  ################################\n",
      "Loss:  10.334851264953613\n",
      "################################  183  ################################\n",
      "Loss:  10.321823120117188\n",
      "################################  184  ################################\n",
      "Loss:  10.308464050292969\n",
      "################################  185  ################################\n",
      "Loss:  10.295663833618164\n",
      "################################  186  ################################\n",
      "Loss:  10.284908294677734\n",
      "################################  187  ################################\n",
      "Loss:  10.273775100708008\n",
      "################################  188  ################################\n",
      "Loss:  10.263875961303711\n",
      "################################  189  ################################\n",
      "Loss:  10.254383087158203\n",
      "################################  190  ################################\n",
      "Loss:  10.24540901184082\n",
      "################################  191  ################################\n",
      "Loss:  10.236047744750977\n",
      "################################  192  ################################\n",
      "Loss:  10.226320266723633\n",
      "################################  193  ################################\n",
      "Loss:  10.215450286865234\n",
      "################################  194  ################################\n",
      "Loss:  10.204692840576172\n",
      "################################  195  ################################\n",
      "Loss:  10.19424819946289\n",
      "################################  196  ################################\n",
      "Loss:  10.183547973632812\n",
      "################################  197  ################################\n",
      "Loss:  10.172046661376953\n",
      "################################  198  ################################\n",
      "Loss:  10.159850120544434\n",
      "################################  199  ################################\n",
      "Loss:  10.14666748046875\n",
      "################################  200  ################################\n",
      "Loss:  10.132311820983887\n",
      "################################  201  ################################\n",
      "Loss:  10.117063522338867\n",
      "################################  202  ################################\n",
      "Loss:  10.100438117980957\n",
      "################################  203  ################################\n",
      "Loss:  10.085906028747559\n",
      "################################  204  ################################\n",
      "Loss:  10.070560455322266\n",
      "################################  205  ################################\n",
      "Loss:  10.055800437927246\n",
      "################################  206  ################################\n",
      "Loss:  10.038829803466797\n",
      "################################  207  ################################\n",
      "Loss:  10.023571014404297\n",
      "################################  208  ################################\n",
      "Loss:  10.008776664733887\n",
      "################################  209  ################################\n",
      "Loss:  9.994765281677246\n",
      "################################  210  ################################\n",
      "Loss:  9.982929229736328\n",
      "################################  211  ################################\n",
      "Loss:  9.971673965454102\n",
      "################################  212  ################################\n",
      "Loss:  9.961024284362793\n",
      "################################  213  ################################\n",
      "Loss:  9.94936752319336\n",
      "################################  214  ################################\n",
      "Loss:  9.9379243850708\n",
      "################################  215  ################################\n",
      "Loss:  9.926077842712402\n",
      "################################  216  ################################\n",
      "Loss:  9.91505241394043\n",
      "################################  217  ################################\n",
      "Loss:  9.903711318969727\n",
      "################################  218  ################################\n",
      "Loss:  9.892953872680664\n",
      "################################  219  ################################\n",
      "Loss:  9.882098197937012\n",
      "################################  220  ################################\n",
      "Loss:  9.871776580810547\n",
      "################################  221  ################################\n",
      "Loss:  9.861428260803223\n",
      "################################  222  ################################\n",
      "Loss:  9.851144790649414\n",
      "################################  223  ################################\n",
      "Loss:  9.840066909790039\n",
      "################################  224  ################################\n",
      "Loss:  9.82798957824707\n",
      "################################  225  ################################\n",
      "Loss:  9.815177917480469\n",
      "################################  226  ################################\n",
      "Loss:  9.801548957824707\n",
      "################################  227  ################################\n",
      "Loss:  9.787616729736328\n",
      "################################  228  ################################\n",
      "Loss:  9.77152156829834\n",
      "################################  229  ################################\n",
      "Loss:  9.755949974060059\n",
      "################################  230  ################################\n",
      "Loss:  9.739398002624512\n",
      "################################  231  ################################\n",
      "Loss:  9.727728843688965\n",
      "################################  232  ################################\n",
      "Loss:  9.716663360595703\n",
      "################################  233  ################################\n",
      "Loss:  9.705209732055664\n",
      "################################  234  ################################\n",
      "Loss:  9.689836502075195\n",
      "################################  235  ################################\n",
      "Loss:  9.676326751708984\n",
      "################################  236  ################################\n",
      "Loss:  9.663900375366211\n",
      "################################  237  ################################\n",
      "Loss:  9.649885177612305\n",
      "################################  238  ################################\n",
      "Loss:  9.63509750366211\n",
      "################################  239  ################################\n",
      "Loss:  9.620138168334961\n",
      "################################  240  ################################\n",
      "Loss:  9.6024808883667\n",
      "################################  241  ################################\n",
      "Loss:  9.585887908935547\n",
      "################################  242  ################################\n",
      "Loss:  9.569226264953613\n",
      "################################  243  ################################\n",
      "Loss:  9.553624153137207\n",
      "################################  244  ################################\n",
      "Loss:  9.538071632385254\n",
      "################################  245  ################################\n",
      "Loss:  9.522350311279297\n",
      "################################  246  ################################\n",
      "Loss:  9.505697250366211\n",
      "################################  247  ################################\n",
      "Loss:  9.488004684448242\n",
      "################################  248  ################################\n",
      "Loss:  9.470483779907227\n",
      "################################  249  ################################\n",
      "Loss:  9.45572566986084\n",
      "################################  250  ################################\n",
      "Loss:  9.440938949584961\n",
      "################################  251  ################################\n",
      "Loss:  9.42510986328125\n",
      "################################  252  ################################\n",
      "Loss:  9.409253120422363\n",
      "################################  253  ################################\n",
      "Loss:  9.396116256713867\n",
      "################################  254  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  9.386110305786133\n",
      "################################  255  ################################\n",
      "Loss:  9.376408576965332\n",
      "################################  256  ################################\n",
      "Loss:  9.366679191589355\n",
      "################################  257  ################################\n",
      "Loss:  9.357331275939941\n",
      "################################  258  ################################\n",
      "Loss:  9.348204612731934\n",
      "################################  259  ################################\n",
      "Loss:  9.339666366577148\n",
      "################################  260  ################################\n",
      "Loss:  9.331358909606934\n",
      "################################  261  ################################\n",
      "Loss:  9.323471069335938\n",
      "################################  262  ################################\n",
      "Loss:  9.315692901611328\n",
      "################################  263  ################################\n",
      "Loss:  9.30806827545166\n",
      "################################  264  ################################\n",
      "Loss:  9.300535202026367\n",
      "################################  265  ################################\n",
      "Loss:  9.292882919311523\n",
      "################################  266  ################################\n",
      "Loss:  9.284989356994629\n",
      "################################  267  ################################\n",
      "Loss:  9.276243209838867\n",
      "################################  268  ################################\n",
      "Loss:  9.266342163085938\n",
      "################################  269  ################################\n",
      "Loss:  9.25406551361084\n",
      "################################  270  ################################\n",
      "Loss:  9.241793632507324\n",
      "################################  271  ################################\n",
      "Loss:  9.230264663696289\n",
      "################################  272  ################################\n",
      "Loss:  9.2189359664917\n",
      "################################  273  ################################\n",
      "Loss:  9.20796012878418\n",
      "################################  274  ################################\n",
      "Loss:  9.196945190429688\n",
      "################################  275  ################################\n",
      "Loss:  9.186217308044434\n",
      "################################  276  ################################\n",
      "Loss:  9.17655086517334\n",
      "################################  277  ################################\n",
      "Loss:  9.168306350708008\n",
      "################################  278  ################################\n",
      "Loss:  9.161595344543457\n",
      "################################  279  ################################\n",
      "Loss:  9.155226707458496\n",
      "################################  280  ################################\n",
      "Loss:  9.149439811706543\n",
      "################################  281  ################################\n",
      "Loss:  9.143841743469238\n",
      "################################  282  ################################\n",
      "Loss:  9.13868522644043\n",
      "################################  283  ################################\n",
      "Loss:  9.133649826049805\n",
      "################################  284  ################################\n",
      "Loss:  9.128902435302734\n",
      "################################  285  ################################\n",
      "Loss:  9.1241455078125\n",
      "################################  286  ################################\n",
      "Loss:  9.119551658630371\n",
      "################################  287  ################################\n",
      "Loss:  9.114930152893066\n",
      "################################  288  ################################\n",
      "Loss:  9.110422134399414\n",
      "################################  289  ################################\n",
      "Loss:  9.105834007263184\n",
      "################################  290  ################################\n",
      "Loss:  9.101324081420898\n",
      "################################  291  ################################\n",
      "Loss:  9.09694766998291\n",
      "################################  292  ################################\n",
      "Loss:  9.09250259399414\n",
      "################################  293  ################################\n",
      "Loss:  9.08796215057373\n",
      "################################  294  ################################\n",
      "Loss:  9.083124160766602\n",
      "################################  295  ################################\n",
      "Loss:  9.078083992004395\n",
      "################################  296  ################################\n",
      "Loss:  9.072985649108887\n",
      "################################  297  ################################\n",
      "Loss:  9.068281173706055\n",
      "################################  298  ################################\n",
      "Loss:  9.063623428344727\n",
      "################################  299  ################################\n",
      "Loss:  9.058866500854492\n",
      "################################  300  ################################\n",
      "Loss:  9.054085731506348\n",
      "################################  301  ################################\n",
      "Loss:  9.049178123474121\n",
      "################################  302  ################################\n",
      "Loss:  9.044254302978516\n",
      "################################  303  ################################\n",
      "Loss:  9.03915786743164\n",
      "################################  304  ################################\n",
      "Loss:  9.034004211425781\n",
      "################################  305  ################################\n",
      "Loss:  9.028448104858398\n",
      "################################  306  ################################\n",
      "Loss:  9.022747993469238\n",
      "################################  307  ################################\n",
      "Loss:  9.016385078430176\n",
      "################################  308  ################################\n",
      "Loss:  9.01032829284668\n",
      "################################  309  ################################\n",
      "Loss:  9.0034761428833\n",
      "################################  310  ################################\n",
      "Loss:  8.996917724609375\n",
      "################################  311  ################################\n",
      "Loss:  8.989989280700684\n",
      "################################  312  ################################\n",
      "Loss:  8.983054161071777\n",
      "################################  313  ################################\n",
      "Loss:  8.97580337524414\n",
      "################################  314  ################################\n",
      "Loss:  8.968131065368652\n",
      "################################  315  ################################\n",
      "Loss:  8.96031379699707\n",
      "################################  316  ################################\n",
      "Loss:  8.951979637145996\n",
      "################################  317  ################################\n",
      "Loss:  8.94375991821289\n",
      "################################  318  ################################\n",
      "Loss:  8.935193061828613\n",
      "################################  319  ################################\n",
      "Loss:  8.926708221435547\n",
      "################################  320  ################################\n",
      "Loss:  8.917825698852539\n",
      "################################  321  ################################\n",
      "Loss:  8.908544540405273\n",
      "################################  322  ################################\n",
      "Loss:  8.898913383483887\n",
      "################################  323  ################################\n",
      "Loss:  8.888484001159668\n",
      "################################  324  ################################\n",
      "Loss:  8.87890911102295\n",
      "################################  325  ################################\n",
      "Loss:  8.86862564086914\n",
      "################################  326  ################################\n",
      "Loss:  8.86008358001709\n",
      "################################  327  ################################\n",
      "Loss:  8.851993560791016\n",
      "################################  328  ################################\n",
      "Loss:  8.844879150390625\n",
      "################################  329  ################################\n",
      "Loss:  8.837800025939941\n",
      "################################  330  ################################\n",
      "Loss:  8.830681800842285\n",
      "################################  331  ################################\n",
      "Loss:  8.823026657104492\n",
      "################################  332  ################################\n",
      "Loss:  8.81482982635498\n",
      "################################  333  ################################\n",
      "Loss:  8.805377006530762\n",
      "################################  334  ################################\n",
      "Loss:  8.795304298400879\n",
      "################################  335  ################################\n",
      "Loss:  8.783236503601074\n",
      "################################  336  ################################\n",
      "Loss:  8.772109985351562\n",
      "################################  337  ################################\n",
      "Loss:  8.76026439666748\n",
      "################################  338  ################################\n",
      "Loss:  8.748608589172363\n",
      "################################  339  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  8.73745346069336\n",
      "################################  340  ################################\n",
      "Loss:  8.727364540100098\n",
      "################################  341  ################################\n",
      "Loss:  8.718502044677734\n",
      "################################  342  ################################\n",
      "Loss:  8.709537506103516\n",
      "################################  343  ################################\n",
      "Loss:  8.702496528625488\n",
      "################################  344  ################################\n",
      "Loss:  8.696626663208008\n",
      "################################  345  ################################\n",
      "Loss:  8.688704490661621\n",
      "################################  346  ################################\n",
      "Loss:  8.68043327331543\n",
      "################################  347  ################################\n",
      "Loss:  8.670685768127441\n",
      "################################  348  ################################\n",
      "Loss:  8.661221504211426\n",
      "################################  349  ################################\n",
      "Loss:  8.649016380310059\n",
      "################################  350  ################################\n",
      "Loss:  8.636816024780273\n",
      "################################  351  ################################\n",
      "Loss:  8.62425708770752\n",
      "################################  352  ################################\n",
      "Loss:  8.611844062805176\n",
      "################################  353  ################################\n",
      "Loss:  8.597997665405273\n",
      "################################  354  ################################\n",
      "Loss:  8.583174705505371\n",
      "################################  355  ################################\n",
      "Loss:  8.568554878234863\n",
      "################################  356  ################################\n",
      "Loss:  8.552412986755371\n",
      "################################  357  ################################\n",
      "Loss:  8.535157203674316\n",
      "################################  358  ################################\n",
      "Loss:  8.518627166748047\n",
      "################################  359  ################################\n",
      "Loss:  8.49956226348877\n",
      "################################  360  ################################\n",
      "Loss:  8.487037658691406\n",
      "################################  361  ################################\n",
      "Loss:  8.473759651184082\n",
      "################################  362  ################################\n",
      "Loss:  8.455574035644531\n",
      "################################  363  ################################\n",
      "Loss:  8.434000968933105\n",
      "################################  364  ################################\n",
      "Loss:  8.408851623535156\n",
      "################################  365  ################################\n",
      "Loss:  8.388014793395996\n",
      "################################  366  ################################\n",
      "Loss:  8.371929168701172\n",
      "################################  367  ################################\n",
      "Loss:  8.35981273651123\n",
      "################################  368  ################################\n",
      "Loss:  8.348742485046387\n",
      "################################  369  ################################\n",
      "Loss:  8.337019920349121\n",
      "################################  370  ################################\n",
      "Loss:  8.325708389282227\n",
      "################################  371  ################################\n",
      "Loss:  8.313426971435547\n",
      "################################  372  ################################\n",
      "Loss:  8.300073623657227\n",
      "################################  373  ################################\n",
      "Loss:  8.28529167175293\n",
      "################################  374  ################################\n",
      "Loss:  8.270354270935059\n",
      "################################  375  ################################\n",
      "Loss:  8.254354476928711\n",
      "################################  376  ################################\n",
      "Loss:  8.237771034240723\n",
      "################################  377  ################################\n",
      "Loss:  8.220329284667969\n",
      "################################  378  ################################\n",
      "Loss:  8.203486442565918\n",
      "################################  379  ################################\n",
      "Loss:  8.187822341918945\n",
      "################################  380  ################################\n",
      "Loss:  8.17248821258545\n",
      "################################  381  ################################\n",
      "Loss:  8.15698528289795\n",
      "################################  382  ################################\n",
      "Loss:  8.140874862670898\n",
      "################################  383  ################################\n",
      "Loss:  8.124760627746582\n",
      "################################  384  ################################\n",
      "Loss:  8.108367919921875\n",
      "################################  385  ################################\n",
      "Loss:  8.092575073242188\n",
      "################################  386  ################################\n",
      "Loss:  8.077020645141602\n",
      "################################  387  ################################\n",
      "Loss:  8.06224536895752\n",
      "################################  388  ################################\n",
      "Loss:  8.047988891601562\n",
      "################################  389  ################################\n",
      "Loss:  8.034343719482422\n",
      "################################  390  ################################\n",
      "Loss:  8.021166801452637\n",
      "################################  391  ################################\n",
      "Loss:  8.008233070373535\n",
      "################################  392  ################################\n",
      "Loss:  7.9956135749816895\n",
      "################################  393  ################################\n",
      "Loss:  7.983083724975586\n",
      "################################  394  ################################\n",
      "Loss:  7.970844268798828\n",
      "################################  395  ################################\n",
      "Loss:  7.958482265472412\n",
      "################################  396  ################################\n",
      "Loss:  7.946198463439941\n",
      "################################  397  ################################\n",
      "Loss:  7.932995796203613\n",
      "################################  398  ################################\n",
      "Loss:  7.919246196746826\n",
      "################################  399  ################################\n",
      "Loss:  7.9046454429626465\n",
      "################################  400  ################################\n",
      "Loss:  7.891733646392822\n",
      "################################  401  ################################\n",
      "Loss:  7.877867698669434\n",
      "################################  402  ################################\n",
      "Loss:  7.862880706787109\n",
      "################################  403  ################################\n",
      "Loss:  7.845738410949707\n",
      "################################  404  ################################\n",
      "Loss:  7.827704429626465\n",
      "################################  405  ################################\n",
      "Loss:  7.810121536254883\n",
      "################################  406  ################################\n",
      "Loss:  7.793065071105957\n",
      "################################  407  ################################\n",
      "Loss:  7.77617883682251\n",
      "################################  408  ################################\n",
      "Loss:  7.758027076721191\n",
      "################################  409  ################################\n",
      "Loss:  7.739654064178467\n",
      "################################  410  ################################\n",
      "Loss:  7.72076940536499\n",
      "################################  411  ################################\n",
      "Loss:  7.7026214599609375\n",
      "################################  412  ################################\n",
      "Loss:  7.684475898742676\n",
      "################################  413  ################################\n",
      "Loss:  7.666924476623535\n",
      "################################  414  ################################\n",
      "Loss:  7.648559093475342\n",
      "################################  415  ################################\n",
      "Loss:  7.630147933959961\n",
      "################################  416  ################################\n",
      "Loss:  7.612708568572998\n",
      "################################  417  ################################\n",
      "Loss:  7.596629619598389\n",
      "################################  418  ################################\n",
      "Loss:  7.580136299133301\n",
      "################################  419  ################################\n",
      "Loss:  7.5627899169921875\n",
      "################################  420  ################################\n",
      "Loss:  7.545840263366699\n",
      "################################  421  ################################\n",
      "Loss:  7.527568340301514\n",
      "################################  422  ################################\n",
      "Loss:  7.508065223693848\n",
      "################################  423  ################################\n",
      "Loss:  7.4918999671936035\n",
      "################################  424  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  7.47746467590332\n",
      "################################  425  ################################\n",
      "Loss:  7.462278366088867\n",
      "################################  426  ################################\n",
      "Loss:  7.444904327392578\n",
      "################################  427  ################################\n",
      "Loss:  7.429030418395996\n",
      "################################  428  ################################\n",
      "Loss:  7.414653301239014\n",
      "################################  429  ################################\n",
      "Loss:  7.401580810546875\n",
      "################################  430  ################################\n",
      "Loss:  7.3892645835876465\n",
      "################################  431  ################################\n",
      "Loss:  7.377867221832275\n",
      "################################  432  ################################\n",
      "Loss:  7.366848468780518\n",
      "################################  433  ################################\n",
      "Loss:  7.356223106384277\n",
      "################################  434  ################################\n",
      "Loss:  7.346603870391846\n",
      "################################  435  ################################\n",
      "Loss:  7.338118076324463\n",
      "################################  436  ################################\n",
      "Loss:  7.330338001251221\n",
      "################################  437  ################################\n",
      "Loss:  7.323079586029053\n",
      "################################  438  ################################\n",
      "Loss:  7.316063404083252\n",
      "################################  439  ################################\n",
      "Loss:  7.3090667724609375\n",
      "################################  440  ################################\n",
      "Loss:  7.301599979400635\n",
      "################################  441  ################################\n",
      "Loss:  7.293675422668457\n",
      "################################  442  ################################\n",
      "Loss:  7.285759449005127\n",
      "################################  443  ################################\n",
      "Loss:  7.278087615966797\n",
      "################################  444  ################################\n",
      "Loss:  7.270693302154541\n",
      "################################  445  ################################\n",
      "Loss:  7.263576030731201\n",
      "################################  446  ################################\n",
      "Loss:  7.2564287185668945\n",
      "################################  447  ################################\n",
      "Loss:  7.249183654785156\n",
      "################################  448  ################################\n",
      "Loss:  7.241711139678955\n",
      "################################  449  ################################\n",
      "Loss:  7.233933925628662\n",
      "################################  450  ################################\n",
      "Loss:  7.225589752197266\n",
      "################################  451  ################################\n",
      "Loss:  7.216460704803467\n",
      "################################  452  ################################\n",
      "Loss:  7.205028057098389\n",
      "################################  453  ################################\n",
      "Loss:  7.190640449523926\n",
      "################################  454  ################################\n",
      "Loss:  7.175836563110352\n",
      "################################  455  ################################\n",
      "Loss:  7.159885406494141\n",
      "################################  456  ################################\n",
      "Loss:  7.143476486206055\n",
      "################################  457  ################################\n",
      "Loss:  7.126965045928955\n",
      "################################  458  ################################\n",
      "Loss:  7.110438823699951\n",
      "################################  459  ################################\n",
      "Loss:  7.094377517700195\n",
      "################################  460  ################################\n",
      "Loss:  7.078527927398682\n",
      "################################  461  ################################\n",
      "Loss:  7.063791275024414\n",
      "################################  462  ################################\n",
      "Loss:  7.049455642700195\n",
      "################################  463  ################################\n",
      "Loss:  7.036739826202393\n",
      "################################  464  ################################\n",
      "Loss:  7.025502681732178\n",
      "################################  465  ################################\n",
      "Loss:  7.016185760498047\n",
      "################################  466  ################################\n",
      "Loss:  7.006875514984131\n",
      "################################  467  ################################\n",
      "Loss:  6.997481346130371\n",
      "################################  468  ################################\n",
      "Loss:  6.987936019897461\n",
      "################################  469  ################################\n",
      "Loss:  6.978668212890625\n",
      "################################  470  ################################\n",
      "Loss:  6.970806121826172\n",
      "################################  471  ################################\n",
      "Loss:  6.962421894073486\n",
      "################################  472  ################################\n",
      "Loss:  6.954412937164307\n",
      "################################  473  ################################\n",
      "Loss:  6.946906566619873\n",
      "################################  474  ################################\n",
      "Loss:  6.939831733703613\n",
      "################################  475  ################################\n",
      "Loss:  6.9329681396484375\n",
      "################################  476  ################################\n",
      "Loss:  6.926470756530762\n",
      "################################  477  ################################\n",
      "Loss:  6.9204583168029785\n",
      "################################  478  ################################\n",
      "Loss:  6.914762496948242\n",
      "################################  479  ################################\n",
      "Loss:  6.909123420715332\n",
      "################################  480  ################################\n",
      "Loss:  6.903425216674805\n",
      "################################  481  ################################\n",
      "Loss:  6.897603988647461\n",
      "################################  482  ################################\n",
      "Loss:  6.891772270202637\n",
      "################################  483  ################################\n",
      "Loss:  6.88580846786499\n",
      "################################  484  ################################\n",
      "Loss:  6.879918098449707\n",
      "################################  485  ################################\n",
      "Loss:  6.873982906341553\n",
      "################################  486  ################################\n",
      "Loss:  6.868286609649658\n",
      "################################  487  ################################\n",
      "Loss:  6.8624467849731445\n",
      "################################  488  ################################\n",
      "Loss:  6.8563666343688965\n",
      "################################  489  ################################\n",
      "Loss:  6.849725723266602\n",
      "################################  490  ################################\n",
      "Loss:  6.8426127433776855\n",
      "################################  491  ################################\n",
      "Loss:  6.8347930908203125\n",
      "################################  492  ################################\n",
      "Loss:  6.826624870300293\n",
      "################################  493  ################################\n",
      "Loss:  6.817770957946777\n",
      "################################  494  ################################\n",
      "Loss:  6.80962610244751\n",
      "################################  495  ################################\n",
      "Loss:  6.801375865936279\n",
      "################################  496  ################################\n",
      "Loss:  6.793763637542725\n",
      "################################  497  ################################\n",
      "Loss:  6.786210060119629\n",
      "################################  498  ################################\n",
      "Loss:  6.778929233551025\n",
      "################################  499  ################################\n",
      "Loss:  6.771727561950684\n",
      "################################  500  ################################\n",
      "Loss:  6.76595401763916\n",
      "################################  501  ################################\n",
      "Loss:  6.760509014129639\n",
      "################################  502  ################################\n",
      "Loss:  6.755673885345459\n",
      "################################  503  ################################\n",
      "Loss:  6.750656604766846\n",
      "################################  504  ################################\n",
      "Loss:  6.745651721954346\n",
      "################################  505  ################################\n",
      "Loss:  6.74107027053833\n",
      "################################  506  ################################\n",
      "Loss:  6.736684799194336\n",
      "################################  507  ################################\n",
      "Loss:  6.732456684112549\n",
      "################################  508  ################################\n",
      "Loss:  6.727573871612549\n",
      "################################  509  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  6.723687171936035\n",
      "################################  510  ################################\n",
      "Loss:  6.72071647644043\n",
      "################################  511  ################################\n",
      "Loss:  6.7174787521362305\n",
      "################################  512  ################################\n",
      "Loss:  6.714106559753418\n",
      "################################  513  ################################\n",
      "Loss:  6.710633754730225\n",
      "################################  514  ################################\n",
      "Loss:  6.707064151763916\n",
      "################################  515  ################################\n",
      "Loss:  6.703409671783447\n",
      "################################  516  ################################\n",
      "Loss:  6.699874401092529\n",
      "################################  517  ################################\n",
      "Loss:  6.696496963500977\n",
      "################################  518  ################################\n",
      "Loss:  6.693207263946533\n",
      "################################  519  ################################\n",
      "Loss:  6.690019130706787\n",
      "################################  520  ################################\n",
      "Loss:  6.6869659423828125\n",
      "################################  521  ################################\n",
      "Loss:  6.684027194976807\n",
      "################################  522  ################################\n",
      "Loss:  6.681219577789307\n",
      "################################  523  ################################\n",
      "Loss:  6.678470134735107\n",
      "################################  524  ################################\n",
      "Loss:  6.675735950469971\n",
      "################################  525  ################################\n",
      "Loss:  6.6729912757873535\n",
      "################################  526  ################################\n",
      "Loss:  6.670194625854492\n",
      "################################  527  ################################\n",
      "Loss:  6.667419910430908\n",
      "################################  528  ################################\n",
      "Loss:  6.6646504402160645\n",
      "################################  529  ################################\n",
      "Loss:  6.661913871765137\n",
      "################################  530  ################################\n",
      "Loss:  6.659120559692383\n",
      "################################  531  ################################\n",
      "Loss:  6.656493663787842\n",
      "################################  532  ################################\n",
      "Loss:  6.653441429138184\n",
      "################################  533  ################################\n",
      "Loss:  6.650286674499512\n",
      "################################  534  ################################\n",
      "Loss:  6.647546768188477\n",
      "################################  535  ################################\n",
      "Loss:  6.643898963928223\n",
      "################################  536  ################################\n",
      "Loss:  6.639335632324219\n",
      "################################  537  ################################\n",
      "Loss:  6.634834289550781\n",
      "################################  538  ################################\n",
      "Loss:  6.629825592041016\n",
      "################################  539  ################################\n",
      "Loss:  6.624853610992432\n",
      "################################  540  ################################\n",
      "Loss:  6.618980407714844\n",
      "################################  541  ################################\n",
      "Loss:  6.613542079925537\n",
      "################################  542  ################################\n",
      "Loss:  6.607800006866455\n",
      "################################  543  ################################\n",
      "Loss:  6.601303577423096\n",
      "################################  544  ################################\n",
      "Loss:  6.59487771987915\n",
      "################################  545  ################################\n",
      "Loss:  6.58817720413208\n",
      "################################  546  ################################\n",
      "Loss:  6.5810770988464355\n",
      "################################  547  ################################\n",
      "Loss:  6.57396125793457\n",
      "################################  548  ################################\n",
      "Loss:  6.56715202331543\n",
      "################################  549  ################################\n",
      "Loss:  6.5604071617126465\n",
      "################################  550  ################################\n",
      "Loss:  6.554053783416748\n",
      "################################  551  ################################\n",
      "Loss:  6.547996997833252\n",
      "################################  552  ################################\n",
      "Loss:  6.542205333709717\n",
      "################################  553  ################################\n",
      "Loss:  6.536957263946533\n",
      "################################  554  ################################\n",
      "Loss:  6.531998157501221\n",
      "################################  555  ################################\n",
      "Loss:  6.527530193328857\n",
      "################################  556  ################################\n",
      "Loss:  6.523189544677734\n",
      "################################  557  ################################\n",
      "Loss:  6.5187482833862305\n",
      "################################  558  ################################\n",
      "Loss:  6.514761447906494\n",
      "################################  559  ################################\n",
      "Loss:  6.510920524597168\n",
      "################################  560  ################################\n",
      "Loss:  6.507320404052734\n",
      "################################  561  ################################\n",
      "Loss:  6.50377893447876\n",
      "################################  562  ################################\n",
      "Loss:  6.499837875366211\n",
      "################################  563  ################################\n",
      "Loss:  6.496344566345215\n",
      "################################  564  ################################\n",
      "Loss:  6.492733001708984\n",
      "################################  565  ################################\n",
      "Loss:  6.488636493682861\n",
      "################################  566  ################################\n",
      "Loss:  6.484885215759277\n",
      "################################  567  ################################\n",
      "Loss:  6.481247901916504\n",
      "################################  568  ################################\n",
      "Loss:  6.477275848388672\n",
      "################################  569  ################################\n",
      "Loss:  6.473339557647705\n",
      "################################  570  ################################\n",
      "Loss:  6.46916389465332\n",
      "################################  571  ################################\n",
      "Loss:  6.464883804321289\n",
      "################################  572  ################################\n",
      "Loss:  6.460480213165283\n",
      "################################  573  ################################\n",
      "Loss:  6.456150054931641\n",
      "################################  574  ################################\n",
      "Loss:  6.452048301696777\n",
      "################################  575  ################################\n",
      "Loss:  6.4474263191223145\n",
      "################################  576  ################################\n",
      "Loss:  6.443021297454834\n",
      "################################  577  ################################\n",
      "Loss:  6.439736843109131\n",
      "################################  578  ################################\n",
      "Loss:  6.43651819229126\n",
      "################################  579  ################################\n",
      "Loss:  6.433322429656982\n",
      "################################  580  ################################\n",
      "Loss:  6.430144786834717\n",
      "################################  581  ################################\n",
      "Loss:  6.427206516265869\n",
      "################################  582  ################################\n",
      "Loss:  6.424376487731934\n",
      "################################  583  ################################\n",
      "Loss:  6.421721935272217\n",
      "################################  584  ################################\n",
      "Loss:  6.419118881225586\n",
      "################################  585  ################################\n",
      "Loss:  6.416573524475098\n",
      "################################  586  ################################\n",
      "Loss:  6.414037227630615\n",
      "################################  587  ################################\n",
      "Loss:  6.411600112915039\n",
      "################################  588  ################################\n",
      "Loss:  6.409236907958984\n",
      "################################  589  ################################\n",
      "Loss:  6.406945705413818\n",
      "################################  590  ################################\n",
      "Loss:  6.404679775238037\n",
      "################################  591  ################################\n",
      "Loss:  6.402231216430664\n",
      "################################  592  ################################\n",
      "Loss:  6.399693012237549\n",
      "################################  593  ################################\n",
      "Loss:  6.397138595581055\n",
      "################################  594  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  6.394594669342041\n",
      "################################  595  ################################\n",
      "Loss:  6.391903400421143\n",
      "################################  596  ################################\n",
      "Loss:  6.389125347137451\n",
      "################################  597  ################################\n",
      "Loss:  6.386438846588135\n",
      "################################  598  ################################\n",
      "Loss:  6.383725643157959\n",
      "################################  599  ################################\n",
      "Loss:  6.381019115447998\n",
      "################################  600  ################################\n",
      "Loss:  6.378300189971924\n",
      "################################  601  ################################\n",
      "Loss:  6.37562370300293\n",
      "################################  602  ################################\n",
      "Loss:  6.373012542724609\n",
      "################################  603  ################################\n",
      "Loss:  6.370527744293213\n",
      "################################  604  ################################\n",
      "Loss:  6.368130683898926\n",
      "################################  605  ################################\n",
      "Loss:  6.365718364715576\n",
      "################################  606  ################################\n",
      "Loss:  6.363284587860107\n",
      "################################  607  ################################\n",
      "Loss:  6.360987663269043\n",
      "################################  608  ################################\n",
      "Loss:  6.358542442321777\n",
      "################################  609  ################################\n",
      "Loss:  6.356421947479248\n",
      "################################  610  ################################\n",
      "Loss:  6.353927135467529\n",
      "################################  611  ################################\n",
      "Loss:  6.351505279541016\n",
      "################################  612  ################################\n",
      "Loss:  6.348832130432129\n",
      "################################  613  ################################\n",
      "Loss:  6.345231056213379\n",
      "################################  614  ################################\n",
      "Loss:  6.34266471862793\n",
      "################################  615  ################################\n",
      "Loss:  6.339395523071289\n",
      "################################  616  ################################\n",
      "Loss:  6.33592414855957\n",
      "################################  617  ################################\n",
      "Loss:  6.332348346710205\n",
      "################################  618  ################################\n",
      "Loss:  6.328533172607422\n",
      "################################  619  ################################\n",
      "Loss:  6.324708938598633\n",
      "################################  620  ################################\n",
      "Loss:  6.320793151855469\n",
      "################################  621  ################################\n",
      "Loss:  6.317049503326416\n",
      "################################  622  ################################\n",
      "Loss:  6.31328010559082\n",
      "################################  623  ################################\n",
      "Loss:  6.309684753417969\n",
      "################################  624  ################################\n",
      "Loss:  6.306028842926025\n",
      "################################  625  ################################\n",
      "Loss:  6.30244255065918\n",
      "################################  626  ################################\n",
      "Loss:  6.2986931800842285\n",
      "################################  627  ################################\n",
      "Loss:  6.294874668121338\n",
      "################################  628  ################################\n",
      "Loss:  6.2908244132995605\n",
      "################################  629  ################################\n",
      "Loss:  6.286637306213379\n",
      "################################  630  ################################\n",
      "Loss:  6.282174110412598\n",
      "################################  631  ################################\n",
      "Loss:  6.277548789978027\n",
      "################################  632  ################################\n",
      "Loss:  6.272512435913086\n",
      "################################  633  ################################\n",
      "Loss:  6.267175674438477\n",
      "################################  634  ################################\n",
      "Loss:  6.262279987335205\n",
      "################################  635  ################################\n",
      "Loss:  6.257363796234131\n",
      "################################  636  ################################\n",
      "Loss:  6.252825736999512\n",
      "################################  637  ################################\n",
      "Loss:  6.247954845428467\n",
      "################################  638  ################################\n",
      "Loss:  6.243061065673828\n",
      "################################  639  ################################\n",
      "Loss:  6.23796272277832\n",
      "################################  640  ################################\n",
      "Loss:  6.233116626739502\n",
      "################################  641  ################################\n",
      "Loss:  6.228222846984863\n",
      "################################  642  ################################\n",
      "Loss:  6.223587512969971\n",
      "################################  643  ################################\n",
      "Loss:  6.218811988830566\n",
      "################################  644  ################################\n",
      "Loss:  6.213932991027832\n",
      "################################  645  ################################\n",
      "Loss:  6.208665370941162\n",
      "################################  646  ################################\n",
      "Loss:  6.20337438583374\n",
      "################################  647  ################################\n",
      "Loss:  6.197669506072998\n",
      "################################  648  ################################\n",
      "Loss:  6.191873073577881\n",
      "################################  649  ################################\n",
      "Loss:  6.186408519744873\n",
      "################################  650  ################################\n",
      "Loss:  6.1805100440979\n",
      "################################  651  ################################\n",
      "Loss:  6.174714088439941\n",
      "################################  652  ################################\n",
      "Loss:  6.1688690185546875\n",
      "################################  653  ################################\n",
      "Loss:  6.163244247436523\n",
      "################################  654  ################################\n",
      "Loss:  6.157679557800293\n",
      "################################  655  ################################\n",
      "Loss:  6.152253150939941\n",
      "################################  656  ################################\n",
      "Loss:  6.146819114685059\n",
      "################################  657  ################################\n",
      "Loss:  6.141305446624756\n",
      "################################  658  ################################\n",
      "Loss:  6.135164737701416\n",
      "################################  659  ################################\n",
      "Loss:  6.1305036544799805\n",
      "################################  660  ################################\n",
      "Loss:  6.125992774963379\n",
      "################################  661  ################################\n",
      "Loss:  6.121269702911377\n",
      "################################  662  ################################\n",
      "Loss:  6.1169753074646\n",
      "################################  663  ################################\n",
      "Loss:  6.112910747528076\n",
      "################################  664  ################################\n",
      "Loss:  6.108948230743408\n",
      "################################  665  ################################\n",
      "Loss:  6.105358123779297\n",
      "################################  666  ################################\n",
      "Loss:  6.101738929748535\n",
      "################################  667  ################################\n",
      "Loss:  6.0983991622924805\n",
      "################################  668  ################################\n",
      "Loss:  6.095165252685547\n",
      "################################  669  ################################\n",
      "Loss:  6.092085361480713\n",
      "################################  670  ################################\n",
      "Loss:  6.088977336883545\n",
      "################################  671  ################################\n",
      "Loss:  6.08608341217041\n",
      "################################  672  ################################\n",
      "Loss:  6.0830864906311035\n",
      "################################  673  ################################\n",
      "Loss:  6.080138206481934\n",
      "################################  674  ################################\n",
      "Loss:  6.076685905456543\n",
      "################################  675  ################################\n",
      "Loss:  6.073109149932861\n",
      "################################  676  ################################\n",
      "Loss:  6.068352222442627\n",
      "################################  677  ################################\n",
      "Loss:  6.063823699951172\n",
      "################################  678  ################################\n",
      "Loss:  6.059265613555908\n",
      "################################  679  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  6.053913116455078\n",
      "################################  680  ################################\n",
      "Loss:  6.04955530166626\n",
      "################################  681  ################################\n",
      "Loss:  6.044833660125732\n",
      "################################  682  ################################\n",
      "Loss:  6.039730072021484\n",
      "################################  683  ################################\n",
      "Loss:  6.034572601318359\n",
      "################################  684  ################################\n",
      "Loss:  6.029504776000977\n",
      "################################  685  ################################\n",
      "Loss:  6.024832725524902\n",
      "################################  686  ################################\n",
      "Loss:  6.020561695098877\n",
      "################################  687  ################################\n",
      "Loss:  6.016807556152344\n",
      "################################  688  ################################\n",
      "Loss:  6.013449668884277\n",
      "################################  689  ################################\n",
      "Loss:  6.010411262512207\n",
      "################################  690  ################################\n",
      "Loss:  6.007605075836182\n",
      "################################  691  ################################\n",
      "Loss:  6.004929065704346\n",
      "################################  692  ################################\n",
      "Loss:  6.002283096313477\n",
      "################################  693  ################################\n",
      "Loss:  5.9998369216918945\n",
      "################################  694  ################################\n",
      "Loss:  5.997572422027588\n",
      "################################  695  ################################\n",
      "Loss:  5.995611667633057\n",
      "################################  696  ################################\n",
      "Loss:  5.99375057220459\n",
      "################################  697  ################################\n",
      "Loss:  5.991962432861328\n",
      "################################  698  ################################\n",
      "Loss:  5.9902167320251465\n",
      "################################  699  ################################\n",
      "Loss:  5.988216876983643\n",
      "################################  700  ################################\n",
      "Loss:  5.986514091491699\n",
      "################################  701  ################################\n",
      "Loss:  5.98427677154541\n",
      "################################  702  ################################\n",
      "Loss:  5.982044219970703\n",
      "################################  703  ################################\n",
      "Loss:  5.979580402374268\n",
      "################################  704  ################################\n",
      "Loss:  5.976649284362793\n",
      "################################  705  ################################\n",
      "Loss:  5.973886966705322\n",
      "################################  706  ################################\n",
      "Loss:  5.9708571434021\n",
      "################################  707  ################################\n",
      "Loss:  5.967527866363525\n",
      "################################  708  ################################\n",
      "Loss:  5.9646148681640625\n",
      "################################  709  ################################\n",
      "Loss:  5.96156120300293\n",
      "################################  710  ################################\n",
      "Loss:  5.958588123321533\n",
      "################################  711  ################################\n",
      "Loss:  5.955684185028076\n",
      "################################  712  ################################\n",
      "Loss:  5.9525675773620605\n",
      "################################  713  ################################\n",
      "Loss:  5.949577808380127\n",
      "################################  714  ################################\n",
      "Loss:  5.946869373321533\n",
      "################################  715  ################################\n",
      "Loss:  5.944088935852051\n",
      "################################  716  ################################\n",
      "Loss:  5.941442489624023\n",
      "################################  717  ################################\n",
      "Loss:  5.938791751861572\n",
      "################################  718  ################################\n",
      "Loss:  5.935997009277344\n",
      "################################  719  ################################\n",
      "Loss:  5.933450222015381\n",
      "################################  720  ################################\n",
      "Loss:  5.931053638458252\n",
      "################################  721  ################################\n",
      "Loss:  5.928812503814697\n",
      "################################  722  ################################\n",
      "Loss:  5.926685810089111\n",
      "################################  723  ################################\n",
      "Loss:  5.924686908721924\n",
      "################################  724  ################################\n",
      "Loss:  5.922723770141602\n",
      "################################  725  ################################\n",
      "Loss:  5.920759201049805\n",
      "################################  726  ################################\n",
      "Loss:  5.9185566902160645\n",
      "################################  727  ################################\n",
      "Loss:  5.91643762588501\n",
      "################################  728  ################################\n",
      "Loss:  5.914577007293701\n",
      "################################  729  ################################\n",
      "Loss:  5.9121809005737305\n",
      "################################  730  ################################\n",
      "Loss:  5.9101972579956055\n",
      "################################  731  ################################\n",
      "Loss:  5.907880783081055\n",
      "################################  732  ################################\n",
      "Loss:  5.905455112457275\n",
      "################################  733  ################################\n",
      "Loss:  5.902923583984375\n",
      "################################  734  ################################\n",
      "Loss:  5.9002156257629395\n",
      "################################  735  ################################\n",
      "Loss:  5.897175312042236\n",
      "################################  736  ################################\n",
      "Loss:  5.894162178039551\n",
      "################################  737  ################################\n",
      "Loss:  5.89121675491333\n",
      "################################  738  ################################\n",
      "Loss:  5.888235569000244\n",
      "################################  739  ################################\n",
      "Loss:  5.885238170623779\n",
      "################################  740  ################################\n",
      "Loss:  5.882356643676758\n",
      "################################  741  ################################\n",
      "Loss:  5.879502296447754\n",
      "################################  742  ################################\n",
      "Loss:  5.8767852783203125\n",
      "################################  743  ################################\n",
      "Loss:  5.874147891998291\n",
      "################################  744  ################################\n",
      "Loss:  5.871676445007324\n",
      "################################  745  ################################\n",
      "Loss:  5.869298934936523\n",
      "################################  746  ################################\n",
      "Loss:  5.867082595825195\n",
      "################################  747  ################################\n",
      "Loss:  5.8649797439575195\n",
      "################################  748  ################################\n",
      "Loss:  5.863074779510498\n",
      "################################  749  ################################\n",
      "Loss:  5.861282825469971\n",
      "################################  750  ################################\n",
      "Loss:  5.859625816345215\n",
      "################################  751  ################################\n",
      "Loss:  5.8580451011657715\n",
      "################################  752  ################################\n",
      "Loss:  5.856509208679199\n",
      "################################  753  ################################\n",
      "Loss:  5.8550238609313965\n",
      "################################  754  ################################\n",
      "Loss:  5.853574275970459\n",
      "################################  755  ################################\n",
      "Loss:  5.852139472961426\n",
      "################################  756  ################################\n",
      "Loss:  5.850709915161133\n",
      "################################  757  ################################\n",
      "Loss:  5.849269866943359\n",
      "################################  758  ################################\n",
      "Loss:  5.847836017608643\n",
      "################################  759  ################################\n",
      "Loss:  5.8463521003723145\n",
      "################################  760  ################################\n",
      "Loss:  5.844820022583008\n",
      "################################  761  ################################\n",
      "Loss:  5.843149662017822\n",
      "################################  762  ################################\n",
      "Loss:  5.8412556648254395\n",
      "################################  763  ################################\n",
      "Loss:  5.83951997756958\n",
      "################################  764  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.837794303894043\n",
      "################################  765  ################################\n",
      "Loss:  5.835873603820801\n",
      "################################  766  ################################\n",
      "Loss:  5.833878517150879\n",
      "################################  767  ################################\n",
      "Loss:  5.83186149597168\n",
      "################################  768  ################################\n",
      "Loss:  5.829667568206787\n",
      "################################  769  ################################\n",
      "Loss:  5.8274827003479\n",
      "################################  770  ################################\n",
      "Loss:  5.8250627517700195\n",
      "################################  771  ################################\n",
      "Loss:  5.822494983673096\n",
      "################################  772  ################################\n",
      "Loss:  5.819838523864746\n",
      "################################  773  ################################\n",
      "Loss:  5.817320823669434\n",
      "################################  774  ################################\n",
      "Loss:  5.814767360687256\n",
      "################################  775  ################################\n",
      "Loss:  5.812342643737793\n",
      "################################  776  ################################\n",
      "Loss:  5.8098883628845215\n",
      "################################  777  ################################\n",
      "Loss:  5.807508945465088\n",
      "################################  778  ################################\n",
      "Loss:  5.8051042556762695\n",
      "################################  779  ################################\n",
      "Loss:  5.802788257598877\n",
      "################################  780  ################################\n",
      "Loss:  5.800500392913818\n",
      "################################  781  ################################\n",
      "Loss:  5.7981743812561035\n",
      "################################  782  ################################\n",
      "Loss:  5.795135021209717\n",
      "################################  783  ################################\n",
      "Loss:  5.793693542480469\n",
      "################################  784  ################################\n",
      "Loss:  5.792267799377441\n",
      "################################  785  ################################\n",
      "Loss:  5.79077672958374\n",
      "################################  786  ################################\n",
      "Loss:  5.789226055145264\n",
      "################################  787  ################################\n",
      "Loss:  5.787635803222656\n",
      "################################  788  ################################\n",
      "Loss:  5.786025047302246\n",
      "################################  789  ################################\n",
      "Loss:  5.784412384033203\n",
      "################################  790  ################################\n",
      "Loss:  5.782816410064697\n",
      "################################  791  ################################\n",
      "Loss:  5.7812652587890625\n",
      "################################  792  ################################\n",
      "Loss:  5.779773235321045\n",
      "################################  793  ################################\n",
      "Loss:  5.778332233428955\n",
      "################################  794  ################################\n",
      "Loss:  5.77690315246582\n",
      "################################  795  ################################\n",
      "Loss:  5.7754316329956055\n",
      "################################  796  ################################\n",
      "Loss:  5.773861408233643\n",
      "################################  797  ################################\n",
      "Loss:  5.771985054016113\n",
      "################################  798  ################################\n",
      "Loss:  5.770378112792969\n",
      "################################  799  ################################\n",
      "Loss:  5.768780708312988\n",
      "################################  800  ################################\n",
      "Loss:  5.766973972320557\n",
      "################################  801  ################################\n",
      "Loss:  5.764928340911865\n",
      "################################  802  ################################\n",
      "Loss:  5.762726306915283\n",
      "################################  803  ################################\n",
      "Loss:  5.760467052459717\n",
      "################################  804  ################################\n",
      "Loss:  5.758162021636963\n",
      "################################  805  ################################\n",
      "Loss:  5.755745887756348\n",
      "################################  806  ################################\n",
      "Loss:  5.7533135414123535\n",
      "################################  807  ################################\n",
      "Loss:  5.750906944274902\n",
      "################################  808  ################################\n",
      "Loss:  5.748495101928711\n",
      "################################  809  ################################\n",
      "Loss:  5.74601936340332\n",
      "################################  810  ################################\n",
      "Loss:  5.74365234375\n",
      "################################  811  ################################\n",
      "Loss:  5.7413010597229\n",
      "################################  812  ################################\n",
      "Loss:  5.739053726196289\n",
      "################################  813  ################################\n",
      "Loss:  5.736902236938477\n",
      "################################  814  ################################\n",
      "Loss:  5.734676361083984\n",
      "################################  815  ################################\n",
      "Loss:  5.732478141784668\n",
      "################################  816  ################################\n",
      "Loss:  5.730443477630615\n",
      "################################  817  ################################\n",
      "Loss:  5.728442668914795\n",
      "################################  818  ################################\n",
      "Loss:  5.725836753845215\n",
      "################################  819  ################################\n",
      "Loss:  5.724081039428711\n",
      "################################  820  ################################\n",
      "Loss:  5.721705913543701\n",
      "################################  821  ################################\n",
      "Loss:  5.719511985778809\n",
      "################################  822  ################################\n",
      "Loss:  5.7171196937561035\n",
      "################################  823  ################################\n",
      "Loss:  5.71433687210083\n",
      "################################  824  ################################\n",
      "Loss:  5.710484504699707\n",
      "################################  825  ################################\n",
      "Loss:  5.707577228546143\n",
      "################################  826  ################################\n",
      "Loss:  5.704760551452637\n",
      "################################  827  ################################\n",
      "Loss:  5.7017340660095215\n",
      "################################  828  ################################\n",
      "Loss:  5.698541164398193\n",
      "################################  829  ################################\n",
      "Loss:  5.695329189300537\n",
      "################################  830  ################################\n",
      "Loss:  5.692173480987549\n",
      "################################  831  ################################\n",
      "Loss:  5.68907356262207\n",
      "################################  832  ################################\n",
      "Loss:  5.686068534851074\n",
      "################################  833  ################################\n",
      "Loss:  5.683137893676758\n",
      "################################  834  ################################\n",
      "Loss:  5.6803297996521\n",
      "################################  835  ################################\n",
      "Loss:  5.677623748779297\n",
      "################################  836  ################################\n",
      "Loss:  5.675034046173096\n",
      "################################  837  ################################\n",
      "Loss:  5.672506332397461\n",
      "################################  838  ################################\n",
      "Loss:  5.669960975646973\n",
      "################################  839  ################################\n",
      "Loss:  5.667543411254883\n",
      "################################  840  ################################\n",
      "Loss:  5.665334224700928\n",
      "################################  841  ################################\n",
      "Loss:  5.6632585525512695\n",
      "################################  842  ################################\n",
      "Loss:  5.661346435546875\n",
      "################################  843  ################################\n",
      "Loss:  5.659571647644043\n",
      "################################  844  ################################\n",
      "Loss:  5.657868385314941\n",
      "################################  845  ################################\n",
      "Loss:  5.656196117401123\n",
      "################################  846  ################################\n",
      "Loss:  5.654489517211914\n",
      "################################  847  ################################\n",
      "Loss:  5.6527791023254395\n",
      "################################  848  ################################\n",
      "Loss:  5.651096820831299\n",
      "################################  849  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.649472236633301\n",
      "################################  850  ################################\n",
      "Loss:  5.647860050201416\n",
      "################################  851  ################################\n",
      "Loss:  5.646220684051514\n",
      "################################  852  ################################\n",
      "Loss:  5.644473075866699\n",
      "################################  853  ################################\n",
      "Loss:  5.642745494842529\n",
      "################################  854  ################################\n",
      "Loss:  5.641207695007324\n",
      "################################  855  ################################\n",
      "Loss:  5.6391754150390625\n",
      "################################  856  ################################\n",
      "Loss:  5.6371541023254395\n",
      "################################  857  ################################\n",
      "Loss:  5.634256362915039\n",
      "################################  858  ################################\n",
      "Loss:  5.631436347961426\n",
      "################################  859  ################################\n",
      "Loss:  5.628804683685303\n",
      "################################  860  ################################\n",
      "Loss:  5.626005172729492\n",
      "################################  861  ################################\n",
      "Loss:  5.623056888580322\n",
      "################################  862  ################################\n",
      "Loss:  5.619993686676025\n",
      "################################  863  ################################\n",
      "Loss:  5.6168317794799805\n",
      "################################  864  ################################\n",
      "Loss:  5.6136369705200195\n",
      "################################  865  ################################\n",
      "Loss:  5.610454082489014\n",
      "################################  866  ################################\n",
      "Loss:  5.607351303100586\n",
      "################################  867  ################################\n",
      "Loss:  5.6043500900268555\n",
      "################################  868  ################################\n",
      "Loss:  5.6014933586120605\n",
      "################################  869  ################################\n",
      "Loss:  5.598769664764404\n",
      "################################  870  ################################\n",
      "Loss:  5.596162796020508\n",
      "################################  871  ################################\n",
      "Loss:  5.593703746795654\n",
      "################################  872  ################################\n",
      "Loss:  5.5913777351379395\n",
      "################################  873  ################################\n",
      "Loss:  5.589225769042969\n",
      "################################  874  ################################\n",
      "Loss:  5.586973667144775\n",
      "################################  875  ################################\n",
      "Loss:  5.584855556488037\n",
      "################################  876  ################################\n",
      "Loss:  5.583033084869385\n",
      "################################  877  ################################\n",
      "Loss:  5.5807647705078125\n",
      "################################  878  ################################\n",
      "Loss:  5.578710556030273\n",
      "################################  879  ################################\n",
      "Loss:  5.576430320739746\n",
      "################################  880  ################################\n",
      "Loss:  5.573822021484375\n",
      "################################  881  ################################\n",
      "Loss:  5.571056842803955\n",
      "################################  882  ################################\n",
      "Loss:  5.568115234375\n",
      "################################  883  ################################\n",
      "Loss:  5.565234661102295\n",
      "################################  884  ################################\n",
      "Loss:  5.5619401931762695\n",
      "################################  885  ################################\n",
      "Loss:  5.559151649475098\n",
      "################################  886  ################################\n",
      "Loss:  5.556413650512695\n",
      "################################  887  ################################\n",
      "Loss:  5.55421781539917\n",
      "################################  888  ################################\n",
      "Loss:  5.55214786529541\n",
      "################################  889  ################################\n",
      "Loss:  5.550158977508545\n",
      "################################  890  ################################\n",
      "Loss:  5.548232555389404\n",
      "################################  891  ################################\n",
      "Loss:  5.546390056610107\n",
      "################################  892  ################################\n",
      "Loss:  5.544595241546631\n",
      "################################  893  ################################\n",
      "Loss:  5.542919158935547\n",
      "################################  894  ################################\n",
      "Loss:  5.541331768035889\n",
      "################################  895  ################################\n",
      "Loss:  5.53989315032959\n",
      "################################  896  ################################\n",
      "Loss:  5.538437843322754\n",
      "################################  897  ################################\n",
      "Loss:  5.536791801452637\n",
      "################################  898  ################################\n",
      "Loss:  5.535222053527832\n",
      "################################  899  ################################\n",
      "Loss:  5.533617973327637\n",
      "################################  900  ################################\n",
      "Loss:  5.532107353210449\n",
      "################################  901  ################################\n",
      "Loss:  5.530660629272461\n",
      "################################  902  ################################\n",
      "Loss:  5.528947353363037\n",
      "################################  903  ################################\n",
      "Loss:  5.527194976806641\n",
      "################################  904  ################################\n",
      "Loss:  5.525390148162842\n",
      "################################  905  ################################\n",
      "Loss:  5.523444652557373\n",
      "################################  906  ################################\n",
      "Loss:  5.521556377410889\n",
      "################################  907  ################################\n",
      "Loss:  5.5196404457092285\n",
      "################################  908  ################################\n",
      "Loss:  5.517770290374756\n",
      "################################  909  ################################\n",
      "Loss:  5.515929222106934\n",
      "################################  910  ################################\n",
      "Loss:  5.514115333557129\n",
      "################################  911  ################################\n",
      "Loss:  5.512353897094727\n",
      "################################  912  ################################\n",
      "Loss:  5.5106048583984375\n",
      "################################  913  ################################\n",
      "Loss:  5.508926868438721\n",
      "################################  914  ################################\n",
      "Loss:  5.5072550773620605\n",
      "################################  915  ################################\n",
      "Loss:  5.505650997161865\n",
      "################################  916  ################################\n",
      "Loss:  5.504000186920166\n",
      "################################  917  ################################\n",
      "Loss:  5.502367973327637\n",
      "################################  918  ################################\n",
      "Loss:  5.500580787658691\n",
      "################################  919  ################################\n",
      "Loss:  5.498108386993408\n",
      "################################  920  ################################\n",
      "Loss:  5.496349334716797\n",
      "################################  921  ################################\n",
      "Loss:  5.494532108306885\n",
      "################################  922  ################################\n",
      "Loss:  5.49255895614624\n",
      "################################  923  ################################\n",
      "Loss:  5.490755081176758\n",
      "################################  924  ################################\n",
      "Loss:  5.488873481750488\n",
      "################################  925  ################################\n",
      "Loss:  5.486909866333008\n",
      "################################  926  ################################\n",
      "Loss:  5.485106468200684\n",
      "################################  927  ################################\n",
      "Loss:  5.48344612121582\n",
      "################################  928  ################################\n",
      "Loss:  5.481781482696533\n",
      "################################  929  ################################\n",
      "Loss:  5.480152130126953\n",
      "################################  930  ################################\n",
      "Loss:  5.478597640991211\n",
      "################################  931  ################################\n",
      "Loss:  5.47709321975708\n",
      "################################  932  ################################\n",
      "Loss:  5.475656032562256\n",
      "################################  933  ################################\n",
      "Loss:  5.474322319030762\n",
      "################################  934  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.47305965423584\n",
      "################################  935  ################################\n",
      "Loss:  5.471864223480225\n",
      "################################  936  ################################\n",
      "Loss:  5.470722198486328\n",
      "################################  937  ################################\n",
      "Loss:  5.469638824462891\n",
      "################################  938  ################################\n",
      "Loss:  5.468575954437256\n",
      "################################  939  ################################\n",
      "Loss:  5.467545986175537\n",
      "################################  940  ################################\n",
      "Loss:  5.466516494750977\n",
      "################################  941  ################################\n",
      "Loss:  5.465468883514404\n",
      "################################  942  ################################\n",
      "Loss:  5.464369773864746\n",
      "################################  943  ################################\n",
      "Loss:  5.463167190551758\n",
      "################################  944  ################################\n",
      "Loss:  5.461981773376465\n",
      "################################  945  ################################\n",
      "Loss:  5.460826873779297\n",
      "################################  946  ################################\n",
      "Loss:  5.45969820022583\n",
      "################################  947  ################################\n",
      "Loss:  5.458532810211182\n",
      "################################  948  ################################\n",
      "Loss:  5.457330226898193\n",
      "################################  949  ################################\n",
      "Loss:  5.456111907958984\n",
      "################################  950  ################################\n",
      "Loss:  5.454851150512695\n",
      "################################  951  ################################\n",
      "Loss:  5.453578472137451\n",
      "################################  952  ################################\n",
      "Loss:  5.452280044555664\n",
      "################################  953  ################################\n",
      "Loss:  5.450968265533447\n",
      "################################  954  ################################\n",
      "Loss:  5.449631214141846\n",
      "################################  955  ################################\n",
      "Loss:  5.448287487030029\n",
      "################################  956  ################################\n",
      "Loss:  5.446885585784912\n",
      "################################  957  ################################\n",
      "Loss:  5.445422649383545\n",
      "################################  958  ################################\n",
      "Loss:  5.443597316741943\n",
      "################################  959  ################################\n",
      "Loss:  5.441650390625\n",
      "################################  960  ################################\n",
      "Loss:  5.439196586608887\n",
      "################################  961  ################################\n",
      "Loss:  5.437131404876709\n",
      "################################  962  ################################\n",
      "Loss:  5.434883117675781\n",
      "################################  963  ################################\n",
      "Loss:  5.432071685791016\n",
      "################################  964  ################################\n",
      "Loss:  5.429366111755371\n",
      "################################  965  ################################\n",
      "Loss:  5.426544189453125\n",
      "################################  966  ################################\n",
      "Loss:  5.423593044281006\n",
      "################################  967  ################################\n",
      "Loss:  5.420695781707764\n",
      "################################  968  ################################\n",
      "Loss:  5.417961597442627\n",
      "################################  969  ################################\n",
      "Loss:  5.4152655601501465\n",
      "################################  970  ################################\n",
      "Loss:  5.412832260131836\n",
      "################################  971  ################################\n",
      "Loss:  5.410562038421631\n",
      "################################  972  ################################\n",
      "Loss:  5.408526420593262\n",
      "################################  973  ################################\n",
      "Loss:  5.406553745269775\n",
      "################################  974  ################################\n",
      "Loss:  5.4046711921691895\n",
      "################################  975  ################################\n",
      "Loss:  5.402655124664307\n",
      "################################  976  ################################\n",
      "Loss:  5.40053129196167\n",
      "################################  977  ################################\n",
      "Loss:  5.398430824279785\n",
      "################################  978  ################################\n",
      "Loss:  5.396605968475342\n",
      "################################  979  ################################\n",
      "Loss:  5.394924163818359\n",
      "################################  980  ################################\n",
      "Loss:  5.393406391143799\n",
      "################################  981  ################################\n",
      "Loss:  5.3919501304626465\n",
      "################################  982  ################################\n",
      "Loss:  5.390569686889648\n",
      "################################  983  ################################\n",
      "Loss:  5.389186382293701\n",
      "################################  984  ################################\n",
      "Loss:  5.3879876136779785\n",
      "################################  985  ################################\n",
      "Loss:  5.386717319488525\n",
      "################################  986  ################################\n",
      "Loss:  5.3853840827941895\n",
      "################################  987  ################################\n",
      "Loss:  5.384091377258301\n",
      "################################  988  ################################\n",
      "Loss:  5.382673740386963\n",
      "################################  989  ################################\n",
      "Loss:  5.38145112991333\n",
      "################################  990  ################################\n",
      "Loss:  5.380266189575195\n",
      "################################  991  ################################\n",
      "Loss:  5.379007339477539\n",
      "################################  992  ################################\n",
      "Loss:  5.377706050872803\n",
      "################################  993  ################################\n",
      "Loss:  5.376371383666992\n",
      "################################  994  ################################\n",
      "Loss:  5.375042915344238\n",
      "################################  995  ################################\n",
      "Loss:  5.373693466186523\n",
      "################################  996  ################################\n",
      "Loss:  5.3723464012146\n",
      "################################  997  ################################\n",
      "Loss:  5.370977878570557\n",
      "################################  998  ################################\n",
      "Loss:  5.369598388671875\n",
      "################################  999  ################################\n",
      "Loss:  5.3682050704956055\n",
      "################################  1000  ################################\n",
      "Loss:  5.366771221160889\n",
      "################################  1001  ################################\n",
      "Loss:  5.365325450897217\n",
      "################################  1002  ################################\n",
      "Loss:  5.363892555236816\n",
      "################################  1003  ################################\n",
      "Loss:  5.362466335296631\n",
      "################################  1004  ################################\n",
      "Loss:  5.361086845397949\n",
      "################################  1005  ################################\n",
      "Loss:  5.359772205352783\n",
      "################################  1006  ################################\n",
      "Loss:  5.358368873596191\n",
      "################################  1007  ################################\n",
      "Loss:  5.357051372528076\n",
      "################################  1008  ################################\n",
      "Loss:  5.355714321136475\n",
      "################################  1009  ################################\n",
      "Loss:  5.354305744171143\n",
      "################################  1010  ################################\n",
      "Loss:  5.3528828620910645\n",
      "################################  1011  ################################\n",
      "Loss:  5.351455211639404\n",
      "################################  1012  ################################\n",
      "Loss:  5.350029468536377\n",
      "################################  1013  ################################\n",
      "Loss:  5.348625659942627\n",
      "################################  1014  ################################\n",
      "Loss:  5.347217559814453\n",
      "################################  1015  ################################\n",
      "Loss:  5.345664978027344\n",
      "################################  1016  ################################\n",
      "Loss:  5.343559265136719\n",
      "################################  1017  ################################\n",
      "Loss:  5.342616081237793\n",
      "################################  1018  ################################\n",
      "Loss:  5.341672897338867\n",
      "################################  1019  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.340047836303711\n",
      "################################  1020  ################################\n",
      "Loss:  5.337905406951904\n",
      "################################  1021  ################################\n",
      "Loss:  5.335677146911621\n",
      "################################  1022  ################################\n",
      "Loss:  5.333754539489746\n",
      "################################  1023  ################################\n",
      "Loss:  5.331718921661377\n",
      "################################  1024  ################################\n",
      "Loss:  5.329344749450684\n",
      "################################  1025  ################################\n",
      "Loss:  5.327345371246338\n",
      "################################  1026  ################################\n",
      "Loss:  5.3252081871032715\n",
      "################################  1027  ################################\n",
      "Loss:  5.322749137878418\n",
      "################################  1028  ################################\n",
      "Loss:  5.3206706047058105\n",
      "################################  1029  ################################\n",
      "Loss:  5.318898677825928\n",
      "################################  1030  ################################\n",
      "Loss:  5.317198276519775\n",
      "################################  1031  ################################\n",
      "Loss:  5.31553316116333\n",
      "################################  1032  ################################\n",
      "Loss:  5.313940048217773\n",
      "################################  1033  ################################\n",
      "Loss:  5.312403678894043\n",
      "################################  1034  ################################\n",
      "Loss:  5.310939311981201\n",
      "################################  1035  ################################\n",
      "Loss:  5.309542179107666\n",
      "################################  1036  ################################\n",
      "Loss:  5.308199405670166\n",
      "################################  1037  ################################\n",
      "Loss:  5.306911468505859\n",
      "################################  1038  ################################\n",
      "Loss:  5.3052873611450195\n",
      "################################  1039  ################################\n",
      "Loss:  5.304407119750977\n",
      "################################  1040  ################################\n",
      "Loss:  5.302967548370361\n",
      "################################  1041  ################################\n",
      "Loss:  5.3011298179626465\n",
      "################################  1042  ################################\n",
      "Loss:  5.299251556396484\n",
      "################################  1043  ################################\n",
      "Loss:  5.297130107879639\n",
      "################################  1044  ################################\n",
      "Loss:  5.294808864593506\n",
      "################################  1045  ################################\n",
      "Loss:  5.292169094085693\n",
      "################################  1046  ################################\n",
      "Loss:  5.289587497711182\n",
      "################################  1047  ################################\n",
      "Loss:  5.286618232727051\n",
      "################################  1048  ################################\n",
      "Loss:  5.283724784851074\n",
      "################################  1049  ################################\n",
      "Loss:  5.281077861785889\n",
      "################################  1050  ################################\n",
      "Loss:  5.278443813323975\n",
      "################################  1051  ################################\n",
      "Loss:  5.275879383087158\n",
      "################################  1052  ################################\n",
      "Loss:  5.273503303527832\n",
      "################################  1053  ################################\n",
      "Loss:  5.27121639251709\n",
      "################################  1054  ################################\n",
      "Loss:  5.269135475158691\n",
      "################################  1055  ################################\n",
      "Loss:  5.26715087890625\n",
      "################################  1056  ################################\n",
      "Loss:  5.265388488769531\n",
      "################################  1057  ################################\n",
      "Loss:  5.263731479644775\n",
      "################################  1058  ################################\n",
      "Loss:  5.262292385101318\n",
      "################################  1059  ################################\n",
      "Loss:  5.26093864440918\n",
      "################################  1060  ################################\n",
      "Loss:  5.259734153747559\n",
      "################################  1061  ################################\n",
      "Loss:  5.258596420288086\n",
      "################################  1062  ################################\n",
      "Loss:  5.25743293762207\n",
      "################################  1063  ################################\n",
      "Loss:  5.256248950958252\n",
      "################################  1064  ################################\n",
      "Loss:  5.255152225494385\n",
      "################################  1065  ################################\n",
      "Loss:  5.254025459289551\n",
      "################################  1066  ################################\n",
      "Loss:  5.252881050109863\n",
      "################################  1067  ################################\n",
      "Loss:  5.251723766326904\n",
      "################################  1068  ################################\n",
      "Loss:  5.250589370727539\n",
      "################################  1069  ################################\n",
      "Loss:  5.249494552612305\n",
      "################################  1070  ################################\n",
      "Loss:  5.248123645782471\n",
      "################################  1071  ################################\n",
      "Loss:  5.2469964027404785\n",
      "################################  1072  ################################\n",
      "Loss:  5.245894908905029\n",
      "################################  1073  ################################\n",
      "Loss:  5.244706630706787\n",
      "################################  1074  ################################\n",
      "Loss:  5.243537425994873\n",
      "################################  1075  ################################\n",
      "Loss:  5.2423858642578125\n",
      "################################  1076  ################################\n",
      "Loss:  5.241279125213623\n",
      "################################  1077  ################################\n",
      "Loss:  5.240187168121338\n",
      "################################  1078  ################################\n",
      "Loss:  5.239101409912109\n",
      "################################  1079  ################################\n",
      "Loss:  5.238015651702881\n",
      "################################  1080  ################################\n",
      "Loss:  5.236947536468506\n",
      "################################  1081  ################################\n",
      "Loss:  5.23590087890625\n",
      "################################  1082  ################################\n",
      "Loss:  5.234894275665283\n",
      "################################  1083  ################################\n",
      "Loss:  5.233932971954346\n",
      "################################  1084  ################################\n",
      "Loss:  5.233012676239014\n",
      "################################  1085  ################################\n",
      "Loss:  5.23215913772583\n",
      "################################  1086  ################################\n",
      "Loss:  5.231366157531738\n",
      "################################  1087  ################################\n",
      "Loss:  5.230624675750732\n",
      "################################  1088  ################################\n",
      "Loss:  5.229909896850586\n",
      "################################  1089  ################################\n",
      "Loss:  5.229203224182129\n",
      "################################  1090  ################################\n",
      "Loss:  5.22850227355957\n",
      "################################  1091  ################################\n",
      "Loss:  5.2278056144714355\n",
      "################################  1092  ################################\n",
      "Loss:  5.227105617523193\n",
      "################################  1093  ################################\n",
      "Loss:  5.226396560668945\n",
      "################################  1094  ################################\n",
      "Loss:  5.225675106048584\n",
      "################################  1095  ################################\n",
      "Loss:  5.2249555587768555\n",
      "################################  1096  ################################\n",
      "Loss:  5.224187850952148\n",
      "################################  1097  ################################\n",
      "Loss:  5.223315715789795\n",
      "################################  1098  ################################\n",
      "Loss:  5.2224202156066895\n",
      "################################  1099  ################################\n",
      "Loss:  5.221602916717529\n",
      "################################  1100  ################################\n",
      "Loss:  5.220737934112549\n",
      "################################  1101  ################################\n",
      "Loss:  5.21953010559082\n",
      "################################  1102  ################################\n",
      "Loss:  5.21864128112793\n",
      "################################  1103  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.217504978179932\n",
      "################################  1104  ################################\n",
      "Loss:  5.216195583343506\n",
      "################################  1105  ################################\n",
      "Loss:  5.214869976043701\n",
      "################################  1106  ################################\n",
      "Loss:  5.213536262512207\n",
      "################################  1107  ################################\n",
      "Loss:  5.212207317352295\n",
      "################################  1108  ################################\n",
      "Loss:  5.210938453674316\n",
      "################################  1109  ################################\n",
      "Loss:  5.209729194641113\n",
      "################################  1110  ################################\n",
      "Loss:  5.208558082580566\n",
      "################################  1111  ################################\n",
      "Loss:  5.207432746887207\n",
      "################################  1112  ################################\n",
      "Loss:  5.20636510848999\n",
      "################################  1113  ################################\n",
      "Loss:  5.205358028411865\n",
      "################################  1114  ################################\n",
      "Loss:  5.20441198348999\n",
      "################################  1115  ################################\n",
      "Loss:  5.203557014465332\n",
      "################################  1116  ################################\n",
      "Loss:  5.202775478363037\n",
      "################################  1117  ################################\n",
      "Loss:  5.202059745788574\n",
      "################################  1118  ################################\n",
      "Loss:  5.201333522796631\n",
      "################################  1119  ################################\n",
      "Loss:  5.200654029846191\n",
      "################################  1120  ################################\n",
      "Loss:  5.200057029724121\n",
      "################################  1121  ################################\n",
      "Loss:  5.1993818283081055\n",
      "################################  1122  ################################\n",
      "Loss:  5.198671817779541\n",
      "################################  1123  ################################\n",
      "Loss:  5.197854995727539\n",
      "################################  1124  ################################\n",
      "Loss:  5.197037220001221\n",
      "################################  1125  ################################\n",
      "Loss:  5.19618034362793\n",
      "################################  1126  ################################\n",
      "Loss:  5.1954569816589355\n",
      "################################  1127  ################################\n",
      "Loss:  5.194589614868164\n",
      "################################  1128  ################################\n",
      "Loss:  5.19364595413208\n",
      "################################  1129  ################################\n",
      "Loss:  5.192450523376465\n",
      "################################  1130  ################################\n",
      "Loss:  5.191585063934326\n",
      "################################  1131  ################################\n",
      "Loss:  5.190579891204834\n",
      "################################  1132  ################################\n",
      "Loss:  5.189462661743164\n",
      "################################  1133  ################################\n",
      "Loss:  5.18828010559082\n",
      "################################  1134  ################################\n",
      "Loss:  5.186999797821045\n",
      "################################  1135  ################################\n",
      "Loss:  5.185688018798828\n",
      "################################  1136  ################################\n",
      "Loss:  5.184342384338379\n",
      "################################  1137  ################################\n",
      "Loss:  5.183017730712891\n",
      "################################  1138  ################################\n",
      "Loss:  5.181700706481934\n",
      "################################  1139  ################################\n",
      "Loss:  5.180447101593018\n",
      "################################  1140  ################################\n",
      "Loss:  5.179201126098633\n",
      "################################  1141  ################################\n",
      "Loss:  5.177985191345215\n",
      "################################  1142  ################################\n",
      "Loss:  5.176800727844238\n",
      "################################  1143  ################################\n",
      "Loss:  5.175670146942139\n",
      "################################  1144  ################################\n",
      "Loss:  5.174588680267334\n",
      "################################  1145  ################################\n",
      "Loss:  5.173573970794678\n",
      "################################  1146  ################################\n",
      "Loss:  5.172613620758057\n",
      "################################  1147  ################################\n",
      "Loss:  5.1717071533203125\n",
      "################################  1148  ################################\n",
      "Loss:  5.170856475830078\n",
      "################################  1149  ################################\n",
      "Loss:  5.170036315917969\n",
      "################################  1150  ################################\n",
      "Loss:  5.169259548187256\n",
      "################################  1151  ################################\n",
      "Loss:  5.168533802032471\n",
      "################################  1152  ################################\n",
      "Loss:  5.167835712432861\n",
      "################################  1153  ################################\n",
      "Loss:  5.167181015014648\n",
      "################################  1154  ################################\n",
      "Loss:  5.166551113128662\n",
      "################################  1155  ################################\n",
      "Loss:  5.165947437286377\n",
      "################################  1156  ################################\n",
      "Loss:  5.165356159210205\n",
      "################################  1157  ################################\n",
      "Loss:  5.16477108001709\n",
      "################################  1158  ################################\n",
      "Loss:  5.164186000823975\n",
      "################################  1159  ################################\n",
      "Loss:  5.163585186004639\n",
      "################################  1160  ################################\n",
      "Loss:  5.162957191467285\n",
      "################################  1161  ################################\n",
      "Loss:  5.162311553955078\n",
      "################################  1162  ################################\n",
      "Loss:  5.161651134490967\n",
      "################################  1163  ################################\n",
      "Loss:  5.160949230194092\n",
      "################################  1164  ################################\n",
      "Loss:  5.1601715087890625\n",
      "################################  1165  ################################\n",
      "Loss:  5.159273147583008\n",
      "################################  1166  ################################\n",
      "Loss:  5.158175468444824\n",
      "################################  1167  ################################\n",
      "Loss:  5.157103538513184\n",
      "################################  1168  ################################\n",
      "Loss:  5.155933380126953\n",
      "################################  1169  ################################\n",
      "Loss:  5.154719352722168\n",
      "################################  1170  ################################\n",
      "Loss:  5.153502941131592\n",
      "################################  1171  ################################\n",
      "Loss:  5.152202129364014\n",
      "################################  1172  ################################\n",
      "Loss:  5.150754928588867\n",
      "################################  1173  ################################\n",
      "Loss:  5.1494364738464355\n",
      "################################  1174  ################################\n",
      "Loss:  5.148159027099609\n",
      "################################  1175  ################################\n",
      "Loss:  5.14666748046875\n",
      "################################  1176  ################################\n",
      "Loss:  5.145107746124268\n",
      "################################  1177  ################################\n",
      "Loss:  5.14394474029541\n",
      "################################  1178  ################################\n",
      "Loss:  5.142644882202148\n",
      "################################  1179  ################################\n",
      "Loss:  5.141138553619385\n",
      "################################  1180  ################################\n",
      "Loss:  5.139932632446289\n",
      "################################  1181  ################################\n",
      "Loss:  5.138797283172607\n",
      "################################  1182  ################################\n",
      "Loss:  5.137604713439941\n",
      "################################  1183  ################################\n",
      "Loss:  5.1364593505859375\n",
      "################################  1184  ################################\n",
      "Loss:  5.135375499725342\n",
      "################################  1185  ################################\n",
      "Loss:  5.134357929229736\n",
      "################################  1186  ################################\n",
      "Loss:  5.133395195007324\n",
      "################################  1187  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.132529258728027\n",
      "################################  1188  ################################\n",
      "Loss:  5.131740093231201\n",
      "################################  1189  ################################\n",
      "Loss:  5.131031036376953\n",
      "################################  1190  ################################\n",
      "Loss:  5.130378723144531\n",
      "################################  1191  ################################\n",
      "Loss:  5.129773139953613\n",
      "################################  1192  ################################\n",
      "Loss:  5.1291704177856445\n",
      "################################  1193  ################################\n",
      "Loss:  5.128529071807861\n",
      "################################  1194  ################################\n",
      "Loss:  5.1279730796813965\n",
      "################################  1195  ################################\n",
      "Loss:  5.1274871826171875\n",
      "################################  1196  ################################\n",
      "Loss:  5.1270246505737305\n",
      "################################  1197  ################################\n",
      "Loss:  5.126554489135742\n",
      "################################  1198  ################################\n",
      "Loss:  5.126063823699951\n",
      "################################  1199  ################################\n",
      "Loss:  5.125565052032471\n",
      "################################  1200  ################################\n",
      "Loss:  5.125065326690674\n",
      "################################  1201  ################################\n",
      "Loss:  5.124573230743408\n",
      "################################  1202  ################################\n",
      "Loss:  5.124054908752441\n",
      "################################  1203  ################################\n",
      "Loss:  5.123537540435791\n",
      "################################  1204  ################################\n",
      "Loss:  5.123033046722412\n",
      "################################  1205  ################################\n",
      "Loss:  5.122550964355469\n",
      "################################  1206  ################################\n",
      "Loss:  5.12204647064209\n",
      "################################  1207  ################################\n",
      "Loss:  5.121538162231445\n",
      "################################  1208  ################################\n",
      "Loss:  5.12100887298584\n",
      "################################  1209  ################################\n",
      "Loss:  5.120442867279053\n",
      "################################  1210  ################################\n",
      "Loss:  5.1198320388793945\n",
      "################################  1211  ################################\n",
      "Loss:  5.119214057922363\n",
      "################################  1212  ################################\n",
      "Loss:  5.118483543395996\n",
      "################################  1213  ################################\n",
      "Loss:  5.117488384246826\n",
      "################################  1214  ################################\n",
      "Loss:  5.116448402404785\n",
      "################################  1215  ################################\n",
      "Loss:  5.115374565124512\n",
      "################################  1216  ################################\n",
      "Loss:  5.114445686340332\n",
      "################################  1217  ################################\n",
      "Loss:  5.1134490966796875\n",
      "################################  1218  ################################\n",
      "Loss:  5.112380027770996\n",
      "################################  1219  ################################\n",
      "Loss:  5.111319541931152\n",
      "################################  1220  ################################\n",
      "Loss:  5.1101884841918945\n",
      "################################  1221  ################################\n",
      "Loss:  5.109053611755371\n",
      "################################  1222  ################################\n",
      "Loss:  5.107901573181152\n",
      "################################  1223  ################################\n",
      "Loss:  5.106777191162109\n",
      "################################  1224  ################################\n",
      "Loss:  5.105671405792236\n",
      "################################  1225  ################################\n",
      "Loss:  5.104597091674805\n",
      "################################  1226  ################################\n",
      "Loss:  5.1035027503967285\n",
      "################################  1227  ################################\n",
      "Loss:  5.102590560913086\n",
      "################################  1228  ################################\n",
      "Loss:  5.10166072845459\n",
      "################################  1229  ################################\n",
      "Loss:  5.100800514221191\n",
      "################################  1230  ################################\n",
      "Loss:  5.099986553192139\n",
      "################################  1231  ################################\n",
      "Loss:  5.099213600158691\n",
      "################################  1232  ################################\n",
      "Loss:  5.098564624786377\n",
      "################################  1233  ################################\n",
      "Loss:  5.097963333129883\n",
      "################################  1234  ################################\n",
      "Loss:  5.09739875793457\n",
      "################################  1235  ################################\n",
      "Loss:  5.096697807312012\n",
      "################################  1236  ################################\n",
      "Loss:  5.09616231918335\n",
      "################################  1237  ################################\n",
      "Loss:  5.095261096954346\n",
      "################################  1238  ################################\n",
      "Loss:  5.094516277313232\n",
      "################################  1239  ################################\n",
      "Loss:  5.093643665313721\n",
      "################################  1240  ################################\n",
      "Loss:  5.092579364776611\n",
      "################################  1241  ################################\n",
      "Loss:  5.091482162475586\n",
      "################################  1242  ################################\n",
      "Loss:  5.090000629425049\n",
      "################################  1243  ################################\n",
      "Loss:  5.08889627456665\n",
      "################################  1244  ################################\n",
      "Loss:  5.087258338928223\n",
      "################################  1245  ################################\n",
      "Loss:  5.086045742034912\n",
      "################################  1246  ################################\n",
      "Loss:  5.084662914276123\n",
      "################################  1247  ################################\n",
      "Loss:  5.083009719848633\n",
      "################################  1248  ################################\n",
      "Loss:  5.081178665161133\n",
      "################################  1249  ################################\n",
      "Loss:  5.07866907119751\n",
      "################################  1250  ################################\n",
      "Loss:  5.0765533447265625\n",
      "################################  1251  ################################\n",
      "Loss:  5.074426174163818\n",
      "################################  1252  ################################\n",
      "Loss:  5.071824550628662\n",
      "################################  1253  ################################\n",
      "Loss:  5.069207668304443\n",
      "################################  1254  ################################\n",
      "Loss:  5.066745281219482\n",
      "################################  1255  ################################\n",
      "Loss:  5.064260482788086\n",
      "################################  1256  ################################\n",
      "Loss:  5.0618672370910645\n",
      "################################  1257  ################################\n",
      "Loss:  5.059615135192871\n",
      "################################  1258  ################################\n",
      "Loss:  5.057581901550293\n",
      "################################  1259  ################################\n",
      "Loss:  5.055731296539307\n",
      "################################  1260  ################################\n",
      "Loss:  5.054060459136963\n",
      "################################  1261  ################################\n",
      "Loss:  5.05250883102417\n",
      "################################  1262  ################################\n",
      "Loss:  5.051089763641357\n",
      "################################  1263  ################################\n",
      "Loss:  5.0497612953186035\n",
      "################################  1264  ################################\n",
      "Loss:  5.048558712005615\n",
      "################################  1265  ################################\n",
      "Loss:  5.047403812408447\n",
      "################################  1266  ################################\n",
      "Loss:  5.046192646026611\n",
      "################################  1267  ################################\n",
      "Loss:  5.045266628265381\n",
      "################################  1268  ################################\n",
      "Loss:  5.0442304611206055\n",
      "################################  1269  ################################\n",
      "Loss:  5.0431013107299805\n",
      "################################  1270  ################################\n",
      "Loss:  5.042088985443115\n",
      "################################  1271  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.040938377380371\n",
      "################################  1272  ################################\n",
      "Loss:  5.039690017700195\n",
      "################################  1273  ################################\n",
      "Loss:  5.038422107696533\n",
      "################################  1274  ################################\n",
      "Loss:  5.037041187286377\n",
      "################################  1275  ################################\n",
      "Loss:  5.03580904006958\n",
      "################################  1276  ################################\n",
      "Loss:  5.034656524658203\n",
      "################################  1277  ################################\n",
      "Loss:  5.033303737640381\n",
      "################################  1278  ################################\n",
      "Loss:  5.032153606414795\n",
      "################################  1279  ################################\n",
      "Loss:  5.0308685302734375\n",
      "################################  1280  ################################\n",
      "Loss:  5.029524803161621\n",
      "################################  1281  ################################\n",
      "Loss:  5.028174877166748\n",
      "################################  1282  ################################\n",
      "Loss:  5.026829242706299\n",
      "################################  1283  ################################\n",
      "Loss:  5.025498390197754\n",
      "################################  1284  ################################\n",
      "Loss:  5.0243611335754395\n",
      "################################  1285  ################################\n",
      "Loss:  5.023196697235107\n",
      "################################  1286  ################################\n",
      "Loss:  5.021847248077393\n",
      "################################  1287  ################################\n",
      "Loss:  5.020711898803711\n",
      "################################  1288  ################################\n",
      "Loss:  5.019600868225098\n",
      "################################  1289  ################################\n",
      "Loss:  5.018476963043213\n",
      "################################  1290  ################################\n",
      "Loss:  5.0173726081848145\n",
      "################################  1291  ################################\n",
      "Loss:  5.016294002532959\n",
      "################################  1292  ################################\n",
      "Loss:  5.015178680419922\n",
      "################################  1293  ################################\n",
      "Loss:  5.014161586761475\n",
      "################################  1294  ################################\n",
      "Loss:  5.013175964355469\n",
      "################################  1295  ################################\n",
      "Loss:  5.012124538421631\n",
      "################################  1296  ################################\n",
      "Loss:  5.011202812194824\n",
      "################################  1297  ################################\n",
      "Loss:  5.010299205780029\n",
      "################################  1298  ################################\n",
      "Loss:  5.009450912475586\n",
      "################################  1299  ################################\n",
      "Loss:  5.008611679077148\n",
      "################################  1300  ################################\n",
      "Loss:  5.007824420928955\n",
      "################################  1301  ################################\n",
      "Loss:  5.007049560546875\n",
      "################################  1302  ################################\n",
      "Loss:  5.006323337554932\n",
      "################################  1303  ################################\n",
      "Loss:  5.005602836608887\n",
      "################################  1304  ################################\n",
      "Loss:  5.004937648773193\n",
      "################################  1305  ################################\n",
      "Loss:  5.00423526763916\n",
      "################################  1306  ################################\n",
      "Loss:  5.003572940826416\n",
      "################################  1307  ################################\n",
      "Loss:  5.002931594848633\n",
      "################################  1308  ################################\n",
      "Loss:  5.00227689743042\n",
      "################################  1309  ################################\n",
      "Loss:  5.001662254333496\n",
      "################################  1310  ################################\n",
      "Loss:  5.0010457038879395\n",
      "################################  1311  ################################\n",
      "Loss:  5.0004119873046875\n",
      "################################  1312  ################################\n",
      "Loss:  4.999761581420898\n",
      "################################  1313  ################################\n",
      "Loss:  4.999135971069336\n",
      "################################  1314  ################################\n",
      "Loss:  4.998510360717773\n",
      "################################  1315  ################################\n",
      "Loss:  4.997898101806641\n",
      "################################  1316  ################################\n",
      "Loss:  4.997242450714111\n",
      "################################  1317  ################################\n",
      "Loss:  4.996563911437988\n",
      "################################  1318  ################################\n",
      "Loss:  4.995874404907227\n",
      "################################  1319  ################################\n",
      "Loss:  4.995198726654053\n",
      "################################  1320  ################################\n",
      "Loss:  4.994538307189941\n",
      "################################  1321  ################################\n",
      "Loss:  4.993866443634033\n",
      "################################  1322  ################################\n",
      "Loss:  4.99318790435791\n",
      "################################  1323  ################################\n",
      "Loss:  4.992527484893799\n",
      "################################  1324  ################################\n",
      "Loss:  4.9918646812438965\n",
      "################################  1325  ################################\n",
      "Loss:  4.991240501403809\n",
      "################################  1326  ################################\n",
      "Loss:  4.990614414215088\n",
      "################################  1327  ################################\n",
      "Loss:  4.989900588989258\n",
      "################################  1328  ################################\n",
      "Loss:  4.989235877990723\n",
      "################################  1329  ################################\n",
      "Loss:  4.988609790802002\n",
      "################################  1330  ################################\n",
      "Loss:  4.987977504730225\n",
      "################################  1331  ################################\n",
      "Loss:  4.987363815307617\n",
      "################################  1332  ################################\n",
      "Loss:  4.986730098724365\n",
      "################################  1333  ################################\n",
      "Loss:  4.986088752746582\n",
      "################################  1334  ################################\n",
      "Loss:  4.985438346862793\n",
      "################################  1335  ################################\n",
      "Loss:  4.984918117523193\n",
      "################################  1336  ################################\n",
      "Loss:  4.984403610229492\n",
      "################################  1337  ################################\n",
      "Loss:  4.983855247497559\n",
      "################################  1338  ################################\n",
      "Loss:  4.983443260192871\n",
      "################################  1339  ################################\n",
      "Loss:  4.9830403327941895\n",
      "################################  1340  ################################\n",
      "Loss:  4.982546329498291\n",
      "################################  1341  ################################\n",
      "Loss:  4.982146263122559\n",
      "################################  1342  ################################\n",
      "Loss:  4.98167085647583\n",
      "################################  1343  ################################\n",
      "Loss:  4.981216907501221\n",
      "################################  1344  ################################\n",
      "Loss:  4.980682373046875\n",
      "################################  1345  ################################\n",
      "Loss:  4.980131149291992\n",
      "################################  1346  ################################\n",
      "Loss:  4.979513168334961\n",
      "################################  1347  ################################\n",
      "Loss:  4.979018688201904\n",
      "################################  1348  ################################\n",
      "Loss:  4.978549480438232\n",
      "################################  1349  ################################\n",
      "Loss:  4.978060245513916\n",
      "################################  1350  ################################\n",
      "Loss:  4.977574348449707\n",
      "################################  1351  ################################\n",
      "Loss:  4.977108955383301\n",
      "################################  1352  ################################\n",
      "Loss:  4.976620197296143\n",
      "################################  1353  ################################\n",
      "Loss:  4.976104736328125\n",
      "################################  1354  ################################\n",
      "Loss:  4.9756903648376465\n",
      "################################  1355  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  4.975066661834717\n",
      "################################  1356  ################################\n",
      "Loss:  4.974427700042725\n",
      "################################  1357  ################################\n",
      "Loss:  4.97362756729126\n",
      "################################  1358  ################################\n",
      "Loss:  4.972786903381348\n",
      "################################  1359  ################################\n",
      "Loss:  4.971896648406982\n",
      "################################  1360  ################################\n",
      "Loss:  4.970898628234863\n",
      "################################  1361  ################################\n",
      "Loss:  4.969926834106445\n",
      "################################  1362  ################################\n",
      "Loss:  4.968915939331055\n",
      "################################  1363  ################################\n",
      "Loss:  4.967877388000488\n",
      "################################  1364  ################################\n",
      "Loss:  4.966888427734375\n",
      "################################  1365  ################################\n",
      "Loss:  4.965925693511963\n",
      "################################  1366  ################################\n",
      "Loss:  4.965014934539795\n",
      "################################  1367  ################################\n",
      "Loss:  4.964169979095459\n",
      "################################  1368  ################################\n",
      "Loss:  4.963379383087158\n",
      "################################  1369  ################################\n",
      "Loss:  4.962642669677734\n",
      "################################  1370  ################################\n",
      "Loss:  4.961950302124023\n",
      "################################  1371  ################################\n",
      "Loss:  4.961309909820557\n",
      "################################  1372  ################################\n",
      "Loss:  4.960729598999023\n",
      "################################  1373  ################################\n",
      "Loss:  4.960158824920654\n",
      "################################  1374  ################################\n",
      "Loss:  4.959609508514404\n",
      "################################  1375  ################################\n",
      "Loss:  4.959054470062256\n",
      "################################  1376  ################################\n",
      "Loss:  4.958466053009033\n",
      "################################  1377  ################################\n",
      "Loss:  4.957695960998535\n",
      "################################  1378  ################################\n",
      "Loss:  4.957142353057861\n",
      "################################  1379  ################################\n",
      "Loss:  4.956556797027588\n",
      "################################  1380  ################################\n",
      "Loss:  4.956005573272705\n",
      "################################  1381  ################################\n",
      "Loss:  4.955496788024902\n",
      "################################  1382  ################################\n",
      "Loss:  4.954999923706055\n",
      "################################  1383  ################################\n",
      "Loss:  4.954497337341309\n",
      "################################  1384  ################################\n",
      "Loss:  4.9539923667907715\n",
      "################################  1385  ################################\n",
      "Loss:  4.953474521636963\n",
      "################################  1386  ################################\n",
      "Loss:  4.952953338623047\n",
      "################################  1387  ################################\n",
      "Loss:  4.952427864074707\n",
      "################################  1388  ################################\n",
      "Loss:  4.951898097991943\n",
      "################################  1389  ################################\n",
      "Loss:  4.9513421058654785\n",
      "################################  1390  ################################\n",
      "Loss:  4.950781345367432\n",
      "################################  1391  ################################\n",
      "Loss:  4.950270175933838\n",
      "################################  1392  ################################\n",
      "Loss:  4.9497599601745605\n",
      "################################  1393  ################################\n",
      "Loss:  4.949269771575928\n",
      "################################  1394  ################################\n",
      "Loss:  4.948798179626465\n",
      "################################  1395  ################################\n",
      "Loss:  4.948281764984131\n",
      "################################  1396  ################################\n",
      "Loss:  4.947761535644531\n",
      "################################  1397  ################################\n",
      "Loss:  4.947275161743164\n",
      "################################  1398  ################################\n",
      "Loss:  4.9468183517456055\n",
      "################################  1399  ################################\n",
      "Loss:  4.9463653564453125\n",
      "################################  1400  ################################\n",
      "Loss:  4.9459333419799805\n",
      "################################  1401  ################################\n",
      "Loss:  4.945497512817383\n",
      "################################  1402  ################################\n",
      "Loss:  4.945077419281006\n",
      "################################  1403  ################################\n",
      "Loss:  4.94465446472168\n",
      "################################  1404  ################################\n",
      "Loss:  4.944241523742676\n",
      "################################  1405  ################################\n",
      "Loss:  4.943827152252197\n",
      "################################  1406  ################################\n",
      "Loss:  4.943431377410889\n",
      "################################  1407  ################################\n",
      "Loss:  4.943023204803467\n",
      "################################  1408  ################################\n",
      "Loss:  4.942611217498779\n",
      "################################  1409  ################################\n",
      "Loss:  4.942196369171143\n",
      "################################  1410  ################################\n",
      "Loss:  4.941778659820557\n",
      "################################  1411  ################################\n",
      "Loss:  4.941340923309326\n",
      "################################  1412  ################################\n",
      "Loss:  4.940882682800293\n",
      "################################  1413  ################################\n",
      "Loss:  4.940401554107666\n",
      "################################  1414  ################################\n",
      "Loss:  4.93977165222168\n",
      "################################  1415  ################################\n",
      "Loss:  4.939357757568359\n",
      "################################  1416  ################################\n",
      "Loss:  4.938904285430908\n",
      "################################  1417  ################################\n",
      "Loss:  4.9383931159973145\n",
      "################################  1418  ################################\n",
      "Loss:  4.937861442565918\n",
      "################################  1419  ################################\n",
      "Loss:  4.9372944831848145\n",
      "################################  1420  ################################\n",
      "Loss:  4.9366774559021\n",
      "################################  1421  ################################\n",
      "Loss:  4.936030387878418\n",
      "################################  1422  ################################\n",
      "Loss:  4.93534517288208\n",
      "################################  1423  ################################\n",
      "Loss:  4.934610843658447\n",
      "################################  1424  ################################\n",
      "Loss:  4.933861255645752\n",
      "################################  1425  ################################\n",
      "Loss:  4.933050155639648\n",
      "################################  1426  ################################\n",
      "Loss:  4.932258129119873\n",
      "################################  1427  ################################\n",
      "Loss:  4.9315690994262695\n",
      "################################  1428  ################################\n",
      "Loss:  4.930848121643066\n",
      "################################  1429  ################################\n",
      "Loss:  4.930078983306885\n",
      "################################  1430  ################################\n",
      "Loss:  4.929309844970703\n",
      "################################  1431  ################################\n",
      "Loss:  4.92854118347168\n",
      "################################  1432  ################################\n",
      "Loss:  4.927773952484131\n",
      "################################  1433  ################################\n",
      "Loss:  4.9270453453063965\n",
      "################################  1434  ################################\n",
      "Loss:  4.926352500915527\n",
      "################################  1435  ################################\n",
      "Loss:  4.925694942474365\n",
      "################################  1436  ################################\n",
      "Loss:  4.925042629241943\n",
      "################################  1437  ################################\n",
      "Loss:  4.924431324005127\n",
      "################################  1438  ################################\n",
      "Loss:  4.923813343048096\n",
      "################################  1439  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  4.923158168792725\n",
      "################################  1440  ################################\n",
      "Loss:  4.922417640686035\n",
      "################################  1441  ################################\n",
      "Loss:  4.921768665313721\n",
      "################################  1442  ################################\n",
      "Loss:  4.921136379241943\n",
      "################################  1443  ################################\n",
      "Loss:  4.920490264892578\n",
      "################################  1444  ################################\n",
      "Loss:  4.919816017150879\n",
      "################################  1445  ################################\n",
      "Loss:  4.919093608856201\n",
      "################################  1446  ################################\n",
      "Loss:  4.918352127075195\n",
      "################################  1447  ################################\n",
      "Loss:  4.917632579803467\n",
      "################################  1448  ################################\n",
      "Loss:  4.916928291320801\n",
      "################################  1449  ################################\n",
      "Loss:  4.9161763191223145\n",
      "################################  1450  ################################\n",
      "Loss:  4.915452480316162\n",
      "################################  1451  ################################\n",
      "Loss:  4.914715766906738\n",
      "################################  1452  ################################\n",
      "Loss:  4.91399621963501\n",
      "################################  1453  ################################\n",
      "Loss:  4.913263320922852\n",
      "################################  1454  ################################\n",
      "Loss:  4.912571430206299\n",
      "################################  1455  ################################\n",
      "Loss:  4.911909103393555\n",
      "################################  1456  ################################\n",
      "Loss:  4.91121768951416\n",
      "################################  1457  ################################\n",
      "Loss:  4.910636901855469\n",
      "################################  1458  ################################\n",
      "Loss:  4.910061836242676\n",
      "################################  1459  ################################\n",
      "Loss:  4.909444808959961\n",
      "################################  1460  ################################\n",
      "Loss:  4.908768653869629\n",
      "################################  1461  ################################\n",
      "Loss:  4.908308029174805\n",
      "################################  1462  ################################\n",
      "Loss:  4.907878875732422\n",
      "################################  1463  ################################\n",
      "Loss:  4.907459735870361\n",
      "################################  1464  ################################\n",
      "Loss:  4.9070844650268555\n",
      "################################  1465  ################################\n",
      "Loss:  4.906732559204102\n",
      "################################  1466  ################################\n",
      "Loss:  4.906400203704834\n",
      "################################  1467  ################################\n",
      "Loss:  4.906095027923584\n",
      "################################  1468  ################################\n",
      "Loss:  4.905797958374023\n",
      "################################  1469  ################################\n",
      "Loss:  4.905505657196045\n",
      "################################  1470  ################################\n",
      "Loss:  4.905228137969971\n",
      "################################  1471  ################################\n",
      "Loss:  4.904956817626953\n",
      "################################  1472  ################################\n",
      "Loss:  4.90470027923584\n",
      "################################  1473  ################################\n",
      "Loss:  4.904450416564941\n",
      "################################  1474  ################################\n",
      "Loss:  4.9041972160339355\n",
      "################################  1475  ################################\n",
      "Loss:  4.903943061828613\n",
      "################################  1476  ################################\n",
      "Loss:  4.903692722320557\n",
      "################################  1477  ################################\n",
      "Loss:  4.903430461883545\n",
      "################################  1478  ################################\n",
      "Loss:  4.903161525726318\n",
      "################################  1479  ################################\n",
      "Loss:  4.90289306640625\n",
      "################################  1480  ################################\n",
      "Loss:  4.902593612670898\n",
      "################################  1481  ################################\n",
      "Loss:  4.9022722244262695\n",
      "################################  1482  ################################\n",
      "Loss:  4.901933193206787\n",
      "################################  1483  ################################\n",
      "Loss:  4.90156364440918\n",
      "################################  1484  ################################\n",
      "Loss:  4.901174068450928\n",
      "################################  1485  ################################\n",
      "Loss:  4.900781154632568\n",
      "################################  1486  ################################\n",
      "Loss:  4.900360584259033\n",
      "################################  1487  ################################\n",
      "Loss:  4.899932861328125\n",
      "################################  1488  ################################\n",
      "Loss:  4.899485111236572\n",
      "################################  1489  ################################\n",
      "Loss:  4.899024486541748\n",
      "################################  1490  ################################\n",
      "Loss:  4.8985466957092285\n",
      "################################  1491  ################################\n",
      "Loss:  4.898064613342285\n",
      "################################  1492  ################################\n",
      "Loss:  4.897582054138184\n",
      "################################  1493  ################################\n",
      "Loss:  4.897107124328613\n",
      "################################  1494  ################################\n",
      "Loss:  4.896641254425049\n",
      "################################  1495  ################################\n",
      "Loss:  4.896205902099609\n",
      "################################  1496  ################################\n",
      "Loss:  4.89577054977417\n",
      "################################  1497  ################################\n",
      "Loss:  4.895357608795166\n",
      "################################  1498  ################################\n",
      "Loss:  4.894917964935303\n",
      "################################  1499  ################################\n",
      "Loss:  4.894576549530029\n",
      "Training time: 2549.41 seconds\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1500\n",
    "start_time = time.time()\n",
    "history = fit(my_network, training_set, interior, n_epochs, optimizer_, p=2, verbose=True )\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Training time: {:.2f} seconds\".format(total_time))\n",
    "\n",
    "\n",
    "with open('PINN_10_noise_tl.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "\n",
    "f.close()\n",
    "\n",
    "model_state_dict = my_network.state_dict()\n",
    "\n",
    "# Save the model state dictionary to a file\n",
    "torch.save(model_state_dict, 'PINN_10_noise_tl.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a87c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading model\n",
    "\n",
    "# # Load the history from the pickle file\n",
    "# with open('PINN_EB.pkl', 'rb') as f:\n",
    "#     history = pickle.load(f)\n",
    "\n",
    "# # # Load the model architecture\n",
    "# # my_network = your_model_module.YourModelClass()  # Instantiate your model class\n",
    "\n",
    "# # Load the saved model state dictionary\n",
    "# model_state_dict = torch.load('PINN_EB.pth',  map_location=torch.device('cpu'))\n",
    "\n",
    "# # Load the model weights\n",
    "# my_network.load_state_dict(model_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf054d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Error Test:  64.63039517402649 %\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.linspace(0, 8*pi, 10000).reshape(-1,1)\n",
    "t_test = torch.ones((10000,1))\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution(x_test, t_test).reshape(-1,1)\n",
    "my_network = my_network.cpu()\n",
    "u_test_pred = my_network(test).reshape(-1,1)\n",
    "\n",
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((u_test_pred - u_test)**2)/torch.mean(u_test**2)\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c935f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0. , 0.2, 0.4, 0.6, 0.8, 1. ]),\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test = 8*pi*torch.rand(100000).reshape(-1,1)\n",
    "t_test = torch.rand(100000).reshape(-1,1)\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution(x_test,t_test).reshape(-1,1)\n",
    "u_test_pred = my_network(test).reshape(-1,1)\n",
    "relative_error = torch.abs(u_test_pred - u_test)\n",
    "u_test = u_test.reshape(-1,)\n",
    "\n",
    "# reshaping and detach numpy\n",
    "x_test = x_test.reshape(-1, )\n",
    "t_test = t_test.reshape(-1, )\n",
    "relative_error = relative_error.reshape(-1,)\n",
    "u_test_pred = u_test_pred.reshape(-1, )\n",
    "\n",
    "x_test = x_test.detach().numpy()\n",
    "t_test = t_test.detach().numpy()\n",
    "u_test_pred = u_test_pred.detach().numpy()\n",
    "relative_error = relative_error.detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "font_path = 'times-new-roman.ttf'\n",
    "\n",
    "custom_font = FontProperties(fname=font_path)\n",
    "\n",
    "\n",
    "        \n",
    "CS1 = plt.tricontourf(x_test, t_test, u_test_pred, 20, cmap='rainbow')\n",
    "#CS1 = plt.tricontourf(x_test, t_test, u_test, 20, cmap='rainbow')\n",
    "#CS1 = plt.tricontourf(x_test, t_test, relative_error, 20, cmap='rainbow')\n",
    "\n",
    "\n",
    "\n",
    "cbar1 = plt.colorbar(CS1)\n",
    "for t in cbar1.ax.get_yticklabels():\n",
    "    t.set_fontproperties(custom_font)\n",
    "    t.set_fontsize(20)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('x', fontsize=20, fontproperties=custom_font)\n",
    "plt.ylabel('t', fontsize=20, fontproperties=custom_font)\n",
    "plt.xticks(fontsize=20, fontproperties=custom_font)\n",
    "plt.yticks(fontsize=20, fontproperties=custom_font)\n",
    "#plt.savefig('PINN_EB.pdf', dpi = 300, bbox_inches = \"tight\")\n",
    "#plt.savefig('Exact_EB.pdf', dpi = 300, bbox_inches = \"tight\")\n",
    "#plt.savefig('Absolute_error_EB.pdf', dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "407f519f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0. , 0.2, 0.4, 0.6, 0.8, 1. ]),\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnkAAAEWCAYAAAAaQ4GzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAw0UlEQVR4nO3de7RcV30f8O/vXlmSAcn2lcxCli2kGGxIqroQU5bNmy4WrDTEoQWHV4DExIbYTVpamxgIhTrF0KQQMDbBJbiOSWtIuuKEVws2kAJd+CFcKqhZGCFbRpZrSRdLwg/J0v31j5mR5s49M3Me+/Hbe38/a2lde+bOzJbOPnt/z2+fc0ZUFURERESUl5nYDSAiIiIi9xjyiIiIiDLEkEdERESUIYY8IiIiogwx5BERERFlaFnsBsSw8gkn6KoTnwIA0EMLzt9fDx52/p5H3/vQoYnPHz7y6OTn5eDE55fp5Ny/YmZ2/HOzsuSx5cdV/+7s8qW/29WRQ+OvFD/0ePXjB49Uv+bgwpHan3tYpvehZbqi1nstmz1+6u/I8uW13gsAZEW7XXxaH47dD4HJfXHJ71b0zRjG9beJr2nQF6lak75y9DWR+0ybvnL0tS37TJ2xLGd1xp262vS5Kk/9h08DAGzZsmWPqp7c9PVFhrxVJz4Fv/7Wa/D4jskTUVsL2+e9vK8Lj+24p/LxPfu3LnlsfmbHov8/eWFxADl9xapF/7/phKXdaf266nasOc1919t73+RgsnPX0se276t+zbaDB5Y8tntmaX+ZW9hQ+fq1qzdXPr5yw8bxDQQws2lu4vMh1enH4/oTUN2nho32r2Gjfa3KaP+bpKpvhjaur01T1RfHqeqjk4zrvzFN6hej6vQToFlfGYjdZ9r2F6BZnxlo2nem8dW3mvSPNur2qWna9LlK9/d+bAHubfPyIpdr9dCCt4BH1JalgFfHpIA3TdeB2tkAGggDXj0+Al4bsQNeF20CXiqKC3gOFBnyiHwaV8VLieVqdGpCBDwaz9KEW1eXKl4brqt4PqQS8KxJ93ClIKMVHk7A1dactmzqkm1M05ZqczJtqXaSXAdbn1jFo1z5DneA2/5k7aCClTzjqpbwUlnWG3c+HpUtxKBtBat4cfFcvHhc7OcljRW+MOQZNinMpRL0qvi46MIlSwNlDF0vuKCeUAGvtCoe5W9+ZkewPpFzFQ9gyHPO1VJqyiEuRymcs0LjhT7HKdTnldgvc16qjVHF89mHmga1kOEOyLsvDTDkGVQ34DEIxmWxImKB7/PxLB4tD7O83Gaxz/qa1K33k1GhD0RCqbN9Q4c7H6z2N9vrZonhBRGUOt9LtakP5NOEDHis4rkX+3y8tqyfYjK8388tbDAxDpRQxQNYySMygVXZ9FmvxLCKZ1esvhPjQCHHgGe5vzHkOcJz8Yi6LdWmrOskzSredKVUXsiv0voRQ54DMZdpGQptSflGyLyqtp0UAl5JVbzUhO4/5JblKh7AkNdZaufhlXRDXkv47z59Uvd9hO1jScz6Em0umvSNtpNuqufjtcVqcBnK6tWOuQ54rMr5s3NX7BZQTlyFO1bxysYqXjg+Ap71Kh7ASl5rqVXwiCZxsVQb4ny8roOqi3AWK+CVKkQVj/JWYgVvgCGvoYXt894CHoMjkW0xl2dZxfMnxlItq3hpS+WAgsu1NTGAUa5C9O3Y5+MN277vcONJ3XW449W0ZYt9LmdJ/ankKh7AkDcRgx1RPandOqVu0PMxGYeqwORQxeNSLXXhK+Cl1NcY8kYw2KVr73280tGXHG+dMghwo2HPZ5WlTcDLZZmWelz0Ly7VTld6BW+g+JDHUOfPzl3A+nWxW0GTsP/HXzorhdUqHm+dQk2kVMUDCg15evAwJ7eI9t53GGtOS6frVQ2KrJTUZ/FE+xhYxSNW8cJgFe8YXl1rDMMnhVK3r/m+dUoJA3KogGeVzyoe0TCffSe1Kh7AkEe0CI+SKWUlVvFSmXh5WoB/PDhYiiGPisVBdzpLF1ykMpmPYhXP7nJ9qPPxrNxAO6d+Ncp3wEt1/GHIIyqQq9MCpi3VWp7gQwgZ8HKp4rEaQ+QOQ55BTSZgnsNHZBOX/v2H/BSqK1aqeDljFW88hjwiR9au3hy7CbW4uuDChVyrNm0nZFbx8uwP5A/7zGQMeUaxQkfWcanWBqsBz/r2D3E+nqUqXo7n44UIeClX8QCGPNOmBT0GQWqKfca/0FW8UqU++VI3rODVw5BXGJ9LcDGvVk39K82shS9LV9WmhMu07eU2afPq/fTlcCDBkGfcuMnfWihoKmQo27kr2EeZ5rLPdL0Bco54YvwxpV9w4TLgsV8tldsBgU8MeQkYnZxTD3iUvzqTfE4DdZeJmFW8vPoC+RWqr1g/kKiLIS8RC9vnj/6xos7ExiqaDU36DZdqw8kx4Fm/4MI3i1W8XM735MFAcwx5RJlzfWDApdrFuJwWlosKi68ra3kenj8hA14uVTyAIY8ayOVokMZjFa8ZLtMu1qaKx+oMTcM+0h5DHkXj6uKL1K+sHfCxFB9jeb+U5boYAY/scl3FY4W4J3TAy6mKBzDkUaG4rLJUnSqeq6Xa1I/MY03Alqt4bTTtB7lNwDQZA153RYY8PXSIy1K0RNXEnXLFxdJFOjnpGvByXKYFyqngVrFcxUt1DEv9QNAK/9/rMoWIzAJ4H4BXAFgAcBuAd6rqI1Ne9yIA7wUw2/8zD+Ddqvr9up9dFfRWbthY9+XkwN77DmPNadG7YXZiBbzcJ/pYAY/s4qpAHnKs4gE2Knk3AjgHwPNU9bkA5gDcJCIy7gUi8nwANwG4XFVfrKovAPB5AH8vIk/t0pjHdtxz9A+54fM2KtPOx/Px2T4rKrHCWcil2lTFPEcqxyoel2ppHFbx3Ika8kTkfACvBnCZqh7qP/weAC8DcMGEl74NwNdU9bbBA6r6KfQqer/mqn0MfM3xqDY+LtO6xy+Iz4fL26f4GO9Kv+AiRsDL+QAidiXvEgB7ANw5eEBVtwO4F8DFE163HMA/EJHjBg+IyDL0Qt6PfTSUYc+fXK6OdaVLSGvz2tD9OrWj9NgBz3oVr1Q8oHUvtbEhBdFCnoisAnAugLtVVUeevgvAWSJy4piX/ycAZwD4ZP+cPgC4EMBnVfXLPto7kHLYS7Xd4zAcLuazgldnqTbH8/EY8KbjvfHcKbmKF6tP5FzFA+JW8k5Fr/JWddbUPgACYFPVC1X1qwD+BYA3A/iKiLwZwM9V9a3jPkxELhSRO0TkjoNHHu7c+NwCU2wMbIs1DWxtAx778XglT7jWWJuIU6nipXKKAAOePzFD3lz/Z9VVtI/3f47d8qr6cQBXAHgAwH8G8Oz+ku24379WVc9W1bNXzD6xXYtHpFzVc6XuROj6AgiGwmNKOQcvZOiy8J2huVbxUpdKwEsFq7p+xQx5g9GvagsPHhs7e4nIhwDcoqpvAPCvAPwegL8WkeB/p9KDnisMbostbJ+fGOCmPT9N3X5b2lItA55fKU/qPgNeiZXjmH2hhCoeEDfkbev/XFPx3BoARwDcX/VCEfkdAC9R1W8CgKr+KYB/CeA8AL/luqF1MOiFUzcMjqsexjgS79I/RsNc13AXk/UJvsSJ1rpSJuPSWB8LchEt5KnqPgBbAJxZ8fTTAdyqqvvHvPwiHAuJg/f7GIA7ALzSZTubyC3ota0mdAlRrOaN5zLcuazi5cLKtwykUsUr7YILVvHcid0PSjpwiH0LlasBrBORswYPiMgZANYD+OTQY3Mjr3sYvSA4aieAqHtLbkEvhr33Ha4Me+Mep7hyWKplwKNJeB6eOwx4YcUOedcDuAXA5dKzDMCVAL4E4AYAEJFLAewVkdcMve6D6F1oceHgARE5B8ALAfxpoLaPZTHoWWhT04svBqGO4c4tC32hLR8VDysBj5ayMCEz4LkTO+ClqssYFTXkqeoCeufR7UPvO2u/DeCHAF41dO+8PQD2A3ho6HVf7r/uAhHZKiJfAfBOAC9W1S3h/gbjpTyRlqi05ZI6cl+q3XbwgKntnlIVr5Sl2hABz1If9OXkheNNbH8LBw1Nde0f0b8ZXlUfRu8cu3HPXwfguorHP4/e99Wa9diOe7Byw8bYzXBq98yjJnbWaXx+X27KXB98NJnsXfWbbQcPdB6sfUysXKZNw/Z9h2t9tRkreG6kMF9Y5WKcir1cm71SK3rjBkiGrzRYr+K1Hfx8Ve+4TJuXUAEv9yqepYCXWhXPVd9gyKPOch+oXIkd+GN/fkw+l2a7BrzUqnihLrSJNSmzgucGA157LscqhrwAYk+usT/fEp8DuOXqV5M+UPfvEfuq2jrBzdp5d6NSC3htWZrwgd44MDoWVD3mk+V+2ZW17Z0S1/0i+jl5MRw+8uiiiWzt6s3ePzPH8/Pa2rkLWL/O7/tTOWJOllymTRurdu5ZC3ipVfFcKzLkjQoV+Bj00pLLBG6hkmtt4HehtGVaIP+lWmrP4j6eWj/yccDK5doRe/Zv9brsZmHCDWXSUXKp1bbQ27/p56WyVBtbiQGvLYuTf2y5LdVyG3fnq08w5I3hO+yFUmeSz+HvOVBqeCQiisFqwEupiucz9DPkTeEj7OVYzcvtyDQHvqp4pSu1ild69daVXMZKKzc4rsKAdwxDXk0Meu1YWrK1cpJ1iG3v8zOaTvZWJ4I2Sg14bbXZ9ilN0Nb52vcs79Mp9Z8QgZ8hrwHXVT3fk72v97d6QULb0JjLkfVAm+3OKt50Vvs9USiWq3dUjSGvhZSCnnU8h45S4CLgpVzF41KtGykfUKYQ7ljFW4ohryXrlY+UwqOLoJdaWPS1fXxX8TjZt5NywGsrhVBQgq7bIZXqHQNeNYa8DlwFvZQC2STjOq6Vc+Fyl0s/sobn4YWT0kTdVMwqXpuQlkq4A9LqN6H7AUNeRxaDXoqTfZdKXJ3XhgqaTfqDhW3uu4qXyiQxDs/Do1zU2RcHwS71/daqGEGfIc8Bi0EvRW2CXmrLtKO6bvPHdtwTJOCVqPTz8Aa4RN+dlXPxqkJc6sEupSpeDAx5jlgJelaDYt1KWpPQlnrAG2i7zaxu6xww4HWTamAoScrBbiClgBcr6DPkORQ76IWc9MdNgi46cp3w5irgWTnCbrrtum7rpn21pGoOl2jjSGnCpvhS6i8x5xmGPMdiBb0UluyanBe3c9exP5Mec/25MdXZhl2WZ0NLvUrQRclVPFrMyoEkxRF7+y+L+umZ2rN/K9au3tz5fQaT+coNG2v9Xq5yWZatI8S2ZBVvPC7TLlbStqd0pFTFi42VvARMqt6kFvBSqap1YfmCBstti40Bz42SK7ijYldxcpRSwLOw/Yus5B2Wg5if2eF1QHZVzRuWSqDbdvCAmR2xhFDpUymVHJ6HF5eV8YJsS6mfWAh4QOGVvPmZHYv+uJZD1cRHEE4heOU46YfsjylVc1xta1bxaJiVST4XKQU8S4oOeaN8hL0cgt44OQahXLXphyVU8Rjwxivx5tdkU2oBz1LAZ8ir4Lq6l3PQG2daJw9RzZv2GW13xDr9wtI2t9QWSxjwyBdLkzyF5WPbdxmrGPKmyLmawcmfxsm53xNROlKr4lnDkFeDi6peiYEqZjUvhfP+QojR71JYskuxird29eajf3KS2yTOKp47qfUNa1U8gCGvEQa9pbp2QIYxf9r2t9yreKkFvKpg5zvo8Xw8ii21gGcVQ15DDHruuQ56VoJjzG3NflYtpYuFplXtcqvoEQ2kGPAsVvEAhrxWcq90uBR66aJuwMt5SaVLwMu5b7sMeL6reAxwacp5XAmFAa/H1XgVPeSJyKyIXCEit4vIrSJylYg8ocHrZ0TkfBH5LyLyIRF5o8/2DnQ5T89ClcVCG4ZZqb65FvrfOfZ2tbpkx4BHZF+KAc+66CEPwI0AzgHwPFV9LoA5ADeJiEx7oYicAuDrAF4L4BJVfaeqfsZra0ekHPRcmTaB1j3K6Rr0cg2KdXXtU7lW8XIOeD4CYch+kNOkzipeN6n2BctVPCByyBOR8wG8GsBlqnqo//B7ALwMwAVTXrsewLcBPAjg1ao677OtPuQU9FzZvu9wq7DW5DWhB+MQ25kBr1pq5+ClymoFl9KQasBLQexK3iUA9gC4c/CAqm4HcC+Ai8e9SESWAfg8et+9+9uquuC5nROlNkHGCJdNg1Xd0NY2FObEysFC7hO9rypejrdFKQ2reGWyXsUDeiEpChFZBeBcALepqo48fReAl4vIiar6UMXL3w7gWQB+X1VN7F3zMztaTQJ79m81P8DPLWyYGmR3zzzqfJKPFd58VH98bGdX4S61g5S6Ulimtb7vE/nGKp5fMSt5pwKYBbCr4rl9AATApjGvvQjAAoB5EblWRG4Tka+IyCvHfZiIXCgid4jIHYcX/IQHnp83mYWj3ZhtcLmdGfAmY8CjUCyMa6lKOeClUMUD4oa8uf7PRyqee7z/c0lpSETWAfglAPcBuEdVLwTw0v77/J2I/GbVh6nqtap6tqqevWzGXwHTetCLHShLHxC7/vvv2b81+jasYmmpNoXz8KwGPN4EmUJJOeD54GvcihnyBn+jqhFi8FjVxRSn9n/+T1X9FgCo6s8BvA3AIQAfdNnINqwHPV9SmFxd6VIBaxPUfIS7HKt4rvugjyqe1YAXWg6TfOkHrW2lvu1T2u4xQ962/s81Fc+tAXAEwP0Vzw3WWvcPP6iqDwD4DoBTRGRuyasCsxj0rITIWDuItR1zWnAbPO9ju7kMeFYqOSkEPKLSpR7wfPBZIIl24YWq7hORLQDOrHj66QBuVdX9Fc/dDeAggKoReFf/uX3OGhpBChdjjFP3AoxtBw9wZ+8LHb5ZwZuO5+HRNNYOGlOQw5if2naPfQuVqwGsE5GzBg+IyBkA1gP45NBjRytz/aXZzwF4iYicNPJ+vwDgi6p6ZNKHLtMZnLxwfOUfl7ou6bnU9f1Sr2qktmP64jrgWajipXKaAANePjieNJdDwPPB9/gVO+RdD+AWAJdLzzIAVwL4EoAbAEBELgWwV0ReM/S6ywDsBfAxEZnt/97r0At5l3ZpkOvAZynohVK303KgDCvHCp4PpZ6Hx4suyJdcAl6Kc1bUkNe/ifF56C2v3obeN1j8EMCrhu6dtwe98+8eGnrdAwCeD2A5gC0i8i30vjnjHFX9iav2uQp7sYOe5bAYYqdp8hmpVIWa8hHwLEzwKSzTphDwqL4UJ/qYcgl4PoSYb6Kdkzegqg+jd9+7cc9fB+C6isd/CuA3PDbtqMFkFisAdDlHL1bAa3JzZJ/n5/kckNveADu0XCt4DHhEtuUU8FIN97GXa5PSpXLRdaJtE9YsV/BGpboDWecr4MWu4qVQcWXAGy/VyZ/jVH2pbuNQQo1hDHkNxQ56dYKbr9tuNK10NO3E2w4ecDqIlj4gM+DV57qKx4BHJcst4KU8l0Rfrk1Rl+VbF0t8owFu7erNZqt2bb7Ttuvybco7pCtcoq2PAY8XXdTBcaWe3AKeDyFXIljJ68DKIGc14HXRtqrXZSBuG9qt8dkmK33eqhQDHk3HgFcPA549rOR1dPLC8Y3DQSon7FeZW9jQOES0qeYNjA6uVYMIB+Bjcg54KVTxiEqVa8BLfX5hyHOgtKDXRpegN8ziDmdhW/quKDLgTccqXp4sjjnW5BrwfGgzlvXGqttafR6Xax1pMwlaXOqj9DDgNceAR+RGzgEvh4DPkOdQ7MkwlLYTpOXbXnRtW4zAPj+zgwcKBpQY8EoZ63KY5H3KOeD50L6K1x5DXmSlTdKWg15KQvWb2JO59SpeDgGvtDGoLga8yRjw0sCQ51gpy7ZdJspcg16I7RiyeseAN1kOAY+ojRICnuuQH6OKBzDkeVFK0OvCUtBz2RZf2zF0uIsd8IhiYhVvvBICXk54da0nba64TU2b26kMG/z75BYoBv8mXY/CYgR/K9uCVTyKhQFvPAa8dmJV8YCGlTwRefGE584WERszhBFNJ8wUq3kuOmLMMOzzs9tsz0HFjgHPLQY8qoMBb7ySAl5O/aBpJe+9AL4x5rnv9Z//wy4NKp2Fe67F4Oo+ek0/07dUgruVgGcdAx77SolKCni5mRryROSFQ/97ooi8AIBU/Op6AG8GQ94iJdwoueuy7UDI5dvcl9KbsDRpW6/i5SaVg5AQcqreuMSA103MpVqgXiVvFsC/B/Dc/v9/Y8Lv3tSxPUGsmJmt1XFd7fQMes34DnsMeD2Wwp0vXKalOhjwqpUY8HLrC1NDnqp+vV+9uxrAiwB8sOrXAOwF8FW3zYtruIN33fC8EKM5H2Ev921Ql8WAZ3nbMODlK7dJ3ZUSA55rsat4QM1z8lT1CIC3icilqnq90xYkYtDhQw4IqVXzAPdBD1i8o7QNJ5YDREgWw50vrvYdBrx8MeBVY8DLR6Ora1X1j301JBVdOn8p98/zGUx3zzy66E/d3y2d9XvfcRtRaAx41UoOeDn2Cd4nr4UuVb0Szs8D/FT0qjAcTGc53PnCKh5RcyUHPNcsLNUC/MaLTtruECVV9FILpzmxXr0bcB3UGfD8cdWfYldMYn++RQx4eWLI6yjkjpFi0AN4C4vQUgl3lpUQ8FIdT7piwFuKAc9tv7BSxQMY8pxos4O0nYRTHZhZ1fOP4Y4HFDQZA95SDHh5Y8hzhEGvHoY991IOdxbPqSyhileabQcPMOBVYMDLH0OeQ1y6rY9Br7uUw50PLvpUagFvz/6tsZvQWqjQxXBXjQHPD0tLtQBDnnNNd5wuk3QOQY9hrzmGO6J6GPCqMeAtlnM/YcjzgEGvGYa96QbBLqdw53KptsQqXg58Tq45T9xdMOCVhffJy0CK99GrMvg75BBcXckp1FnGgJcPhrvxGPD8srZUC7CS503Iah6QVzAaVPZyCK5t5Va186nkftKWpfHCZShjwBuPAa9MDHkeMeh1V1LYy3FJdhxXS7Vcps1D13DGq2cnY8AbL2a/CTG3RQ95IjIrIleIyO0icquIXCUiT2j4Hk8TkX0i8hZPzWwt9M6VY9AD8g57pQQ7ixjw6vPdR9tMtgx30zHghWHxdlCAjXPybgRwEoDnqeohEflLADeJyMtVVae9WERWAvgcgNWe2xlEm++2HZXLOXpVhv9eqQdaBrtucu3jJRsObFXhhIGOSrR29WZg/q9bvTZqyBOR8wG8GsAvq+qh/sPvAfATABcA+FSNt/kogK8DeJaXRjpw+opVjQYnBr16Ur1Qg+HOBlbxbGOg645VPLtCzc+xl2svAbAHwJ2DB1R1O4B7AVw87cUi8kYAPwPweV8NdCX0+XlAeuGnrVSWcrks22N1WYMoJwx407k6kLA8pkULeSKyCsC5AO6uWJa9C8BZInLihNf/IoDXA3i3t0Y6FmOnKyXoAXbDHsOde123M6t4lDMGvHx0HatiLteeCmAWwK6K5/YBEACbMFTlGxCRJwK4GsDrVfWIiEz9MBG5EMCFAPCk2aUT7qYTxv9TbN93eOr7++Bi2RYoY+l2mJVlXAY7mxjwKGcMeDQsZsib6/98pOK5x/s/x82S1wC4QlWrAmIlVb0WwLUAcPLyE3VSqBs1/LtdA1+M8/NKNbewIVrQY8Dzp6QDFh9iH/yQPwx4aQg5hsU8J2+QXKpmw8Fj86NPiMhF6C3xfq3tB6+YnV75G2fTCcsmVv3q4LJtODGWcBnw7GIVj3LFgNdMCufjuRivYoa8bf2fayqeWwPgCID7K557J4D3icjhwR8At/Sf+/P+Y09139zFuga9JhgaugsV9LitJus6ILKKR7QUAx6NE225VlX3icgWAGdWPP10ALeq6v6K514BYPnIY88B8GkA7wXwt6gOh84Ngl6bJdySbqsy7Whkz/6tQdrhe/mWAc82VvGIKLbQc3DsmyFfDeDTInKWqn4PAETkDADrAbxr8EsiMqeq8wCgqj8afRMRWdv/z52q+n3/zV5s0wnLggS9lDSZUEd/12fo8xX0rAW8aUf2ufa7koQ6OCLbWMWLJ4Xz5WPfJ+969JZaL5eeZQCuBPAlADcAgIhcCmCviLwmXjOnC7F8m8q987pWTNau3nz0TwpiB7zTV6xa8qfJa1LR5Qg4lb5E1ERK+y8142rMihryVHUBwHno3TLlNgDfBvBDAK8aunfeHgD7ATwUo41NtAl6MW6S7JPrydRH2Ev9vK6mga7u+xFROrjPthdrJSPG3BN7uRaq+jCAiyY8fx2A66a8xzfQu69edG2XbnPgs1oyeG9XS1Sulm1Dhe4QA/rgMywu5bKKR3QMAx7VFXu5lhC+mudjyTbURJrSMq4LMapsvj4vhfNXiKxjwLPB+q1TBhjyPAixbGtJjNDl4jO7ls59VvG4hOpGSQcERESjGPI88X0hhvVz80LIsapnJdxZaMNAKudQrtywESs3bIzdjIlKvSl6Liztl9RMrHGMIc+jpkEvxR3YQsiy0IaurIS7Ydba01SofjEa7lIIe5Se1PdHioMhzzOfFb0u1bzcjujbTuhtj65cVlI5eKdpWphj0CNXOEa44+LCslTOxwMY8sxJ6ZYq1ipo1tozjcXq3ajY7Wsbwn32BVbqiCgVDHkBlLBsa0UqQa+0bZzLlbVNw11OYTCXbZia0saKHMU8r5ghLxCry7Y5ahr0Qu+AqQ3aqbXXV9DPKbBRGlLb98gehjyjrO/cqVTMfOgSqq1vV0ssXVXbJeAxHBLlI6Xz8QCGvKBCfL9tE7ldfDHMYghlwCNXXH3zC9nF8YJcYMgzrMlOziXbpSwFvdQH7FTa72ObsxJHoaWyv6Umxlc2xl6RsFVaKkDJ3207zrhJ9LEd9wRtB9EoVwFv5YaN7M9UCwMeucRKXgRNlm0tVvNcVUvq3Ges6+0q6rbV59EWB+3m2mwP11U8VvCW4hW2RH74WnliyKPg2gQ3KxNu0yDNgEdEdXG8sC3Fgxwu1w5Zv2767+zc5eazmizbnr5iVZRzCXxwcZVi02Wvtas380R1B3Lqh9NYOaggIuqi+Ere+nXH/jT5fassX4Dh8vymFPCovKfp0W/spdpU+lcsKVYzUsDxIj+xL7oACg55XcOai7Dn69w8i1xPnJyIKTXsszRO6uM72VXkcu3y49y91/p17pZwY5if2dHoaMPSbUmaXLEYesnW16A97cCAV253wyBGlCfLp5r4nFeLreS51KWi5+MGydaWbH1OnCVMyptOWHb0j8vfzYXLq72pHi7ZusMqXhpS7fMMeY6ECHopDgYpTZwWzp8Y1jWslRb2iFKT4phOaWHIc8jyBRk5qxskQy01dx24XYczl+/le1KKFbRTOhixItXKBlEIVooGDHmOtQ16OVbzQk6cuUzSvipvuVb0LJ0jWiIGvfZSGsspXQx5Hlio6Fk7L4+m8x3Ecg16XeVygBALg165hs8B5ukh7fg+UOUWoWxY+H7QtkfnoQZHfndy+nhj7/SlXMWbNlYNP8+xJj5W8jxpU83LacnWanWky1GTr+ooj37bcXEEbLWfpobVvLy1rdRxbHOjyzjFkOeRhWVbl1I4/ym1STvGIGh14LVyojK1s3vmUYa9GlI4SB/WdbywOt40kXK/ZsgzxuUOwfPybIs5+IX67JQHR2qH2zwPLs+xix30Qt8I2dIBK0OeZ76qeZaPBmNX02J9vuVtQkvF7qc5Y1WvWgpjhK8LKGIHvVIx5BnEncEvC8vOFraxhTa0ZWEb0nQMe2lJeUygatFDnojMisgVInK7iNwqIleJyBOmvOY4EXmXiPxIRB4TkR+IyJtCtbkp6+fmzc/siN0EAMDMprnKP7nhQBofq3hhMezZr+KFGJc49i0W4mA1esgDcCOAcwA8T1WfC2AOwE0iIhNe8xEAJwF4E4BXAXgYwPUi8nu+G9uWj6BncdBoO3lOCnNtwl7bdlg6lyKENoOuj35X2r97qQZhr/TAZw3DV76ihjwROR/AqwFcpqqH+g+/B8DLAFww5jWnAPipql6qqt9R1S8D+CcAfgrgChE5LkDTvXO106Vw8UXdAJdDVY+DKVFPSYHP4gE5EOf7rTkGNtN11SF2Je8SAHsA3Dl4QFW3A7gXwMVjXrMCwEeHH1DVAwC+AGA1gDVeWuqA9WXbGJoGN6tBz+ognqOuSxwlL9VarZiWFPisYNiqJ/U+GS3kicgqAOcCuFtVdeTpuwCcJSInjr5OVberatW/+iMA9gPY7bqt5EfbwFb3dSVP5k1wsCdLcgt8Fg8AY+/zsT/fJ2sHUjEreacCmAWwq+K5fQAEwKYG7/cCADeo6pGqJ0XkQhG5Q0Tu2H/oYOPGxlBnR7A0gKQUqmJcnZnzwEbkQ05hzwqOQ2WJGfIG5ZhHKp57vP+z1gllInIugKcBeN+431HVa1X1bFU9e/XyFU3a6RSXbHu6LrtaXbYl21I6EKFjUg17lg7CAQa8EsUMeYM9tirIDR6bn/YmIrIcwMcBXKCqexy1jUbwvmREFFuqYc8CawHPWntCCzWnxgx52/o/qy6UWAPgCID7a7zPVQD+RlX/xlXDfGtSzXOxI1i7wtZVFa7O+1ip3Fgf0GK2r8k5LDzY6M7aOUNtpBD0LFXxrI8/5E+0kKeq+wBsAXBmxdNPB3Crqu6f9B4i8gcADqvqFR6amAxLgwkRUQgpBD0LGPDKFvsWKlcDWCciZw0eEJEzAKwH8Mmhx5aUbPo3Pv5F9G7DMvz4Kd5a61Cp5+a5PpfOwrl5DNlEcTDoEU0WO+RdD+AWAJdLzzIAVwL4EoAbAEBELgWwV0ReM3iRiPxrAK8F8CEAZ4rIM0Rks4i8AcA7Qv8lyM6yqEWlHklbm4DZRykUKwd+1seeEO3bdvCA98/wxcWYFbUHqOqCiJwH4MMAbgOwAOBmAO8funfeHvTuf/cQAIjI5QA+0H/u+xVve47PNsew6YRl2L7vcOxmmDWzaQ4L26deo7PE2tWbsWf/Vg8tcm9c5Xdn1Q2IiKaYW9hg5juru9o986i5844tsB7wUmDtQLWN6L1AVR8GcNGE568DcN3Q/1+JXrUveevXlTVJx1paXblhIx7bcU+Uz+5q2rL+4HkX/YgHE0TdWajiMeDFYfGiJvaEEWtOq/4n2XsfJz8Kq8l5myUcMPDKWrdYzcsTAx4Ni31OnglrTlt29M+037HKwtFjTBYuwHCpzYU5pV7Mk4pUq8mUjhQDXoptTknRIa9NcHMd9OpOzF13hNhHubmFsLrabLcuYY1Bj5qwuLzUloXzp0o/2CZ7igx5s8ulU1izXNGjtLkIaSkFvVAhg1fWUu5YEaMqRYY8Fxj0jsltAs2pukE0Dfu7GzGreAx4aQl5fjFDXgcMerZMWhJOIYi6rMClVM0jcsXCkm1oDHg0CUNeRy6CHidksoITRplYzaOYOO74w5BH3pV60UUTPoI+Dx6oCQa99mIt1TIc0TQMeQ6EWradtkPzyq7meO81omMY9NLBgEd1MOQRecJBmHzwfWDCoEeUD4Y8R3gRhg0pLg37XFblki21waBXX4wVFB5AUl0MeUbEnoxz+XojInIj1aCX+xW2DHjUBEMeERFVSjXoUXoYXv1gyHOIS7ZLWVo+tXivvBAV3NhVYkobg954oZdqGYSoKYa8gsT+/loiShODXnwMeNQGQ15iYuzok67m81kdO24DQynl4bEd9zh9vxi3/plb2MCwRzSG1X2DIc8xLtm6MQh4DHpEtlidzEILuVTLKh61xZCXmRxuiNw12Fk6D5AoRwx6RGlgyCPzWM3rjhdfkGtcvg2DVTzqgiHPEE7EZQU6q9s7x0nF9TlxFlj5Sr4Sg16oFZMc98VJSvv7hsCQ5wHPy3OvpPBHlJoSgx5RChjyiIgSZaWaBzDoucaqFrnAkEdERE6UEPRyuLiNysGQR2bEWpK1VA0haspa/y0h6PnGKh65wpBHyeB5eURpYNAjsoEhj7xJ+X51nKTyk+MVtgPWqnlAnvtQiKVaVvHIJYY8IiLyIsegR34x5LrFkEdU0+6ZR2M3gWgsi9U8gEGvCQacdM3P7IjdhEoMeVSUlRs2xm5CNC5vvhzjCsM9+7cG/8zUMOgR0TCGPCKKJvTRb87n5ZFfvg9sWMUjHxjyiIgywmoe0TGl39eQIY+IKDNWgx5VYxWPfGHIIzMe3zH5woZpzxPVUcqSrcWgx2oeUVgMeUSUDF580YzFoJcin0t+rOItxX8Td/gvaUydKyDXr5u82Z6Pkxb9/7d+eHhopzlp6QuOOrzkkZsf/BnmDt4+/iV3956r3CkfPPaf2/ctfe+mZju/Q88TKh6bW9H7ufjvcYqjT8zT8MS37eCBJc+fvFD3G0p2H/uvGrepmf+5g4s1/m/3t6gyt7DBXLBau3pzlHAc8qKa+n2NKD1/8fdvwg3y5lavFVV13Bz7RGQ3gHtjt4NaWQtgT+xGUCvcdunitksXt126hrfdU1X15KZvUGTIo3SJyB2qenbsdlBz3Hbp4rZLF7ddulxsO56TR0RERJQhhjwiIiKiDDHkUWqujd0Aao3bLl3cdunitktX523Hc/KIiIiIMsRKHhEREVGGGPKIiIiIMsSQR0RERJQhhjxKgoi8VER05M8VsdtFi4nIKSJypYh8d8zzsyJyhYjcLiK3ishVIlL1JSQU2LRt1/8d7ofGSM9FIvJ9EXlURLaJyDtEREZ+74kick1/v7tNRN4vIq6+SIhaqLvt+r/72xX73gVTP4MXXlAKROQWAOuHHlIAL1fVcN+dRBOJyDkAXg7g3QB2qurGit/5K/S+W+9XVPWQiPwlgJPR25YcjCKps+36v8f90BgRuQzAMwH8OYDjAFwG4BUAPqKq7+j/znIAXwfwYwBvQe8rTb8K4Meq+tYIzSbU23b935sF8D0s/iraxwCcq6qPTPwMjqtknYg8D8DbVfWNsdtC04nI7QBOHg0KInI+gM8C+GVV/W7/sU0AfgLgd1T1U6HbSouN23b957gfGtMPbx9Q1X8z9NgsgNsA/CMA61X1gX6YuBLAU1R1d//3XgLgawBepqo3B2984epuu/7jbwTwTFV9d9PP4XItpeAPAWwTkdNjN4RqGXdkeQl638N45+ABVd2O3vdIXxygXTTdpKoA90N7VgP4D8MPqOoRAJ9Db37f2H/4YgB3DgJe3/8CcBDc92Kpte1EZAbAuwDsFJH1aIghj0wTkeegt4z0XgA/FpFv9h8ju5YsD4jIKgDnAri7Yln2LgBniciJAdpGk1Uu7XA/tElV96jqgxVPPQJgAcBPROSZADYA+NHIaw8C2A7gRVXngJFfdbZd////OXpLulcD2CEiXxCRM+p+DkMeWbcHwHkA3gHgmwCeD+BbIvLPoraKmjoVwCyAXRXP7QMgADYFbRE1wf0wLS8A8OV+iNjYf2zcvncSgBPDNItqGN52APB9AL+O3vmy/wfAPwVwu4icW+fNGPLINFXdrqp/p6ofUdUXAjgfvWrD9SLy5MjNo/rm+j+rlgMf7/88PlBbqCHuh+kQkaeiFwQG53px30tExbaDqt6lqn+rqh8A8GwAvw9gFYAbRWTFtPdkyKOkqOpfAfhdAE8C8GuRm0P1Pdr/WTWZDB6bD9QW6oj7oWnXAHiXqv6w///c99Ixuu0W0Z6PAfgjAKehV/WbiCGPUnQdgP8HYE3shlBt2/o/q7bZGgBHANwfrjnkAPdDY0TkcgAPqOpHhx6etu/tVtXHvDeOJhqz7cb5Y/TO25u67y2b9gtE1qiqishO9M5VoASo6j4R2QLgzIqnnw7gVlXdH7hZ1AH3Q1tE5HUA/jGA14w8tRXAgwCeMfL7K9GrBn02SANprAnbrpKqHhCRh1Bj32Mlj5IjImsA/AzA/4jdFqok/T+jrgawTkTOOvqLvavE1gP4ZKC20WTjtt3SX+R+aEb/Apg3AXitqh4eenwdeudO/hmA54jI3NDLXojexVDc9yKatO3GXfXcv2L6FlX9wdT3582QyTIR+Qh6V4V9TFUfE5G1AP4tgA+p6k/jto5G9QelHwA4BcCTVfXQ0HMzAL6C3pWar0NvgvksgJUAfpXfeBHXlG3H/dAoEfkN9G5t8yYAD/cfnkWvQv5KVb1ARI4HcDuAL6rqO0XkiQBuBvC/VfXtMdpN07cdgLcD+ASA7wD4tKoeEZGN6H0zxh/UWf1gyCPTRORPAPwWeicP34zeV7t8gueQ2NMfsN6PY0uy2wH8iapeM/Q7TwTwYfSuEltAb5u+fzhQUHjTth33Q5tE5A0A/gLjV+Vep6o39n/3yQA+jt4S7Qx6B1gf4cFVHHW2HXo3Rr4BwK+it+T+DfS+EePT/RsnT/8cbl8iIiKi/PCcPCIiIqIMMeQRERERZYghj4iIiChDDHlEREREGWLIIyIiIsoQQx4RERFRhhjyiIiIiDLEkEdERESUIYY8IiIiogwx5BERERFliCGPiIiIKEMMeUREREQZYsgjImpBRK4RkcMiov2fbxGRZ4jIvv5jX4jdRiIqm6hq7DYQESVJRF4C4L8D2A3gdFU9KCL/DcDNqvqJuK0jotIx5BERdSAiFwH4MwAfBvBdAL+kqu+K2yoiIoY8IqLOROQzAF4P4IsAzlPVhchNIiJiyCMi6kpENgH4MYAHATxLVR+I3CQiIl54QUTUhYgsB/AfAbwSwCoA/1VEZuO2ioiIIY+IqKurAHxUVb8E4HcBvBjAv4vaIiIicLmWiKg1EXkvgCep6mVDj90M4KUAXqeqn43WOCIqHit5REQtiMjHALwfwEUi8uz+Y78J4AUABL1l289EbCIRFY6VPCIiIqIMsZJHRERElCGGPCIiIqIMMeQRERERZYghj4iIiChDDHlEREREGWLIIyIiIsoQQx4RERFRhhjyiIiIiDLEkEdERESUof8PfmUv6GqcWFMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# Assuming you have imported your data and defined necessary functions\n",
    "\n",
    "# Rest of your code...\n",
    "\n",
    "# # Convert the font size to points\n",
    "font_size = 20\n",
    "# ticks_font = FontProperties(family='Times New Roman', style='normal', size=font_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_test = 8*pi*torch.rand(100000).reshape(-1,1)\n",
    "t_test = torch.rand(100000).reshape(-1,1)\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution(x_test,t_test).reshape(-1,1)\n",
    "u_test_pred = my_network(test).reshape(-1,1)\n",
    "relative_error = torch.abs(u_test_pred - u_test)\n",
    "u_test = u_test.reshape(-1,)\n",
    "\n",
    "# reshaping and detach numpy\n",
    "x_test = x_test.reshape(-1, )\n",
    "t_test = t_test.reshape(-1, )\n",
    "relative_error = relative_error.reshape(-1,)\n",
    "u_test_pred = u_test_pred.reshape(-1, )\n",
    "\n",
    "x_test = x_test.detach().numpy()\n",
    "t_test = t_test.detach().numpy()\n",
    "u_test_pred = u_test_pred.detach().numpy()\n",
    "relative_error = relative_error.detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "font_path = 'times-new-roman.ttf'\n",
    "\n",
    "ticks_font = FontProperties(fname=font_path)\n",
    "\n",
    "# Define the levels for contouring\n",
    "levels = np.linspace(-1.5, 1.5, 20)\n",
    "\n",
    "        \n",
    "CS1 = plt.tricontourf(x_test, t_test, u_test_pred, levels, cmap='twilight')\n",
    "#CS1 = plt.tricontourf(x_test, t_test, u_test, 20, cmap='twilight')\n",
    "#CS1 = plt.tricontourf(x_test, t_test, relative_error, 20, cmap='rainbow')\n",
    "\n",
    "\n",
    "\n",
    "#cbar1 = plt.colorbar(CS1)\n",
    "for t in cbar1.ax.get_yticklabels():\n",
    "    t.set_fontproperties(custom_font)\n",
    "    t.set_fontsize(20)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('x', fontsize=20, fontproperties=custom_font)\n",
    "plt.ylabel('t', fontsize=20, fontproperties=custom_font)\n",
    "plt.xticks(fontsize=20, fontproperties=custom_font)\n",
    "plt.yticks(fontsize=20, fontproperties=custom_font)\n",
    "#plt.savefig('Causal_EB.pdf', dpi = 300, bbox_inches = \"tight\")\n",
    "#plt.savefig('PINN_EB.pdf', dpi=500, bbox_inches=\"tight\", format='pdf', backend='cairo')\n",
    "#plt.savefig('Absolute_error_EB.pdf', dpi = 300, bbox_inches = \"tight\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01ae3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
