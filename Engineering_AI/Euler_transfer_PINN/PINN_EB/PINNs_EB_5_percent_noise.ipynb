{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a6071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import pi\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1beeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.tensor(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd5d8b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "# Define the exact solution\n",
    "def exact_solution(x, t):\n",
    "    return torch.sin(x)*torch.cos(pi*t)\n",
    "\n",
    "def initial_condition(x):\n",
    "    return torch.sin(x)*(1 + sigma*torch.randn(x.shape)).to(device)\n",
    "\n",
    "def initial_condition_t(x):\n",
    "    return 0*torch.cos(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d9104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning number of points\n",
    "initial_pts = 500\n",
    "left_boundary_pts = 500\n",
    "right_boundary_pts = 500\n",
    "residual_pts = 10000\n",
    "\n",
    "# Type of optimizer (ADAM or LBFGS)\n",
    "opt_type = \"LBFGS\"\n",
    "\n",
    "x_init = 8*pi*torch.rand((initial_pts,1)) # initial pts\n",
    "t_init = 0*x_init\n",
    "init = torch.cat([x_init, t_init],1).to(device)\n",
    "u_init = initial_condition(init[:,0]).reshape(-1, 1)\n",
    "u_init_t = 0*initial_condition(init[:,0]).reshape(-1, 1)\n",
    "\n",
    "xb_left = torch.zeros((left_boundary_pts, 1)) # left spatial boundary\n",
    "tb_left = torch.rand((left_boundary_pts, 1)) #\n",
    "b_left = torch.cat([xb_left, tb_left ],1).to(device)\n",
    "u_b_l = 0*torch.sin(tb_left)\n",
    "\n",
    "xb_right = 8*pi*torch.ones((right_boundary_pts, 1)) # right spatial boundary\n",
    "tb_right = torch.rand((right_boundary_pts, 1)) # right boundary pts\n",
    "b_right = torch.cat([xb_right, tb_right ],1).to(device)\n",
    "u_b_r = 0*torch.sin(2*pi - tb_right)\n",
    "\n",
    "x_interior = 8*pi*torch.rand((residual_pts, 1))\n",
    "t_interior = torch.rand((residual_pts, 1))\n",
    "interior = torch.cat([x_interior, t_interior],1).to(device)\n",
    "\n",
    "training_set = DataLoader(torch.utils.data.TensorDataset(init.to(device), u_init.to(device), u_init_t.to(device), b_left.to(device),  b_right.to(device), u_b_l.to(device), u_b_r.to(device)), batch_size=500, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b64fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e5f5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension, output_dimension, n_hidden_layers, neurons):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer\n",
    "        self.neurons = neurons\n",
    "        # Number of hidden layers\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        # Activation function\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.neurons, self.neurons) for _ in range(n_hidden_layers)])\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear transformations defining the network\n",
    "        # (see equation above)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x = self.activation(l(x))\n",
    "        return self.output_layer(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f73441b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "my_network = NeuralNet(input_dimension = init.shape[1], output_dimension = u_init.shape[1], n_hidden_layers=4, neurons=200)\n",
    "model_state_dict = torch.load('PINN_EB.pth', map_location=torch.device('cpu'))\n",
    "my_network = my_network.to(device)\n",
    "\n",
    "# def init_xavier(model, retrain_seed):\n",
    "#     torch.manual_seed(retrain_seed)\n",
    "#     def init_weights(m):\n",
    "#         if type(m) == nn.Linear and m.weight.requires_grad and m.bias.requires_grad:\n",
    "#             g = nn.init.calculate_gain('tanh')\n",
    "#             torch.nn.init.xavier_uniform_(m.weight, gain=g)\n",
    "#             #torch.nn.init.xavier_normal_(m.weight, gain=g)\n",
    "#             m.bias.data.fill_(0)\n",
    "#     model.apply(init_weights)\n",
    "\n",
    "# # Random Seed for weight initialization\n",
    "# retrain = 128\n",
    "# # Xavier weight initialization\n",
    "# init_xavier(my_network, retrain)\n",
    "\n",
    "if opt_type == \"ADAM\":\n",
    "    optimizer_ = optim.Adam(my_network.parameters(), lr=0.001)\n",
    "elif opt_type == \"LBFGS\":\n",
    "    optimizer_ = optim.LBFGS(my_network.parameters(), lr=0.1, max_iter=1, max_eval=50000, tolerance_change=1.0 * np.finfo(float).eps)\n",
    "else:\n",
    "    raise ValueError(\"Optimizer not recognized\")\n",
    "\n",
    "\n",
    "def fit(model, training_set, interior, num_epochs, optimizer, p, verbose=True):\n",
    "    history = list()\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "\n",
    "        running_loss = list([0])\n",
    "\n",
    "        # Loop over batches\n",
    "        for j, (initial, u_initial, u_initial_t, bd_left, bd_right, ubl, ubr) in enumerate(training_set):\n",
    "            def closure():\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # for initial\n",
    "                initial.requires_grad = True\n",
    "                u_initial_pred_ = model(initial)\n",
    "                inputs = torch.ones(initial_pts, 1).to(device)\n",
    "                grad_u_init = torch.autograd.grad(u_initial_pred_, initial, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_init_t = grad_u_init[:, 1].reshape(-1, )\n",
    "\n",
    "                # for left boundary\n",
    "                bd_left.requires_grad = True\n",
    "                bd_left_pred_ = model(bd_left)\n",
    "                inputs = torch.ones(left_boundary_pts, 1).to(device)\n",
    "                grad_bd_left = torch.autograd.grad(bd_left_pred_, bd_left, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_x_left = grad_bd_left[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(left_boundary_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_bd_x_left = torch.autograd.grad(u_bd_x_left, bd_left, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_xx_left = grad_u_bd_x_left[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(left_boundary_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_bd_xx_left = torch.autograd.grad(u_bd_xx_left, bd_left, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_xxx_left = grad_u_bd_xx_left[:, 0].reshape(-1, )\n",
    "\n",
    "                # for right boundary\n",
    "                bd_right.requires_grad = True\n",
    "                bd_right_pred_ = model(bd_right)\n",
    "                inputs = torch.ones(right_boundary_pts, 1).to(device)\n",
    "                grad_bd_right = torch.autograd.grad(bd_right_pred_, bd_right, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_x_right = grad_bd_right[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(right_boundary_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_bd_x_right = torch.autograd.grad(u_bd_x_right, bd_right, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_xx_right = grad_u_bd_x_right[:, 0].reshape(-1, )\n",
    "\n",
    "                # residual calculation\n",
    "                interior.requires_grad = True\n",
    "                u_hat = model(interior)\n",
    "                inputs = torch.ones(residual_pts, 1).to(device)\n",
    "                grad_u_hat = torch.autograd.grad(u_hat, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "\n",
    "                u_x = grad_u_hat[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_x = torch.autograd.grad(u_x, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_xx = grad_u_x[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_xx = torch.autograd.grad(u_xx, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_xxx = grad_u_xx[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_xxx = torch.autograd.grad(u_xxx, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_xxxx = grad_u_xxx[:, 0].reshape(-1, )\n",
    "\n",
    "                u_t = grad_u_hat[:, 1]\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_t = torch.autograd.grad(u_t, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_tt = grad_u_t[:, 1].reshape(-1, )\n",
    "\n",
    "                # Item 1. below\n",
    "\n",
    "                loss_ic = torch.mean((u_initial_pred_.reshape(-1, ) - u_initial.reshape(-1, )) ** p) + \\\n",
    "                          torch.mean((u_init_t.reshape(-1, )) ** p)\n",
    "                loss_pde = torch.mean((u_tt.reshape(-1, ) + u_xxxx.reshape(-1, ) - (2-pi**2)*torch.sin(interior[:,0])*torch.cos(pi*interior[:,1])) ** p)\n",
    "                loss_left_b = torch.mean((u_bd_x_left.reshape(-1, ) - ubl.reshape(-1, )) ** p) + \\\n",
    "                              torch.mean((u_bd_xx_left.reshape(-1, ) - ubl.reshape(-1, )) ** p)\n",
    "                loss_right_b = torch.mean((bd_right_pred_.reshape(-1, ) - ubr.reshape(-1, )) ** p) + \\\n",
    "                               torch.mean((u_bd_xx_right.reshape(-1, ) - ubr.reshape(-1, )) ** p)\n",
    "\n",
    "                loss = loss_ic + loss_pde + loss_left_b + loss_right_b\n",
    "\n",
    "                # Item 2. below\n",
    "                loss.backward()\n",
    "                # Compute average training loss over batches for the current epoch\n",
    "                running_loss[0] += loss.item()\n",
    "                return loss\n",
    "\n",
    "            # Item 3. below\n",
    "            optimizer.step(closure=closure)\n",
    "\n",
    "        print('Loss: ', (running_loss[0] / len(training_set)))\n",
    "        history.append(running_loss[0])\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a242aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################  0  ################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/localhome/tkapoor/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:234: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  16.014644622802734\n",
      "################################  1  ################################\n",
      "Loss:  16.014142990112305\n",
      "################################  2  ################################\n",
      "Loss:  16.010597229003906\n",
      "################################  3  ################################\n",
      "Loss:  16.007429122924805\n",
      "################################  4  ################################\n",
      "Loss:  16.004562377929688\n",
      "################################  5  ################################\n",
      "Loss:  16.001949310302734\n",
      "################################  6  ################################\n",
      "Loss:  15.999544143676758\n",
      "################################  7  ################################\n",
      "Loss:  15.99730396270752\n",
      "################################  8  ################################\n",
      "Loss:  15.995192527770996\n",
      "################################  9  ################################\n",
      "Loss:  15.99317741394043\n",
      "################################  10  ################################\n",
      "Loss:  15.991230964660645\n",
      "################################  11  ################################\n",
      "Loss:  15.989324569702148\n",
      "################################  12  ################################\n",
      "Loss:  15.987420082092285\n",
      "################################  13  ################################\n",
      "Loss:  15.985480308532715\n",
      "################################  14  ################################\n",
      "Loss:  15.983443260192871\n",
      "################################  15  ################################\n",
      "Loss:  15.981213569641113\n",
      "################################  16  ################################\n",
      "Loss:  15.978641510009766\n",
      "################################  17  ################################\n",
      "Loss:  15.975411415100098\n",
      "################################  18  ################################\n",
      "Loss:  15.970756530761719\n",
      "################################  19  ################################\n",
      "Loss:  15.961827278137207\n",
      "################################  20  ################################\n",
      "Loss:  15.934464454650879\n",
      "################################  21  ################################\n",
      "Loss:  15.93371868133545\n",
      "################################  22  ################################\n",
      "Loss:  15.931916236877441\n",
      "################################  23  ################################\n",
      "Loss:  15.92384147644043\n",
      "################################  24  ################################\n",
      "Loss:  15.915132522583008\n",
      "################################  25  ################################\n",
      "Loss:  15.910325050354004\n",
      "################################  26  ################################\n",
      "Loss:  15.90578556060791\n",
      "################################  27  ################################\n",
      "Loss:  15.899734497070312\n",
      "################################  28  ################################\n",
      "Loss:  15.89091968536377\n",
      "################################  29  ################################\n",
      "Loss:  15.877321243286133\n",
      "################################  30  ################################\n",
      "Loss:  15.861054420471191\n",
      "################################  31  ################################\n",
      "Loss:  15.852940559387207\n",
      "################################  32  ################################\n",
      "Loss:  15.846710205078125\n",
      "################################  33  ################################\n",
      "Loss:  15.837794303894043\n",
      "################################  34  ################################\n",
      "Loss:  15.824974060058594\n",
      "################################  35  ################################\n",
      "Loss:  15.811814308166504\n",
      "################################  36  ################################\n",
      "Loss:  15.798774719238281\n",
      "################################  37  ################################\n",
      "Loss:  15.783263206481934\n",
      "################################  38  ################################\n",
      "Loss:  15.764459609985352\n",
      "################################  39  ################################\n",
      "Loss:  15.743208885192871\n",
      "################################  40  ################################\n",
      "Loss:  15.71904182434082\n",
      "################################  41  ################################\n",
      "Loss:  15.690696716308594\n",
      "################################  42  ################################\n",
      "Loss:  15.657719612121582\n",
      "################################  43  ################################\n",
      "Loss:  15.620213508605957\n",
      "################################  44  ################################\n",
      "Loss:  15.578204154968262\n",
      "################################  45  ################################\n",
      "Loss:  15.531614303588867\n",
      "################################  46  ################################\n",
      "Loss:  15.48006534576416\n",
      "################################  47  ################################\n",
      "Loss:  15.422260284423828\n",
      "################################  48  ################################\n",
      "Loss:  15.35516357421875\n",
      "################################  49  ################################\n",
      "Loss:  15.271156311035156\n",
      "################################  50  ################################\n",
      "Loss:  15.154799461364746\n",
      "################################  51  ################################\n",
      "Loss:  15.001055717468262\n",
      "################################  52  ################################\n",
      "Loss:  14.884878158569336\n",
      "################################  53  ################################\n",
      "Loss:  14.817402839660645\n",
      "################################  54  ################################\n",
      "Loss:  14.770666122436523\n",
      "################################  55  ################################\n",
      "Loss:  14.725330352783203\n",
      "################################  56  ################################\n",
      "Loss:  14.676169395446777\n",
      "################################  57  ################################\n",
      "Loss:  14.61864948272705\n",
      "################################  58  ################################\n",
      "Loss:  14.552780151367188\n",
      "################################  59  ################################\n",
      "Loss:  14.493465423583984\n",
      "################################  60  ################################\n",
      "Loss:  14.451653480529785\n",
      "################################  61  ################################\n",
      "Loss:  14.410961151123047\n",
      "################################  62  ################################\n",
      "Loss:  14.358227729797363\n",
      "################################  63  ################################\n",
      "Loss:  14.29072380065918\n",
      "################################  64  ################################\n",
      "Loss:  14.230416297912598\n",
      "################################  65  ################################\n",
      "Loss:  14.186436653137207\n",
      "################################  66  ################################\n",
      "Loss:  14.140504837036133\n",
      "################################  67  ################################\n",
      "Loss:  14.081511497497559\n",
      "################################  68  ################################\n",
      "Loss:  14.02325439453125\n",
      "################################  69  ################################\n",
      "Loss:  13.960149765014648\n",
      "################################  70  ################################\n",
      "Loss:  13.877994537353516\n",
      "################################  71  ################################\n",
      "Loss:  13.797464370727539\n",
      "################################  72  ################################\n",
      "Loss:  13.727447509765625\n",
      "################################  73  ################################\n",
      "Loss:  13.673295021057129\n",
      "################################  74  ################################\n",
      "Loss:  13.618368148803711\n",
      "################################  75  ################################\n",
      "Loss:  13.55958080291748\n",
      "################################  76  ################################\n",
      "Loss:  13.491677284240723\n",
      "################################  77  ################################\n",
      "Loss:  13.415590286254883\n",
      "################################  78  ################################\n",
      "Loss:  13.33072280883789\n",
      "################################  79  ################################\n",
      "Loss:  13.241250991821289\n",
      "################################  80  ################################\n",
      "Loss:  13.153175354003906\n",
      "################################  81  ################################\n",
      "Loss:  13.068686485290527\n",
      "################################  82  ################################\n",
      "Loss:  12.985739707946777\n",
      "################################  83  ################################\n",
      "Loss:  12.922216415405273\n",
      "################################  84  ################################\n",
      "Loss:  12.87393856048584\n",
      "################################  85  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  12.83405590057373\n",
      "################################  86  ################################\n",
      "Loss:  12.794971466064453\n",
      "################################  87  ################################\n",
      "Loss:  12.757987976074219\n",
      "################################  88  ################################\n",
      "Loss:  12.720791816711426\n",
      "################################  89  ################################\n",
      "Loss:  12.684744834899902\n",
      "################################  90  ################################\n",
      "Loss:  12.650808334350586\n",
      "################################  91  ################################\n",
      "Loss:  12.617951393127441\n",
      "################################  92  ################################\n",
      "Loss:  12.584494590759277\n",
      "################################  93  ################################\n",
      "Loss:  12.549375534057617\n",
      "################################  94  ################################\n",
      "Loss:  12.511905670166016\n",
      "################################  95  ################################\n",
      "Loss:  12.4693021774292\n",
      "################################  96  ################################\n",
      "Loss:  12.421737670898438\n",
      "################################  97  ################################\n",
      "Loss:  12.373104095458984\n",
      "################################  98  ################################\n",
      "Loss:  12.330921173095703\n",
      "################################  99  ################################\n",
      "Loss:  12.284546852111816\n",
      "################################  100  ################################\n",
      "Loss:  12.228102684020996\n",
      "################################  101  ################################\n",
      "Loss:  12.171862602233887\n",
      "################################  102  ################################\n",
      "Loss:  12.118786811828613\n",
      "################################  103  ################################\n",
      "Loss:  12.076260566711426\n",
      "################################  104  ################################\n",
      "Loss:  12.047622680664062\n",
      "################################  105  ################################\n",
      "Loss:  12.020421981811523\n",
      "################################  106  ################################\n",
      "Loss:  11.99378490447998\n",
      "################################  107  ################################\n",
      "Loss:  11.96584701538086\n",
      "################################  108  ################################\n",
      "Loss:  11.935346603393555\n",
      "################################  109  ################################\n",
      "Loss:  11.90351390838623\n",
      "################################  110  ################################\n",
      "Loss:  11.86844539642334\n",
      "################################  111  ################################\n",
      "Loss:  11.833149909973145\n",
      "################################  112  ################################\n",
      "Loss:  11.795586585998535\n",
      "################################  113  ################################\n",
      "Loss:  11.757532119750977\n",
      "################################  114  ################################\n",
      "Loss:  11.72115421295166\n",
      "################################  115  ################################\n",
      "Loss:  11.687993049621582\n",
      "################################  116  ################################\n",
      "Loss:  11.659554481506348\n",
      "################################  117  ################################\n",
      "Loss:  11.632746696472168\n",
      "################################  118  ################################\n",
      "Loss:  11.606555938720703\n",
      "################################  119  ################################\n",
      "Loss:  11.58149242401123\n",
      "################################  120  ################################\n",
      "Loss:  11.557146072387695\n",
      "################################  121  ################################\n",
      "Loss:  11.53347110748291\n",
      "################################  122  ################################\n",
      "Loss:  11.509221076965332\n",
      "################################  123  ################################\n",
      "Loss:  11.483675003051758\n",
      "################################  124  ################################\n",
      "Loss:  11.458210945129395\n",
      "################################  125  ################################\n",
      "Loss:  11.433454513549805\n",
      "################################  126  ################################\n",
      "Loss:  11.412369728088379\n",
      "################################  127  ################################\n",
      "Loss:  11.393471717834473\n",
      "################################  128  ################################\n",
      "Loss:  11.375986099243164\n",
      "################################  129  ################################\n",
      "Loss:  11.358555793762207\n",
      "################################  130  ################################\n",
      "Loss:  11.339641571044922\n",
      "################################  131  ################################\n",
      "Loss:  11.318438529968262\n",
      "################################  132  ################################\n",
      "Loss:  11.294416427612305\n",
      "################################  133  ################################\n",
      "Loss:  11.27221393585205\n",
      "################################  134  ################################\n",
      "Loss:  11.250192642211914\n",
      "################################  135  ################################\n",
      "Loss:  11.231097221374512\n",
      "################################  136  ################################\n",
      "Loss:  11.212430000305176\n",
      "################################  137  ################################\n",
      "Loss:  11.1935396194458\n",
      "################################  138  ################################\n",
      "Loss:  11.17388916015625\n",
      "################################  139  ################################\n",
      "Loss:  11.152857780456543\n",
      "################################  140  ################################\n",
      "Loss:  11.12936782836914\n",
      "################################  141  ################################\n",
      "Loss:  11.101820945739746\n",
      "################################  142  ################################\n",
      "Loss:  11.068536758422852\n",
      "################################  143  ################################\n",
      "Loss:  11.027228355407715\n",
      "################################  144  ################################\n",
      "Loss:  10.975433349609375\n",
      "################################  145  ################################\n",
      "Loss:  10.917473793029785\n",
      "################################  146  ################################\n",
      "Loss:  10.847660064697266\n",
      "################################  147  ################################\n",
      "Loss:  10.787687301635742\n",
      "################################  148  ################################\n",
      "Loss:  10.735665321350098\n",
      "################################  149  ################################\n",
      "Loss:  10.68402099609375\n",
      "################################  150  ################################\n",
      "Loss:  10.58973503112793\n",
      "################################  151  ################################\n",
      "Loss:  10.445939064025879\n",
      "################################  152  ################################\n",
      "Loss:  10.369451522827148\n",
      "################################  153  ################################\n",
      "Loss:  10.3132963180542\n",
      "################################  154  ################################\n",
      "Loss:  10.218225479125977\n",
      "################################  155  ################################\n",
      "Loss:  10.141347885131836\n",
      "################################  156  ################################\n",
      "Loss:  10.081040382385254\n",
      "################################  157  ################################\n",
      "Loss:  10.0200777053833\n",
      "################################  158  ################################\n",
      "Loss:  9.93668270111084\n",
      "################################  159  ################################\n",
      "Loss:  9.853801727294922\n",
      "################################  160  ################################\n",
      "Loss:  9.775350570678711\n",
      "################################  161  ################################\n",
      "Loss:  9.675239562988281\n",
      "################################  162  ################################\n",
      "Loss:  9.615578651428223\n",
      "################################  163  ################################\n",
      "Loss:  9.560531616210938\n",
      "################################  164  ################################\n",
      "Loss:  9.493204116821289\n",
      "################################  165  ################################\n",
      "Loss:  9.419550895690918\n",
      "################################  166  ################################\n",
      "Loss:  9.355552673339844\n",
      "################################  167  ################################\n",
      "Loss:  9.298377990722656\n",
      "################################  168  ################################\n",
      "Loss:  9.257573127746582\n",
      "################################  169  ################################\n",
      "Loss:  9.215997695922852\n",
      "################################  170  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  9.171722412109375\n",
      "################################  171  ################################\n",
      "Loss:  9.13016414642334\n",
      "################################  172  ################################\n",
      "Loss:  9.09145736694336\n",
      "################################  173  ################################\n",
      "Loss:  9.054656982421875\n",
      "################################  174  ################################\n",
      "Loss:  9.021906852722168\n",
      "################################  175  ################################\n",
      "Loss:  8.990970611572266\n",
      "################################  176  ################################\n",
      "Loss:  8.962967872619629\n",
      "################################  177  ################################\n",
      "Loss:  8.933927536010742\n",
      "################################  178  ################################\n",
      "Loss:  8.906444549560547\n",
      "################################  179  ################################\n",
      "Loss:  8.878899574279785\n",
      "################################  180  ################################\n",
      "Loss:  8.848917007446289\n",
      "################################  181  ################################\n",
      "Loss:  8.819817543029785\n",
      "################################  182  ################################\n",
      "Loss:  8.792450904846191\n",
      "################################  183  ################################\n",
      "Loss:  8.771596908569336\n",
      "################################  184  ################################\n",
      "Loss:  8.758666038513184\n",
      "################################  185  ################################\n",
      "Loss:  8.747390747070312\n",
      "################################  186  ################################\n",
      "Loss:  8.738083839416504\n",
      "################################  187  ################################\n",
      "Loss:  8.727530479431152\n",
      "################################  188  ################################\n",
      "Loss:  8.715492248535156\n",
      "################################  189  ################################\n",
      "Loss:  8.7015380859375\n",
      "################################  190  ################################\n",
      "Loss:  8.684382438659668\n",
      "################################  191  ################################\n",
      "Loss:  8.66690444946289\n",
      "################################  192  ################################\n",
      "Loss:  8.649665832519531\n",
      "################################  193  ################################\n",
      "Loss:  8.63465690612793\n",
      "################################  194  ################################\n",
      "Loss:  8.620896339416504\n",
      "################################  195  ################################\n",
      "Loss:  8.60844612121582\n",
      "################################  196  ################################\n",
      "Loss:  8.596087455749512\n",
      "################################  197  ################################\n",
      "Loss:  8.582916259765625\n",
      "################################  198  ################################\n",
      "Loss:  8.568893432617188\n",
      "################################  199  ################################\n",
      "Loss:  8.552591323852539\n",
      "################################  200  ################################\n",
      "Loss:  8.534026145935059\n",
      "################################  201  ################################\n",
      "Loss:  8.511848449707031\n",
      "################################  202  ################################\n",
      "Loss:  8.484827041625977\n",
      "################################  203  ################################\n",
      "Loss:  8.453367233276367\n",
      "################################  204  ################################\n",
      "Loss:  8.421534538269043\n",
      "################################  205  ################################\n",
      "Loss:  8.390402793884277\n",
      "################################  206  ################################\n",
      "Loss:  8.36384391784668\n",
      "################################  207  ################################\n",
      "Loss:  8.341218948364258\n",
      "################################  208  ################################\n",
      "Loss:  8.321304321289062\n",
      "################################  209  ################################\n",
      "Loss:  8.303369522094727\n",
      "################################  210  ################################\n",
      "Loss:  8.285228729248047\n",
      "################################  211  ################################\n",
      "Loss:  8.266130447387695\n",
      "################################  212  ################################\n",
      "Loss:  8.245823860168457\n",
      "################################  213  ################################\n",
      "Loss:  8.226166725158691\n",
      "################################  214  ################################\n",
      "Loss:  8.207357406616211\n",
      "################################  215  ################################\n",
      "Loss:  8.184917449951172\n",
      "################################  216  ################################\n",
      "Loss:  8.157575607299805\n",
      "################################  217  ################################\n",
      "Loss:  8.121628761291504\n",
      "################################  218  ################################\n",
      "Loss:  8.071735382080078\n",
      "################################  219  ################################\n",
      "Loss:  8.01382064819336\n",
      "################################  220  ################################\n",
      "Loss:  7.96282958984375\n",
      "################################  221  ################################\n",
      "Loss:  7.91990852355957\n",
      "################################  222  ################################\n",
      "Loss:  7.882643699645996\n",
      "################################  223  ################################\n",
      "Loss:  7.842722415924072\n",
      "################################  224  ################################\n",
      "Loss:  7.8032755851745605\n",
      "################################  225  ################################\n",
      "Loss:  7.768586158752441\n",
      "################################  226  ################################\n",
      "Loss:  7.745619297027588\n",
      "################################  227  ################################\n",
      "Loss:  7.7238850593566895\n",
      "################################  228  ################################\n",
      "Loss:  7.705875873565674\n",
      "################################  229  ################################\n",
      "Loss:  7.688711643218994\n",
      "################################  230  ################################\n",
      "Loss:  7.670669078826904\n",
      "################################  231  ################################\n",
      "Loss:  7.653873920440674\n",
      "################################  232  ################################\n",
      "Loss:  7.634045124053955\n",
      "################################  233  ################################\n",
      "Loss:  7.60965633392334\n",
      "################################  234  ################################\n",
      "Loss:  7.587017059326172\n",
      "################################  235  ################################\n",
      "Loss:  7.557694435119629\n",
      "################################  236  ################################\n",
      "Loss:  7.513859748840332\n",
      "################################  237  ################################\n",
      "Loss:  7.469042778015137\n",
      "################################  238  ################################\n",
      "Loss:  7.418304443359375\n",
      "################################  239  ################################\n",
      "Loss:  7.378207206726074\n",
      "################################  240  ################################\n",
      "Loss:  7.3392157554626465\n",
      "################################  241  ################################\n",
      "Loss:  7.312477111816406\n",
      "################################  242  ################################\n",
      "Loss:  7.285757064819336\n",
      "################################  243  ################################\n",
      "Loss:  7.264585971832275\n",
      "################################  244  ################################\n",
      "Loss:  7.243093013763428\n",
      "################################  245  ################################\n",
      "Loss:  7.225956439971924\n",
      "################################  246  ################################\n",
      "Loss:  7.210299491882324\n",
      "################################  247  ################################\n",
      "Loss:  7.19569206237793\n",
      "################################  248  ################################\n",
      "Loss:  7.182058334350586\n",
      "################################  249  ################################\n",
      "Loss:  7.169451713562012\n",
      "################################  250  ################################\n",
      "Loss:  7.156114101409912\n",
      "################################  251  ################################\n",
      "Loss:  7.141178607940674\n",
      "################################  252  ################################\n",
      "Loss:  7.127120494842529\n",
      "################################  253  ################################\n",
      "Loss:  7.11481237411499\n",
      "################################  254  ################################\n",
      "Loss:  7.101716041564941\n",
      "################################  255  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  7.088838577270508\n",
      "################################  256  ################################\n",
      "Loss:  7.07680082321167\n",
      "################################  257  ################################\n",
      "Loss:  7.066713809967041\n",
      "################################  258  ################################\n",
      "Loss:  7.05665922164917\n",
      "################################  259  ################################\n",
      "Loss:  7.04733419418335\n",
      "################################  260  ################################\n",
      "Loss:  7.038116455078125\n",
      "################################  261  ################################\n",
      "Loss:  7.029101848602295\n",
      "################################  262  ################################\n",
      "Loss:  7.020209312438965\n",
      "################################  263  ################################\n",
      "Loss:  7.011499881744385\n",
      "################################  264  ################################\n",
      "Loss:  7.002997398376465\n",
      "################################  265  ################################\n",
      "Loss:  6.994593620300293\n",
      "################################  266  ################################\n",
      "Loss:  6.986359119415283\n",
      "################################  267  ################################\n",
      "Loss:  6.977996826171875\n",
      "################################  268  ################################\n",
      "Loss:  6.969761371612549\n",
      "################################  269  ################################\n",
      "Loss:  6.961617469787598\n",
      "################################  270  ################################\n",
      "Loss:  6.953909873962402\n",
      "################################  271  ################################\n",
      "Loss:  6.946322441101074\n",
      "################################  272  ################################\n",
      "Loss:  6.9385085105896\n",
      "################################  273  ################################\n",
      "Loss:  6.930610179901123\n",
      "################################  274  ################################\n",
      "Loss:  6.922488689422607\n",
      "################################  275  ################################\n",
      "Loss:  6.914358139038086\n",
      "################################  276  ################################\n",
      "Loss:  6.906055927276611\n",
      "################################  277  ################################\n",
      "Loss:  6.8970160484313965\n",
      "################################  278  ################################\n",
      "Loss:  6.887503147125244\n",
      "################################  279  ################################\n",
      "Loss:  6.8772077560424805\n",
      "################################  280  ################################\n",
      "Loss:  6.866349697113037\n",
      "################################  281  ################################\n",
      "Loss:  6.855224609375\n",
      "################################  282  ################################\n",
      "Loss:  6.8441362380981445\n",
      "################################  283  ################################\n",
      "Loss:  6.833123207092285\n",
      "################################  284  ################################\n",
      "Loss:  6.8226237297058105\n",
      "################################  285  ################################\n",
      "Loss:  6.811924457550049\n",
      "################################  286  ################################\n",
      "Loss:  6.802335262298584\n",
      "################################  287  ################################\n",
      "Loss:  6.792110443115234\n",
      "################################  288  ################################\n",
      "Loss:  6.782474040985107\n",
      "################################  289  ################################\n",
      "Loss:  6.772163391113281\n",
      "################################  290  ################################\n",
      "Loss:  6.762258529663086\n",
      "################################  291  ################################\n",
      "Loss:  6.752134323120117\n",
      "################################  292  ################################\n",
      "Loss:  6.742124080657959\n",
      "################################  293  ################################\n",
      "Loss:  6.731639385223389\n",
      "################################  294  ################################\n",
      "Loss:  6.720454216003418\n",
      "################################  295  ################################\n",
      "Loss:  6.711196422576904\n",
      "################################  296  ################################\n",
      "Loss:  6.701109886169434\n",
      "################################  297  ################################\n",
      "Loss:  6.691853046417236\n",
      "################################  298  ################################\n",
      "Loss:  6.68190336227417\n",
      "################################  299  ################################\n",
      "Loss:  6.67233943939209\n",
      "################################  300  ################################\n",
      "Loss:  6.662623405456543\n",
      "################################  301  ################################\n",
      "Loss:  6.6530656814575195\n",
      "################################  302  ################################\n",
      "Loss:  6.644161224365234\n",
      "################################  303  ################################\n",
      "Loss:  6.635226726531982\n",
      "################################  304  ################################\n",
      "Loss:  6.627341270446777\n",
      "################################  305  ################################\n",
      "Loss:  6.620190620422363\n",
      "################################  306  ################################\n",
      "Loss:  6.613534927368164\n",
      "################################  307  ################################\n",
      "Loss:  6.606499195098877\n",
      "################################  308  ################################\n",
      "Loss:  6.599903583526611\n",
      "################################  309  ################################\n",
      "Loss:  6.593487739562988\n",
      "################################  310  ################################\n",
      "Loss:  6.587164402008057\n",
      "################################  311  ################################\n",
      "Loss:  6.58173131942749\n",
      "################################  312  ################################\n",
      "Loss:  6.577096939086914\n",
      "################################  313  ################################\n",
      "Loss:  6.571928977966309\n",
      "################################  314  ################################\n",
      "Loss:  6.566303253173828\n",
      "################################  315  ################################\n",
      "Loss:  6.560823917388916\n",
      "################################  316  ################################\n",
      "Loss:  6.555541038513184\n",
      "################################  317  ################################\n",
      "Loss:  6.550358772277832\n",
      "################################  318  ################################\n",
      "Loss:  6.545197486877441\n",
      "################################  319  ################################\n",
      "Loss:  6.539923191070557\n",
      "################################  320  ################################\n",
      "Loss:  6.534342288970947\n",
      "################################  321  ################################\n",
      "Loss:  6.528237819671631\n",
      "################################  322  ################################\n",
      "Loss:  6.521377086639404\n",
      "################################  323  ################################\n",
      "Loss:  6.513312339782715\n",
      "################################  324  ################################\n",
      "Loss:  6.504066467285156\n",
      "################################  325  ################################\n",
      "Loss:  6.4929280281066895\n",
      "################################  326  ################################\n",
      "Loss:  6.481903553009033\n",
      "################################  327  ################################\n",
      "Loss:  6.470407962799072\n",
      "################################  328  ################################\n",
      "Loss:  6.4578938484191895\n",
      "################################  329  ################################\n",
      "Loss:  6.446550369262695\n",
      "################################  330  ################################\n",
      "Loss:  6.4353837966918945\n",
      "################################  331  ################################\n",
      "Loss:  6.42391300201416\n",
      "################################  332  ################################\n",
      "Loss:  6.411469459533691\n",
      "################################  333  ################################\n",
      "Loss:  6.399990081787109\n",
      "################################  334  ################################\n",
      "Loss:  6.3889288902282715\n",
      "################################  335  ################################\n",
      "Loss:  6.377843379974365\n",
      "################################  336  ################################\n",
      "Loss:  6.366355895996094\n",
      "################################  337  ################################\n",
      "Loss:  6.3545241355896\n",
      "################################  338  ################################\n",
      "Loss:  6.342702388763428\n",
      "################################  339  ################################\n",
      "Loss:  6.330800533294678\n",
      "################################  340  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  6.317900657653809\n",
      "################################  341  ################################\n",
      "Loss:  6.3031182289123535\n",
      "################################  342  ################################\n",
      "Loss:  6.287880897521973\n",
      "################################  343  ################################\n",
      "Loss:  6.273482799530029\n",
      "################################  344  ################################\n",
      "Loss:  6.259820461273193\n",
      "################################  345  ################################\n",
      "Loss:  6.246296405792236\n",
      "################################  346  ################################\n",
      "Loss:  6.232451915740967\n",
      "################################  347  ################################\n",
      "Loss:  6.219444274902344\n",
      "################################  348  ################################\n",
      "Loss:  6.206212043762207\n",
      "################################  349  ################################\n",
      "Loss:  6.194238185882568\n",
      "################################  350  ################################\n",
      "Loss:  6.182246208190918\n",
      "################################  351  ################################\n",
      "Loss:  6.1713948249816895\n",
      "################################  352  ################################\n",
      "Loss:  6.160220623016357\n",
      "################################  353  ################################\n",
      "Loss:  6.147797107696533\n",
      "################################  354  ################################\n",
      "Loss:  6.1373443603515625\n",
      "################################  355  ################################\n",
      "Loss:  6.1266608238220215\n",
      "################################  356  ################################\n",
      "Loss:  6.115909099578857\n",
      "################################  357  ################################\n",
      "Loss:  6.105954647064209\n",
      "################################  358  ################################\n",
      "Loss:  6.0959343910217285\n",
      "################################  359  ################################\n",
      "Loss:  6.0845489501953125\n",
      "################################  360  ################################\n",
      "Loss:  6.073017120361328\n",
      "################################  361  ################################\n",
      "Loss:  6.063346862792969\n",
      "################################  362  ################################\n",
      "Loss:  6.052960395812988\n",
      "################################  363  ################################\n",
      "Loss:  6.042623043060303\n",
      "################################  364  ################################\n",
      "Loss:  6.0318450927734375\n",
      "################################  365  ################################\n",
      "Loss:  6.020888805389404\n",
      "################################  366  ################################\n",
      "Loss:  6.009702682495117\n",
      "################################  367  ################################\n",
      "Loss:  5.998904705047607\n",
      "################################  368  ################################\n",
      "Loss:  5.988223552703857\n",
      "################################  369  ################################\n",
      "Loss:  5.977991104125977\n",
      "################################  370  ################################\n",
      "Loss:  5.96852445602417\n",
      "################################  371  ################################\n",
      "Loss:  5.958841323852539\n",
      "################################  372  ################################\n",
      "Loss:  5.949034214019775\n",
      "################################  373  ################################\n",
      "Loss:  5.93851900100708\n",
      "################################  374  ################################\n",
      "Loss:  5.929633617401123\n",
      "################################  375  ################################\n",
      "Loss:  5.921772480010986\n",
      "################################  376  ################################\n",
      "Loss:  5.913979530334473\n",
      "################################  377  ################################\n",
      "Loss:  5.906167507171631\n",
      "################################  378  ################################\n",
      "Loss:  5.898324012756348\n",
      "################################  379  ################################\n",
      "Loss:  5.889939308166504\n",
      "################################  380  ################################\n",
      "Loss:  5.880452632904053\n",
      "################################  381  ################################\n",
      "Loss:  5.8729400634765625\n",
      "################################  382  ################################\n",
      "Loss:  5.865451335906982\n",
      "################################  383  ################################\n",
      "Loss:  5.857232570648193\n",
      "################################  384  ################################\n",
      "Loss:  5.8494486808776855\n",
      "################################  385  ################################\n",
      "Loss:  5.841587066650391\n",
      "################################  386  ################################\n",
      "Loss:  5.833295822143555\n",
      "################################  387  ################################\n",
      "Loss:  5.824497699737549\n",
      "################################  388  ################################\n",
      "Loss:  5.81463623046875\n",
      "################################  389  ################################\n",
      "Loss:  5.804264068603516\n",
      "################################  390  ################################\n",
      "Loss:  5.792266845703125\n",
      "################################  391  ################################\n",
      "Loss:  5.779907703399658\n",
      "################################  392  ################################\n",
      "Loss:  5.7684125900268555\n",
      "################################  393  ################################\n",
      "Loss:  5.755612850189209\n",
      "################################  394  ################################\n",
      "Loss:  5.742659568786621\n",
      "################################  395  ################################\n",
      "Loss:  5.729736804962158\n",
      "################################  396  ################################\n",
      "Loss:  5.716322898864746\n",
      "################################  397  ################################\n",
      "Loss:  5.703084468841553\n",
      "################################  398  ################################\n",
      "Loss:  5.688796043395996\n",
      "################################  399  ################################\n",
      "Loss:  5.6774516105651855\n",
      "################################  400  ################################\n",
      "Loss:  5.6655707359313965\n",
      "################################  401  ################################\n",
      "Loss:  5.654716968536377\n",
      "################################  402  ################################\n",
      "Loss:  5.643690586090088\n",
      "################################  403  ################################\n",
      "Loss:  5.6330437660217285\n",
      "################################  404  ################################\n",
      "Loss:  5.621895790100098\n",
      "################################  405  ################################\n",
      "Loss:  5.610045909881592\n",
      "################################  406  ################################\n",
      "Loss:  5.599582195281982\n",
      "################################  407  ################################\n",
      "Loss:  5.5891032218933105\n",
      "################################  408  ################################\n",
      "Loss:  5.579265117645264\n",
      "################################  409  ################################\n",
      "Loss:  5.570065975189209\n",
      "################################  410  ################################\n",
      "Loss:  5.561193943023682\n",
      "################################  411  ################################\n",
      "Loss:  5.55290412902832\n",
      "################################  412  ################################\n",
      "Loss:  5.5448899269104\n",
      "################################  413  ################################\n",
      "Loss:  5.5371809005737305\n",
      "################################  414  ################################\n",
      "Loss:  5.529605388641357\n",
      "################################  415  ################################\n",
      "Loss:  5.522143363952637\n",
      "################################  416  ################################\n",
      "Loss:  5.5147504806518555\n",
      "################################  417  ################################\n",
      "Loss:  5.5073041915893555\n",
      "################################  418  ################################\n",
      "Loss:  5.499802112579346\n",
      "################################  419  ################################\n",
      "Loss:  5.492053508758545\n",
      "################################  420  ################################\n",
      "Loss:  5.484150409698486\n",
      "################################  421  ################################\n",
      "Loss:  5.4758620262146\n",
      "################################  422  ################################\n",
      "Loss:  5.46738338470459\n",
      "################################  423  ################################\n",
      "Loss:  5.458426475524902\n",
      "################################  424  ################################\n",
      "Loss:  5.449313163757324\n",
      "################################  425  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.439902305603027\n",
      "################################  426  ################################\n",
      "Loss:  5.430243492126465\n",
      "################################  427  ################################\n",
      "Loss:  5.420330047607422\n",
      "################################  428  ################################\n",
      "Loss:  5.409718990325928\n",
      "################################  429  ################################\n",
      "Loss:  5.399316787719727\n",
      "################################  430  ################################\n",
      "Loss:  5.388945579528809\n",
      "################################  431  ################################\n",
      "Loss:  5.377865791320801\n",
      "################################  432  ################################\n",
      "Loss:  5.367147922515869\n",
      "################################  433  ################################\n",
      "Loss:  5.356409072875977\n",
      "################################  434  ################################\n",
      "Loss:  5.345238208770752\n",
      "################################  435  ################################\n",
      "Loss:  5.334343433380127\n",
      "################################  436  ################################\n",
      "Loss:  5.324060916900635\n",
      "################################  437  ################################\n",
      "Loss:  5.314117431640625\n",
      "################################  438  ################################\n",
      "Loss:  5.303813934326172\n",
      "################################  439  ################################\n",
      "Loss:  5.293724536895752\n",
      "################################  440  ################################\n",
      "Loss:  5.284196853637695\n",
      "################################  441  ################################\n",
      "Loss:  5.275156021118164\n",
      "################################  442  ################################\n",
      "Loss:  5.266176700592041\n",
      "################################  443  ################################\n",
      "Loss:  5.257012367248535\n",
      "################################  444  ################################\n",
      "Loss:  5.247679710388184\n",
      "################################  445  ################################\n",
      "Loss:  5.237931728363037\n",
      "################################  446  ################################\n",
      "Loss:  5.228200912475586\n",
      "################################  447  ################################\n",
      "Loss:  5.218560218811035\n",
      "################################  448  ################################\n",
      "Loss:  5.208713531494141\n",
      "################################  449  ################################\n",
      "Loss:  5.19916296005249\n",
      "################################  450  ################################\n",
      "Loss:  5.189586639404297\n",
      "################################  451  ################################\n",
      "Loss:  5.1804890632629395\n",
      "################################  452  ################################\n",
      "Loss:  5.17094612121582\n",
      "################################  453  ################################\n",
      "Loss:  5.160999774932861\n",
      "################################  454  ################################\n",
      "Loss:  5.150311470031738\n",
      "################################  455  ################################\n",
      "Loss:  5.139866352081299\n",
      "################################  456  ################################\n",
      "Loss:  5.129424571990967\n",
      "################################  457  ################################\n",
      "Loss:  5.118141174316406\n",
      "################################  458  ################################\n",
      "Loss:  5.1070709228515625\n",
      "################################  459  ################################\n",
      "Loss:  5.096104621887207\n",
      "################################  460  ################################\n",
      "Loss:  5.085416793823242\n",
      "################################  461  ################################\n",
      "Loss:  5.074803829193115\n",
      "################################  462  ################################\n",
      "Loss:  5.064640522003174\n",
      "################################  463  ################################\n",
      "Loss:  5.053582668304443\n",
      "################################  464  ################################\n",
      "Loss:  5.042048931121826\n",
      "################################  465  ################################\n",
      "Loss:  5.029451370239258\n",
      "################################  466  ################################\n",
      "Loss:  5.016092300415039\n",
      "################################  467  ################################\n",
      "Loss:  5.002323150634766\n",
      "################################  468  ################################\n",
      "Loss:  4.988533973693848\n",
      "################################  469  ################################\n",
      "Loss:  4.974030017852783\n",
      "################################  470  ################################\n",
      "Loss:  4.959397792816162\n",
      "################################  471  ################################\n",
      "Loss:  4.943792343139648\n",
      "################################  472  ################################\n",
      "Loss:  4.929948329925537\n",
      "################################  473  ################################\n",
      "Loss:  4.915804862976074\n",
      "################################  474  ################################\n",
      "Loss:  4.901365756988525\n",
      "################################  475  ################################\n",
      "Loss:  4.8861083984375\n",
      "################################  476  ################################\n",
      "Loss:  4.8713507652282715\n",
      "################################  477  ################################\n",
      "Loss:  4.857916355133057\n",
      "################################  478  ################################\n",
      "Loss:  4.84546422958374\n",
      "################################  479  ################################\n",
      "Loss:  4.833118915557861\n",
      "################################  480  ################################\n",
      "Loss:  4.820105075836182\n",
      "################################  481  ################################\n",
      "Loss:  4.805914402008057\n",
      "################################  482  ################################\n",
      "Loss:  4.789055824279785\n",
      "################################  483  ################################\n",
      "Loss:  4.771449089050293\n",
      "################################  484  ################################\n",
      "Loss:  4.7541375160217285\n",
      "################################  485  ################################\n",
      "Loss:  4.734499454498291\n",
      "################################  486  ################################\n",
      "Loss:  4.71444845199585\n",
      "################################  487  ################################\n",
      "Loss:  4.693145275115967\n",
      "################################  488  ################################\n",
      "Loss:  4.668473243713379\n",
      "################################  489  ################################\n",
      "Loss:  4.643965244293213\n",
      "################################  490  ################################\n",
      "Loss:  4.6214680671691895\n",
      "################################  491  ################################\n",
      "Loss:  4.600172996520996\n",
      "################################  492  ################################\n",
      "Loss:  4.578762054443359\n",
      "################################  493  ################################\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1500\n",
    "start_time = time.time()\n",
    "history = fit(my_network, training_set, interior, n_epochs, optimizer_, p=2, verbose=True )\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Training time: {:.2f} seconds\".format(total_time))\n",
    "\n",
    "\n",
    "with open('PINN_5_noise_tl.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "\n",
    "f.close()\n",
    "\n",
    "model_state_dict = my_network.state_dict()\n",
    "\n",
    "# Save the model state dictionary to a file\n",
    "torch.save(model_state_dict, 'PINN_5_noise_tl.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a87c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading model\n",
    "\n",
    "# # Load the history from the pickle file\n",
    "# with open('PINN_EB.pkl', 'rb') as f:\n",
    "#     history = pickle.load(f)\n",
    "\n",
    "# # # Load the model architecture\n",
    "# # my_network = your_model_module.YourModelClass()  # Instantiate your model class\n",
    "\n",
    "# # Load the saved model state dictionary\n",
    "# model_state_dict = torch.load('PINN_EB.pth',  map_location=torch.device('cpu'))\n",
    "\n",
    "# # Load the model weights\n",
    "# my_network.load_state_dict(model_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf054d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(0, 8*pi, 10000).reshape(-1,1)\n",
    "t_test = torch.ones((10000,1))\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution(x_test, t_test).reshape(-1,1)\n",
    "my_network = my_network.cpu()\n",
    "u_test_pred = my_network(test).reshape(-1,1)\n",
    "\n",
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((u_test_pred - u_test)**2)/torch.mean(u_test**2)\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c935f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = 8*pi*torch.rand(100000).reshape(-1,1)\n",
    "t_test = torch.rand(100000).reshape(-1,1)\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution(x_test,t_test).reshape(-1,1)\n",
    "u_test_pred = my_network(test).reshape(-1,1)\n",
    "relative_error = torch.abs(u_test_pred - u_test)\n",
    "u_test = u_test.reshape(-1,)\n",
    "\n",
    "# reshaping and detach numpy\n",
    "x_test = x_test.reshape(-1, )\n",
    "t_test = t_test.reshape(-1, )\n",
    "relative_error = relative_error.reshape(-1,)\n",
    "u_test_pred = u_test_pred.reshape(-1, )\n",
    "\n",
    "x_test = x_test.detach().numpy()\n",
    "t_test = t_test.detach().numpy()\n",
    "u_test_pred = u_test_pred.detach().numpy()\n",
    "relative_error = relative_error.detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "font_path = 'times-new-roman.ttf'\n",
    "\n",
    "custom_font = FontProperties(fname=font_path)\n",
    "\n",
    "\n",
    "        \n",
    "CS1 = plt.tricontourf(x_test, t_test, u_test_pred, 20, cmap='rainbow')\n",
    "#CS1 = plt.tricontourf(x_test, t_test, u_test, 20, cmap='rainbow')\n",
    "#CS1 = plt.tricontourf(x_test, t_test, relative_error, 20, cmap='rainbow')\n",
    "\n",
    "\n",
    "\n",
    "cbar1 = plt.colorbar(CS1)\n",
    "for t in cbar1.ax.get_yticklabels():\n",
    "    t.set_fontproperties(custom_font)\n",
    "    t.set_fontsize(20)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('x', fontsize=20, fontproperties=custom_font)\n",
    "plt.ylabel('t', fontsize=20, fontproperties=custom_font)\n",
    "plt.xticks(fontsize=20, fontproperties=custom_font)\n",
    "plt.yticks(fontsize=20, fontproperties=custom_font)\n",
    "#plt.savefig('PINN_EB.pdf', dpi = 300, bbox_inches = \"tight\")\n",
    "#plt.savefig('Exact_EB.pdf', dpi = 300, bbox_inches = \"tight\")\n",
    "#plt.savefig('Absolute_error_EB.pdf', dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# Assuming you have imported your data and defined necessary functions\n",
    "\n",
    "# Rest of your code...\n",
    "\n",
    "# # Convert the font size to points\n",
    "font_size = 20\n",
    "# ticks_font = FontProperties(family='Times New Roman', style='normal', size=font_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_test = 8*pi*torch.rand(100000).reshape(-1,1)\n",
    "t_test = torch.rand(100000).reshape(-1,1)\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution(x_test,t_test).reshape(-1,1)\n",
    "u_test_pred = my_network(test).reshape(-1,1)\n",
    "relative_error = torch.abs(u_test_pred - u_test)\n",
    "u_test = u_test.reshape(-1,)\n",
    "\n",
    "# reshaping and detach numpy\n",
    "x_test = x_test.reshape(-1, )\n",
    "t_test = t_test.reshape(-1, )\n",
    "relative_error = relative_error.reshape(-1,)\n",
    "u_test_pred = u_test_pred.reshape(-1, )\n",
    "\n",
    "x_test = x_test.detach().numpy()\n",
    "t_test = t_test.detach().numpy()\n",
    "u_test_pred = u_test_pred.detach().numpy()\n",
    "relative_error = relative_error.detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "font_path = 'times-new-roman.ttf'\n",
    "\n",
    "ticks_font = FontProperties(fname=font_path)\n",
    "\n",
    "# Define the levels for contouring\n",
    "levels = np.linspace(-1.5, 1.5, 20)\n",
    "\n",
    "        \n",
    "CS1 = plt.tricontourf(x_test, t_test, u_test_pred, levels, cmap='twilight')\n",
    "#CS1 = plt.tricontourf(x_test, t_test, u_test, 20, cmap='twilight')\n",
    "#CS1 = plt.tricontourf(x_test, t_test, relative_error, 20, cmap='rainbow')\n",
    "\n",
    "\n",
    "\n",
    "#cbar1 = plt.colorbar(CS1)\n",
    "for t in cbar1.ax.get_yticklabels():\n",
    "    t.set_fontproperties(custom_font)\n",
    "    t.set_fontsize(20)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('x', fontsize=20, fontproperties=custom_font)\n",
    "plt.ylabel('t', fontsize=20, fontproperties=custom_font)\n",
    "plt.xticks(fontsize=20, fontproperties=custom_font)\n",
    "plt.yticks(fontsize=20, fontproperties=custom_font)\n",
    "#plt.savefig('Causal_EB.pdf', dpi = 300, bbox_inches = \"tight\")\n",
    "#plt.savefig('PINN_EB.pdf', dpi=500, bbox_inches=\"tight\", format='pdf', backend='cairo')\n",
    "#plt.savefig('Absolute_error_EB.pdf', dpi = 300, bbox_inches = \"tight\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01ae3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
