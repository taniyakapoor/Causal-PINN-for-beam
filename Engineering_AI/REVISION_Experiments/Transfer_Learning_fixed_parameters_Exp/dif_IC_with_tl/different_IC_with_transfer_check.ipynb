{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad512cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import pi\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb58902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causality param\n",
    "eps = 5\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Define the exact solution\n",
    "def exact_solution(x, t):\n",
    "    return torch.sin(x)*torch.exp(t)\n",
    "\n",
    "def initial_condition(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "def initial_condition_t(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "# assigning number of points\n",
    "initial_pts = 500\n",
    "left_boundary_pts = 500\n",
    "right_boundary_pts = 500\n",
    "residual_pts = 10000\n",
    "\n",
    "# Type of optimizer (ADAM or LBFGS)\n",
    "opt_type = \"LBFGS\"\n",
    "\n",
    "x_init = 8*pi*torch.rand((initial_pts,1)) # initial pts\n",
    "t_init = 0*x_init\n",
    "init = torch.cat([x_init, t_init],1).to(device)\n",
    "u_init = initial_condition(init[:,0]).reshape(-1, 1).to(device)\n",
    "u_init_t = initial_condition(init[:,0]).reshape(-1, 1).to(device)\n",
    "\n",
    "xb_left = torch.zeros((left_boundary_pts, 1)) # left spatial boundary\n",
    "tb_left = torch.rand((left_boundary_pts, 1)) #\n",
    "b_left = torch.cat([xb_left, tb_left ],1).to(device)\n",
    "u_b_l = 0*torch.sin(tb_left).to(device)\n",
    "\n",
    "xb_right = 8*pi*torch.ones((right_boundary_pts, 1)) # right spatial boundary\n",
    "tb_right = torch.rand((right_boundary_pts, 1)) # right boundary pts\n",
    "b_right = torch.cat([xb_right, tb_right ],1).to(device)\n",
    "u_b_r = 0*torch.sin(2*pi - tb_right).to(device)\n",
    "\n",
    "x_int = torch.linspace(0, 8*pi, 102)\n",
    "x_int = x_int[1:-1]\n",
    "\n",
    "t_int = torch.linspace(0, 1, 102)\n",
    "t_int = t_int[1:-1]\n",
    "\n",
    "x_interior = x_int.tile((100,))\n",
    "x_interior = x_interior.reshape(-1,1)\n",
    "\n",
    "t_interior = t_int.repeat_interleave(100)\n",
    "t_interior = t_interior.reshape(-1,1)\n",
    "\n",
    "# torch.set_printoptions(threshold=10_000)\n",
    "\n",
    "interior = torch.cat([x_interior, t_interior],1).to(device)\n",
    "\n",
    "n = 100  # size of matrix\n",
    "W = torch.tril(torch.ones(n, n), diagonal=-1).to(device)  # create a lower triangular matrix of ones\n",
    "W -= torch.diag(torch.diag(W)).to(device)  # set the diagonal elements to zero\n",
    "\n",
    "training_set = DataLoader(torch.utils.data.TensorDataset(init.to(device), u_init.to(device), u_init_t.to(device), b_left.to(device),  b_right.to(device), u_b_l.to(device), u_b_r.to(device)), batch_size=500, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79235936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension, output_dimension, n_hidden_layers, neurons):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer\n",
    "        self.neurons = neurons\n",
    "        # Number of hidden layers\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        # Activation function\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.neurons, self.neurons) for _ in range(n_hidden_layers)])\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear transformations defining the network\n",
    "        # (see equation above)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x = self.activation(l(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a572e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNet_Seq(input_dimension, output_dimension, n_hidden_layers, neurons):\n",
    "    modules = list()\n",
    "    modules.append(nn.Linear(input_dimension, neurons))\n",
    "    modules.append(nn.Tanh())\n",
    "    for _ in range(n_hidden_layers):\n",
    "        modules.append(nn.Linear(neurons, neurons))\n",
    "        modules.append(nn.Tanh())\n",
    "    modules.append(nn.Linear(neurons, output_dimension))\n",
    "    model =  nn.Sequential(*modules)\n",
    "    return model\n",
    "\n",
    "# Model definition\n",
    "my_network = NeuralNet(input_dimension = init.shape[1], output_dimension = u_init.shape[1], n_hidden_layers=4, neurons=200)\n",
    "model_state_dict = torch.load('causal_eb1.pth', map_location=torch.device('cpu'))\n",
    "my_network = my_network.to(device)\n",
    "\n",
    "# # after defining my network - also dont forget to comment xavier\n",
    "# my_network.load_state_dict(model_state_dict)\n",
    "# my_network = my_network.to(device)\n",
    "# # after defining my network - also dont forget to comment xavier\n",
    "# my_network.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76367e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('input_layer.weight',\n",
       "              tensor([[-4.1217e-02,  3.9779e-01],\n",
       "                      [-2.3341e-02, -3.8373e-01],\n",
       "                      [ 4.8723e-02, -3.4235e-01],\n",
       "                      [-3.5261e-01, -2.9153e-01],\n",
       "                      [-3.9347e-01,  3.3110e-01],\n",
       "                      [ 9.3517e-03,  3.3976e-01],\n",
       "                      [ 2.7382e-01,  2.0674e-01],\n",
       "                      [ 6.1502e-01, -1.5997e-01],\n",
       "                      [-5.4564e-02,  2.5594e-01],\n",
       "                      [ 4.2477e-01,  1.1277e-01],\n",
       "                      [ 3.7104e-02,  4.4382e-02],\n",
       "                      [ 1.8099e-02,  1.8369e-01],\n",
       "                      [ 2.7865e-02,  1.2604e-02],\n",
       "                      [ 3.8196e-01,  8.4665e-02],\n",
       "                      [-6.0569e-02,  3.5445e-01],\n",
       "                      [-6.0163e-01,  1.3410e-01],\n",
       "                      [ 7.5129e-01, -5.2868e-02],\n",
       "                      [ 2.8001e-01, -1.3908e-01],\n",
       "                      [-2.7274e-01,  1.8141e-01],\n",
       "                      [-1.9149e-01, -3.5476e-01],\n",
       "                      [ 4.6174e-01,  1.2505e-01],\n",
       "                      [-2.4511e-01,  4.2722e-01],\n",
       "                      [-4.8669e-01,  1.1729e-02],\n",
       "                      [-1.6723e-01, -1.9353e-01],\n",
       "                      [-4.1253e-01, -2.4686e-02],\n",
       "                      [ 3.9500e-02, -3.5713e-01],\n",
       "                      [-4.6559e-02,  2.7641e-01],\n",
       "                      [-6.3170e-01,  2.1280e-01],\n",
       "                      [-7.2085e-02, -1.4225e-01],\n",
       "                      [-3.1086e-02,  2.2689e-01],\n",
       "                      [-6.8181e-03, -4.5411e-01],\n",
       "                      [ 2.4233e-01, -2.5821e-02],\n",
       "                      [-2.8100e-03,  1.2145e-01],\n",
       "                      [ 4.1894e-02,  3.8224e-02],\n",
       "                      [ 4.9067e-02, -2.0873e-01],\n",
       "                      [-4.8383e-02, -1.4559e-01],\n",
       "                      [ 3.5225e-02, -2.0981e-01],\n",
       "                      [-5.8618e-01,  6.5794e-03],\n",
       "                      [-4.6643e-02, -2.0037e-01],\n",
       "                      [ 5.6809e-01, -1.7785e-01],\n",
       "                      [ 6.6562e-01, -1.3489e-01],\n",
       "                      [ 4.2863e-03,  4.1933e-01],\n",
       "                      [-2.1951e-01, -2.1738e-01],\n",
       "                      [-3.9811e-01,  3.4010e-02],\n",
       "                      [-3.9777e-02,  4.6239e-03],\n",
       "                      [-1.9568e-01,  1.7738e-01],\n",
       "                      [-4.0585e-01, -1.6228e-01],\n",
       "                      [-2.8730e-01,  4.6067e-02],\n",
       "                      [-2.5726e-01, -1.4369e-02],\n",
       "                      [ 3.5788e-02,  2.3394e-01],\n",
       "                      [-3.1758e-02, -2.0211e-01],\n",
       "                      [-3.8039e-02,  3.5584e-01],\n",
       "                      [ 1.4248e-01, -4.9290e-01],\n",
       "                      [ 4.4903e-02, -1.4727e-01],\n",
       "                      [ 2.7225e-01, -3.0961e-02],\n",
       "                      [-3.1575e-02,  1.3025e-01],\n",
       "                      [-6.9980e-02, -6.2239e-02],\n",
       "                      [-2.5311e-02, -1.4930e-01],\n",
       "                      [-6.0606e-02, -2.4102e-01],\n",
       "                      [ 4.1108e-01,  1.9573e-01],\n",
       "                      [-4.6147e-02,  2.2244e-01],\n",
       "                      [ 3.9334e-02,  1.0663e-01],\n",
       "                      [ 1.1555e-02, -1.7851e-01],\n",
       "                      [ 3.3834e-02, -2.7209e-01],\n",
       "                      [ 2.6403e-02,  1.0515e-02],\n",
       "                      [ 1.1704e-02, -4.1784e-01],\n",
       "                      [ 2.4131e-02,  6.9545e-01],\n",
       "                      [-2.0656e-01, -2.2821e-01],\n",
       "                      [-2.1532e-02, -3.3835e-01],\n",
       "                      [ 3.0560e-02,  8.5622e-02],\n",
       "                      [-1.2992e-01,  2.8727e-01],\n",
       "                      [-2.1642e-02, -2.0754e-01],\n",
       "                      [ 8.0928e-03,  2.8404e-01],\n",
       "                      [ 3.9001e-01,  2.8057e-01],\n",
       "                      [ 1.9866e-01,  1.6347e-01],\n",
       "                      [ 5.3986e-02, -4.1433e-01],\n",
       "                      [ 3.7637e-01,  3.2144e-01],\n",
       "                      [-4.7030e-01, -1.7988e-01],\n",
       "                      [ 1.5604e-01,  1.5744e-01],\n",
       "                      [-4.0021e-02, -4.1384e-01],\n",
       "                      [ 1.0097e-02,  6.5644e-01],\n",
       "                      [-3.6802e-01,  4.5706e-03],\n",
       "                      [ 2.1057e-02,  4.2514e-01],\n",
       "                      [ 3.5748e-02, -4.8626e-01],\n",
       "                      [ 2.4565e-01, -2.0448e-01],\n",
       "                      [ 3.6275e-01,  3.0165e-01],\n",
       "                      [ 1.6481e-01, -2.0982e-01],\n",
       "                      [-1.6249e-01, -7.9746e-02],\n",
       "                      [-2.7843e-01,  2.5795e-01],\n",
       "                      [-5.3555e-02,  1.9849e-01],\n",
       "                      [ 6.5039e-03, -5.8650e-01],\n",
       "                      [ 4.9239e-01,  1.6427e-02],\n",
       "                      [ 2.8743e-01, -1.7366e-01],\n",
       "                      [-5.6797e-01, -4.2091e-01],\n",
       "                      [ 3.3088e-01, -3.7846e-01],\n",
       "                      [ 4.0502e-01,  2.4048e-01],\n",
       "                      [-2.0139e-02, -1.4969e-01],\n",
       "                      [ 2.0222e-01,  8.3686e-02],\n",
       "                      [-8.0337e-02,  1.0946e-01],\n",
       "                      [ 1.2415e-01,  1.7106e-01],\n",
       "                      [-3.0502e-02, -1.2840e-01],\n",
       "                      [-2.0811e-01, -4.7043e-02],\n",
       "                      [ 1.5928e-01, -6.9818e-02],\n",
       "                      [-4.2328e-02, -1.0259e-01],\n",
       "                      [ 1.4497e-01, -2.8796e-01],\n",
       "                      [ 3.4602e-01, -2.9564e-04],\n",
       "                      [ 4.5200e-01,  3.7942e-01],\n",
       "                      [ 2.6171e-01, -2.7719e-01],\n",
       "                      [ 1.5029e-02,  3.2004e-01],\n",
       "                      [-6.8147e-02, -2.3644e-01],\n",
       "                      [-4.1699e-01,  2.1064e-01],\n",
       "                      [-2.5701e-02, -9.4169e-02],\n",
       "                      [-4.9964e-02,  1.7334e-01],\n",
       "                      [ 4.3165e-02, -2.5521e-01],\n",
       "                      [-2.9713e-01,  2.4636e-01],\n",
       "                      [-6.4405e-02, -1.6614e-01],\n",
       "                      [-3.7972e-02, -8.1339e-02],\n",
       "                      [ 2.5633e-01, -1.6075e-01],\n",
       "                      [ 2.8655e-02, -2.5345e-02],\n",
       "                      [-2.5893e-02, -1.2861e-01],\n",
       "                      [-6.7889e-02,  4.2429e-01],\n",
       "                      [ 2.8816e-01, -2.9555e-01],\n",
       "                      [-3.5302e-02,  1.3389e-01],\n",
       "                      [-3.9641e-02, -1.3444e-02],\n",
       "                      [-4.4824e-02,  1.6968e-01],\n",
       "                      [ 3.7713e-01,  1.9642e-01],\n",
       "                      [-1.2800e-01,  2.7784e-01],\n",
       "                      [ 4.8252e-02,  4.3848e-01],\n",
       "                      [-2.6857e-02, -1.1736e-01],\n",
       "                      [ 4.3344e-01,  4.1167e-02],\n",
       "                      [ 4.5806e-01,  4.7304e-02],\n",
       "                      [-3.5935e-02,  4.1379e-01],\n",
       "                      [-3.4109e-02, -9.8958e-02],\n",
       "                      [-2.7530e-01, -2.4914e-01],\n",
       "                      [ 3.8893e-02,  2.0025e-01],\n",
       "                      [ 2.8964e-01,  1.9516e-01],\n",
       "                      [ 5.7908e-02, -1.0350e-01],\n",
       "                      [ 3.2573e-01, -5.8778e-03],\n",
       "                      [-3.9831e-01, -5.1991e-02],\n",
       "                      [ 1.6503e-02,  2.8208e-01],\n",
       "                      [ 3.4217e-01, -2.7890e-01],\n",
       "                      [-7.2294e-01,  3.0575e-03],\n",
       "                      [ 2.8760e-01, -1.7334e-01],\n",
       "                      [-4.1363e-02,  3.2807e-01],\n",
       "                      [-1.7320e-02, -4.0780e-01],\n",
       "                      [ 4.6566e-02, -2.5003e-01],\n",
       "                      [-3.3061e-01,  1.4935e-01],\n",
       "                      [-3.4249e-01, -1.1910e-01],\n",
       "                      [-1.7602e-01,  1.5950e-01],\n",
       "                      [ 9.6573e-02,  1.8741e-02],\n",
       "                      [ 2.7061e-02,  4.4818e-02],\n",
       "                      [ 3.8797e-02, -2.2516e-01],\n",
       "                      [ 1.1806e-01, -3.7929e-01],\n",
       "                      [-1.6282e-02, -1.6106e-01],\n",
       "                      [ 2.4179e-01,  2.4065e-01],\n",
       "                      [ 6.2500e-02,  5.1036e-02],\n",
       "                      [-2.4012e-01,  1.0461e-01],\n",
       "                      [ 3.9607e-01,  1.2707e-01],\n",
       "                      [ 2.3780e-02, -9.1050e-03],\n",
       "                      [-1.3205e-01, -3.7648e-02],\n",
       "                      [-7.1658e-02,  2.1408e-01],\n",
       "                      [-3.0440e-01, -4.9980e-01],\n",
       "                      [-5.2117e-01, -1.6421e-01],\n",
       "                      [-2.6110e-02, -4.5892e-02],\n",
       "                      [-1.0479e-02,  2.3486e-01],\n",
       "                      [ 4.7447e-01, -4.1185e-02],\n",
       "                      [-2.3189e-01, -6.3095e-02],\n",
       "                      [ 3.5089e-02, -4.8430e-01],\n",
       "                      [-1.4420e-01, -2.6387e-01],\n",
       "                      [ 5.1226e-02, -3.5832e-01],\n",
       "                      [-1.0846e-01, -6.4740e-02],\n",
       "                      [-4.7289e-01,  1.3297e-01],\n",
       "                      [-3.2235e-01, -9.6128e-03],\n",
       "                      [-3.4909e-02,  9.9754e-02],\n",
       "                      [ 5.1952e-01, -8.3315e-04],\n",
       "                      [-4.1042e-01, -2.1776e-01],\n",
       "                      [-5.2232e-01,  2.8318e-02],\n",
       "                      [-7.9435e-02,  1.1258e-02],\n",
       "                      [-5.3452e-02,  2.6692e-01],\n",
       "                      [ 1.5247e-02,  3.7069e-01],\n",
       "                      [-2.4496e-02,  9.0354e-04],\n",
       "                      [ 2.6098e-01,  9.0072e-02],\n",
       "                      [ 3.8542e-01, -2.2830e-01],\n",
       "                      [-4.5532e-01, -1.9881e-02],\n",
       "                      [ 5.2727e-01,  8.5771e-02],\n",
       "                      [-3.4884e-02,  3.5815e-01],\n",
       "                      [-3.6168e-01, -6.7152e-02],\n",
       "                      [ 3.0858e-01,  1.5708e-01],\n",
       "                      [ 1.5423e-02, -4.3855e-01],\n",
       "                      [ 3.6842e-01, -1.4252e-02],\n",
       "                      [ 1.2282e-02, -5.6946e-01],\n",
       "                      [-3.9593e-02, -7.8049e-02],\n",
       "                      [-3.4269e-01,  3.6368e-01],\n",
       "                      [-1.0312e-02,  3.5882e-01],\n",
       "                      [-4.8118e-02, -2.9450e-01],\n",
       "                      [-1.0887e-01,  4.3025e-01],\n",
       "                      [ 3.1034e-01,  1.7513e-01],\n",
       "                      [-5.6803e-03, -3.7004e-01],\n",
       "                      [-9.1692e-02, -1.8487e-01],\n",
       "                      [-2.1476e-02, -1.6170e-01]])),\n",
       "             ('input_layer.bias',\n",
       "              tensor([ 1.1399e-01,  4.6516e-02, -3.2062e-01, -1.3022e-02,  7.0050e-02,\n",
       "                      -7.8019e-02,  2.5992e-01,  2.9908e-01,  2.3370e-01,  1.9475e-01,\n",
       "                      -2.4838e-01, -7.6040e-03, -1.2898e-01,  2.9970e-01,  8.0686e-02,\n",
       "                      -1.5371e-01,  9.3012e-02,  2.0074e-01,  2.8236e-01, -1.1484e-01,\n",
       "                      -2.1111e-02,  1.1120e-01, -2.5023e-01, -2.9942e-02, -1.2053e-01,\n",
       "                      -6.8871e-01, -4.1319e-02, -4.3854e-02, -1.4002e-01,  5.8159e-02,\n",
       "                      -6.7497e-02, -1.2985e-01,  2.5272e-01, -1.8737e-01, -2.2138e-01,\n",
       "                       3.2317e-01,  3.2238e-02, -3.5430e-02,  3.4743e-01, -7.3868e-02,\n",
       "                       3.7165e-02, -2.4507e-01, -3.5530e-01,  6.0298e-02, -7.5614e-02,\n",
       "                       1.2838e-01, -2.8307e-02, -2.5597e-01,  2.9742e-02, -3.6079e-01,\n",
       "                       3.3738e-01,  1.7871e-01, -2.4402e-02, -1.5992e-01,  1.6794e-01,\n",
       "                       1.2723e-01,  2.8106e-01,  2.6735e-01, -8.4313e-02, -5.6183e-02,\n",
       "                       3.3386e-01, -1.4864e-01,  6.9333e-02, -7.4027e-02, -1.1526e-01,\n",
       "                      -9.6543e-02, -1.7196e-02,  2.3542e-02,  3.7121e-01,  1.6582e-01,\n",
       "                      -1.4469e-01,  9.3352e-02, -2.6680e-01, -8.5833e-02,  1.4876e-01,\n",
       "                      -2.4162e-01,  1.8168e-03, -1.4943e-01, -2.9746e-01,  1.6352e-01,\n",
       "                      -1.0346e-01, -7.7627e-02, -4.7660e-02, -5.3944e-02, -1.4093e-01,\n",
       "                       2.2223e-01, -1.2472e-01, -8.8147e-02,  1.2794e-01,  4.1066e-01,\n",
       "                       2.7080e-01, -1.5500e-01,  1.9302e-02, -1.2888e-01, -3.0344e-02,\n",
       "                       1.8288e-01,  1.1150e-01, -5.0712e-02, -6.1872e-03,  1.8758e-02,\n",
       "                      -1.1210e-01, -1.5215e-01, -1.6737e-01, -6.8905e-02,  7.3146e-02,\n",
       "                       1.5315e-02,  1.4351e-01,  3.8561e-02,  9.6672e-02, -1.7412e-01,\n",
       "                       1.4118e-01,  3.9494e-04,  1.8059e-01, -3.1655e-01,  6.7743e-02,\n",
       "                      -1.4690e-01,  1.7247e-01,  1.6251e-01, -3.4499e-01,  3.0785e-03,\n",
       "                       3.1424e-01, -8.7466e-02,  2.6715e-01,  3.2555e-01,  1.3568e-01,\n",
       "                       1.1899e-01,  2.2165e-01,  2.2013e-01,  1.3074e-01,  3.2594e-01,\n",
       "                       4.4550e-02,  2.0181e-01, -1.0451e-01,  3.8734e-01, -1.1916e-01,\n",
       "                       2.7593e-01, -2.5899e-01,  2.0240e-02,  1.0639e-01, -2.5462e-01,\n",
       "                       1.4220e-01, -1.6081e-01, -2.3023e-01,  3.5077e-01,  3.2438e-01,\n",
       "                      -3.9216e-01,  2.6401e-02, -2.1136e-01, -5.9285e-03, -1.6435e-01,\n",
       "                      -2.7226e-01, -1.6991e-01, -1.2969e-01, -1.4290e-01,  1.4436e-01,\n",
       "                      -5.2102e-02,  1.5636e-01,  4.5986e-02,  2.5968e-02,  1.4916e-01,\n",
       "                       3.4043e-01, -2.0730e-01,  4.6178e-02,  1.9954e-01,  4.4741e-02,\n",
       "                       8.6960e-02, -2.4156e-01, -2.5200e-01,  4.0628e-02, -3.6574e-01,\n",
       "                       3.3522e-01, -1.2390e-01, -6.6621e-02,  8.8886e-02,  3.9949e-02,\n",
       "                      -6.7897e-03,  4.8035e-02,  8.7073e-02,  3.2982e-01, -1.5834e-01,\n",
       "                       1.2622e-01,  7.2167e-02,  2.0680e-01, -5.1646e-02,  7.0026e-02,\n",
       "                       3.3928e-01, -4.3489e-02,  8.8472e-02, -2.2025e-01,  6.8946e-02,\n",
       "                      -1.8475e-01,  3.1519e-01, -1.2166e-01, -1.0845e-01,  5.2138e-02,\n",
       "                       2.4851e-01,  1.7561e-01,  2.2940e-02, -3.8928e-02,  2.0833e-01])),\n",
       "             ('hidden_layers.0.weight',\n",
       "              tensor([[ 0.1046,  0.1380, -0.1711,  ...,  0.1795,  0.0283, -0.1115],\n",
       "                      [ 0.0224, -0.1112, -0.0845,  ...,  0.0931, -0.0420, -0.0206],\n",
       "                      [-0.0495,  0.0994, -0.2440,  ...,  0.2113,  0.1670,  0.0428],\n",
       "                      ...,\n",
       "                      [ 0.1746, -0.0402,  0.1064,  ..., -0.2202,  0.0013, -0.1176],\n",
       "                      [ 0.0657, -0.1022, -0.1538,  ..., -0.0768, -0.0824, -0.1321],\n",
       "                      [ 0.0862,  0.0672,  0.1007,  ..., -0.0552, -0.1308, -0.0730]])),\n",
       "             ('hidden_layers.0.bias',\n",
       "              tensor([ 0.0859,  0.0370,  0.1581, -0.0307,  0.0730,  0.0878,  0.0025,  0.1019,\n",
       "                       0.0133, -0.1046, -0.0049, -0.0100, -0.0372, -0.0150, -0.0674,  0.0488,\n",
       "                       0.1539, -0.0637,  0.0279,  0.1186,  0.0425,  0.0464,  0.0220,  0.0162,\n",
       "                       0.0244,  0.1203, -0.0380, -0.0557,  0.0432,  0.0605, -0.0737,  0.0999,\n",
       "                      -0.0631,  0.1222, -0.0399, -0.0418, -0.0719,  0.0188, -0.0767, -0.0720,\n",
       "                      -0.1341,  0.0077,  0.0331, -0.0403, -0.0533, -0.0165,  0.0701,  0.0853,\n",
       "                       0.0808, -0.0101,  0.0282,  0.0830,  0.0974,  0.0959,  0.0863, -0.0261,\n",
       "                      -0.0386, -0.0251, -0.0180,  0.0486, -0.0213, -0.0081, -0.0073, -0.0455,\n",
       "                       0.1353,  0.0327, -0.0804,  0.0612,  0.0463,  0.0922, -0.0790, -0.0644,\n",
       "                      -0.0507, -0.1374,  0.0420,  0.0376,  0.0444, -0.0558,  0.1714, -0.0738,\n",
       "                       0.0376, -0.0909,  0.1366,  0.0195,  0.1019, -0.0150, -0.1276, -0.0051,\n",
       "                       0.0864, -0.0306, -0.0168, -0.1609,  0.0819,  0.0580,  0.0397,  0.0609,\n",
       "                      -0.1516, -0.1348,  0.0250, -0.1665,  0.1425,  0.2215, -0.0656,  0.1992,\n",
       "                      -0.0352, -0.0712,  0.0050,  0.1456, -0.0301, -0.0198,  0.0629,  0.0265,\n",
       "                       0.0141,  0.1078,  0.1109,  0.1140, -0.0964,  0.0590,  0.0359, -0.0760,\n",
       "                      -0.0399,  0.0013, -0.1016,  0.0746,  0.1057, -0.0467, -0.0959,  0.0880,\n",
       "                      -0.0981,  0.0224, -0.0762,  0.0159,  0.1716,  0.0763,  0.0721, -0.0188,\n",
       "                      -0.0183, -0.0146,  0.0537,  0.0333,  0.0139,  0.0574,  0.0727, -0.0535,\n",
       "                       0.0481, -0.1007, -0.0096, -0.0523,  0.0374,  0.0096,  0.0260, -0.0265,\n",
       "                      -0.0726, -0.0347,  0.0963,  0.0644, -0.0585, -0.0617, -0.0068,  0.2055,\n",
       "                      -0.0902, -0.0233, -0.0007, -0.0415,  0.1549,  0.0047,  0.0176, -0.0976,\n",
       "                      -0.1124,  0.1425, -0.1401,  0.0573,  0.0212,  0.1264,  0.0944,  0.0090,\n",
       "                      -0.0319,  0.0205,  0.0624, -0.0174, -0.0158,  0.0298, -0.0427,  0.0441,\n",
       "                      -0.0081,  0.0686,  0.0011,  0.1176, -0.0235,  0.0520,  0.0902,  0.0139,\n",
       "                      -0.0187, -0.0121, -0.1043,  0.0207,  0.0367,  0.2250,  0.0481,  0.0406])),\n",
       "             ('hidden_layers.1.weight',\n",
       "              tensor([[ 0.0669,  0.1198,  0.1354,  ...,  0.1364, -0.0822,  0.1016],\n",
       "                      [ 0.1108,  0.0365,  0.0481,  ..., -0.0198,  0.1121,  0.1655],\n",
       "                      [-0.1105, -0.2037,  0.0129,  ..., -0.1091,  0.0341,  0.0044],\n",
       "                      ...,\n",
       "                      [-0.0923,  0.1746,  0.0253,  ...,  0.1253, -0.0535,  0.1234],\n",
       "                      [ 0.1913, -0.0975,  0.1498,  ...,  0.1105, -0.1108,  0.0033],\n",
       "                      [ 0.1459, -0.1569,  0.0815,  ...,  0.1526, -0.1881,  0.1119]])),\n",
       "             ('hidden_layers.1.bias',\n",
       "              tensor([-0.0068,  0.0034, -0.0416,  0.0385, -0.0275,  0.0704,  0.0467,  0.0266,\n",
       "                       0.0269, -0.0341,  0.0380,  0.0312,  0.0138, -0.0384, -0.0044,  0.0413,\n",
       "                      -0.0097,  0.0624, -0.0423, -0.0200,  0.0303, -0.0188,  0.0064,  0.0463,\n",
       "                       0.0522,  0.0168,  0.0136,  0.0025, -0.0071,  0.0100, -0.0043, -0.0522,\n",
       "                      -0.0100, -0.0250,  0.0289, -0.0736, -0.0181, -0.0377, -0.0185,  0.0458,\n",
       "                      -0.0121, -0.0119,  0.0271, -0.0290,  0.0337,  0.0090, -0.0164,  0.0007,\n",
       "                      -0.0222, -0.0050, -0.0362,  0.0011,  0.0569,  0.0475,  0.0086, -0.0160,\n",
       "                       0.0707, -0.0350,  0.0767, -0.0747,  0.0021, -0.0140, -0.0049, -0.0529,\n",
       "                      -0.0443,  0.0715,  0.0628,  0.0398,  0.0872,  0.0369,  0.0076,  0.0255,\n",
       "                       0.0219, -0.0176, -0.0395, -0.0247,  0.0695, -0.0936, -0.0361,  0.0124,\n",
       "                      -0.0195, -0.0413, -0.0018,  0.0267,  0.0604,  0.0236, -0.0277, -0.0418,\n",
       "                       0.0340,  0.0162,  0.0420, -0.0037,  0.0537, -0.0016,  0.0726,  0.0341,\n",
       "                      -0.0432, -0.0478,  0.0305, -0.0792,  0.0630, -0.0591,  0.0733,  0.0140,\n",
       "                      -0.0268,  0.0416, -0.0082, -0.0008, -0.0244,  0.0540, -0.0629, -0.0343,\n",
       "                      -0.0191, -0.0593, -0.0160,  0.0166, -0.0331,  0.0546,  0.0143,  0.0583,\n",
       "                      -0.0365, -0.0711, -0.0011,  0.0217,  0.0022, -0.0579,  0.0066,  0.0115,\n",
       "                      -0.0390,  0.0509,  0.0194, -0.0442, -0.0127, -0.0793, -0.0099, -0.0671,\n",
       "                      -0.0326, -0.0110,  0.0317, -0.0255, -0.0059, -0.0235,  0.0930, -0.0278,\n",
       "                       0.0030, -0.0436, -0.0354,  0.0572, -0.0161,  0.0343, -0.1172, -0.0293,\n",
       "                      -0.0452, -0.0089,  0.0012, -0.0777, -0.0360, -0.0258,  0.0075, -0.0606,\n",
       "                       0.0086,  0.0198, -0.0860, -0.0309, -0.0518,  0.0069,  0.0046, -0.0965,\n",
       "                       0.0375, -0.0091,  0.0350, -0.0275, -0.0586, -0.0053,  0.0498, -0.0112,\n",
       "                       0.0669,  0.0145,  0.0046,  0.0801,  0.0568,  0.0140,  0.0113,  0.0004,\n",
       "                      -0.0349,  0.0106, -0.0015,  0.0034,  0.0409, -0.0605,  0.0411,  0.0201,\n",
       "                       0.0323,  0.0449, -0.0587,  0.0123, -0.0584,  0.0179,  0.0258,  0.0379])),\n",
       "             ('hidden_layers.2.weight',\n",
       "              tensor([[ 1.0669e-01, -3.6015e-02, -2.0791e-01,  ...,  6.4242e-02,\n",
       "                        9.5805e-02,  1.4890e-01],\n",
       "                      [ 2.3483e-01,  1.1727e-05,  2.6225e-02,  ...,  9.7407e-02,\n",
       "                        1.6595e-01, -1.6545e-01],\n",
       "                      [ 1.6204e-02,  1.6020e-01,  1.0227e-01,  ..., -8.5098e-02,\n",
       "                        8.7885e-02,  1.0688e-01],\n",
       "                      ...,\n",
       "                      [-1.7858e-01,  1.0609e-01,  7.8679e-02,  ..., -4.8720e-02,\n",
       "                       -2.0938e-01,  1.7960e-01],\n",
       "                      [-7.2042e-02,  2.3014e-01,  1.7415e-01,  ..., -2.0387e-01,\n",
       "                        5.0393e-02, -3.5900e-01],\n",
       "                      [ 1.4170e-01, -6.0703e-02, -1.3969e-01,  ...,  2.0856e-01,\n",
       "                       -2.5380e-02,  1.2153e-01]])),\n",
       "             ('hidden_layers.2.bias',\n",
       "              tensor([ 3.6504e-02, -2.8098e-02, -7.1477e-03, -2.4411e-02,  1.0232e-02,\n",
       "                       4.4537e-02,  6.2278e-02,  3.8125e-02, -8.4810e-03, -1.0122e-02,\n",
       "                      -4.9010e-02,  2.5521e-02, -9.1122e-03, -7.4446e-03, -1.0165e-02,\n",
       "                      -4.9772e-02, -1.3430e-02, -6.9862e-03, -3.5357e-03, -1.6643e-02,\n",
       "                       3.6019e-02,  1.7969e-02, -1.5131e-02,  1.6306e-02,  3.2599e-02,\n",
       "                      -2.7890e-02, -7.6404e-03,  6.4267e-03,  4.1275e-02, -4.8222e-02,\n",
       "                      -5.2228e-02, -1.5545e-02,  8.1409e-03, -3.6359e-02,  1.9891e-02,\n",
       "                       7.8213e-03,  2.1738e-02,  1.5062e-02, -4.3710e-02, -3.0381e-02,\n",
       "                       2.0938e-02, -8.0168e-03, -3.6647e-03, -2.1198e-02, -5.0556e-03,\n",
       "                      -2.3409e-02, -1.5787e-02,  6.2248e-02,  4.5024e-02,  2.4008e-02,\n",
       "                       1.5776e-02,  7.8768e-03,  4.5201e-02,  8.9695e-03,  1.7959e-02,\n",
       "                      -5.4980e-03, -3.0477e-02,  1.2220e-02, -3.4844e-02,  1.2250e-02,\n",
       "                       1.7637e-02, -3.1364e-03, -8.7676e-03,  1.1380e-02,  5.3991e-02,\n",
       "                       2.9269e-02,  1.4721e-02, -3.2749e-02, -1.2797e-02,  6.1287e-03,\n",
       "                       1.0117e-02, -2.8284e-02,  2.4471e-03,  3.5667e-02, -3.5015e-02,\n",
       "                       1.3505e-02, -3.8055e-02, -2.5849e-02, -7.3466e-02,  4.5309e-02,\n",
       "                       6.0525e-03,  1.3245e-02, -1.2108e-02, -4.8644e-03,  3.1135e-02,\n",
       "                       1.8998e-02,  6.3470e-02,  9.4041e-02,  1.4250e-02,  3.8794e-03,\n",
       "                       3.7627e-03,  3.1618e-02, -2.8686e-02,  3.7121e-02,  9.9887e-03,\n",
       "                      -5.5001e-03, -4.0625e-04,  2.4997e-02, -1.4325e-02,  1.7431e-02,\n",
       "                      -5.9822e-02, -2.4269e-02,  1.7158e-02,  3.7550e-03, -2.1960e-02,\n",
       "                       6.8619e-03, -2.7061e-02,  6.2446e-03, -5.1875e-02,  2.4225e-02,\n",
       "                       1.0985e-02,  5.2308e-02,  9.0097e-03, -2.1031e-02,  3.0119e-02,\n",
       "                      -2.9034e-02,  2.8870e-02, -4.4382e-02,  1.4247e-02, -1.2376e-02,\n",
       "                       1.0838e-03, -2.1833e-02,  1.0499e-03, -5.2194e-02,  4.5860e-02,\n",
       "                      -3.4840e-03,  1.2114e-03, -1.7969e-02,  4.9999e-02,  2.5788e-02,\n",
       "                       6.7380e-05,  5.6189e-04,  4.8026e-02, -4.0138e-03, -9.1845e-03,\n",
       "                      -8.8705e-03,  2.9779e-02,  7.9078e-02,  1.4115e-02, -2.0660e-03,\n",
       "                       1.4423e-02,  7.3830e-03, -2.1082e-02, -4.1733e-03,  5.4031e-02,\n",
       "                       5.2243e-02,  2.2225e-02, -1.8947e-02, -3.3379e-02, -1.9673e-02,\n",
       "                      -2.4277e-02,  4.3823e-03, -1.5992e-02, -1.4926e-02, -2.4227e-02,\n",
       "                       4.1926e-02, -9.9414e-03, -3.8015e-02,  8.1449e-03,  1.9436e-02,\n",
       "                       4.1810e-02,  2.5298e-02,  3.1678e-03,  4.9693e-02, -9.1436e-03,\n",
       "                      -5.9609e-02,  2.0264e-03, -1.7424e-02,  1.1667e-02, -8.2125e-03,\n",
       "                       1.8592e-02, -5.8449e-03,  5.3216e-02, -1.1645e-02, -4.8877e-02,\n",
       "                       2.0721e-02,  1.1116e-02, -7.4691e-04,  2.4755e-03, -1.2394e-03,\n",
       "                      -3.8950e-03,  3.3417e-02,  3.6384e-02,  4.8268e-03, -3.1038e-02,\n",
       "                       1.1632e-02, -4.2585e-02,  5.1360e-02, -6.9920e-03,  5.8776e-05,\n",
       "                      -1.6191e-02,  8.8216e-03, -6.0393e-04, -9.7349e-04, -3.6574e-03,\n",
       "                      -3.4019e-02,  5.7901e-02, -1.0106e-02, -5.5662e-02,  4.0831e-02])),\n",
       "             ('hidden_layers.3.weight',\n",
       "              tensor([[ 0.2306,  0.1342, -0.0065,  ..., -0.1369, -0.1301,  0.2162],\n",
       "                      [-0.1099,  0.1701,  0.0928,  ..., -0.0747, -0.2164, -0.1666],\n",
       "                      [ 0.1390,  0.0234,  0.1420,  ..., -0.1973,  0.0728,  0.1698],\n",
       "                      ...,\n",
       "                      [ 0.1594,  0.1191, -0.0619,  ..., -0.1113,  0.1705,  0.2199],\n",
       "                      [-0.1357, -0.0392,  0.1682,  ..., -0.0102,  0.0003, -0.0931],\n",
       "                      [ 0.0156, -0.0330,  0.0466,  ..., -0.1079,  0.0728,  0.1685]])),\n",
       "             ('hidden_layers.3.bias',\n",
       "              tensor([ 3.3447e-02, -3.8660e-02, -1.2533e-02,  1.2788e-02,  1.6778e-03,\n",
       "                      -3.7189e-02,  6.4795e-04, -1.5213e-03,  1.4601e-03,  7.0366e-03,\n",
       "                       1.1121e-02,  2.1210e-02,  1.5623e-02,  2.9893e-02, -2.9132e-03,\n",
       "                       1.0178e-03, -7.4711e-03, -4.1016e-02,  1.8596e-03,  2.2179e-03,\n",
       "                       1.1019e-02,  3.9081e-03,  1.9522e-02, -2.8879e-02,  8.0518e-04,\n",
       "                       3.1983e-02, -4.2813e-03, -2.5839e-03, -3.0953e-03, -4.7962e-03,\n",
       "                       3.6698e-03,  1.6717e-02,  5.2598e-02,  2.9724e-03, -2.3639e-02,\n",
       "                       4.3021e-02, -2.9178e-02, -7.2890e-03,  2.1537e-03,  3.2850e-02,\n",
       "                       2.5197e-02,  1.9671e-03, -1.4895e-02,  4.2268e-03, -1.9777e-02,\n",
       "                       1.6920e-02,  2.8397e-02,  4.5355e-02, -4.0931e-03, -4.7944e-02,\n",
       "                       4.0135e-03, -9.5669e-03, -4.3167e-03, -1.3496e-02,  8.3057e-03,\n",
       "                       2.0702e-02, -7.5420e-03, -5.6880e-03, -1.2631e-03, -2.0860e-02,\n",
       "                      -9.1193e-03,  1.5357e-02, -3.3587e-04,  2.1950e-02, -7.0168e-02,\n",
       "                      -4.1861e-03, -1.5469e-03,  5.4882e-03,  5.6950e-02, -3.8052e-02,\n",
       "                       6.1744e-03,  3.7141e-02,  7.5256e-03, -2.2492e-03,  7.6432e-03,\n",
       "                      -4.1085e-02, -1.6299e-02,  3.6101e-02, -2.5729e-03, -2.5318e-03,\n",
       "                      -7.3287e-03, -1.5556e-04,  4.2220e-03, -5.5370e-02, -4.1906e-02,\n",
       "                       4.7348e-03,  1.2096e-03,  2.5366e-02,  2.1930e-02,  1.0550e-02,\n",
       "                      -2.9742e-02,  1.2005e-02,  7.7752e-03,  6.5660e-03,  1.1124e-02,\n",
       "                       1.7817e-02,  2.6589e-02,  3.2336e-02,  4.7062e-02, -1.1054e-05,\n",
       "                       3.5695e-03, -2.2400e-02, -3.3147e-02,  4.6832e-02, -3.5998e-05,\n",
       "                      -2.1141e-02, -1.4055e-02,  5.2349e-04,  1.4587e-02, -3.2192e-02,\n",
       "                       4.5866e-03, -1.3688e-03,  1.3990e-03,  2.9474e-02,  2.2674e-02,\n",
       "                       8.0396e-04, -4.9868e-04,  3.6494e-02, -9.4537e-03, -3.3017e-02,\n",
       "                      -4.3751e-02, -3.6302e-02, -1.2391e-02,  2.0586e-02, -1.0237e-03,\n",
       "                      -3.1543e-03,  1.9227e-02,  3.4075e-02, -2.2771e-02, -4.5378e-02,\n",
       "                      -7.9686e-03, -7.1836e-03, -3.9284e-03, -1.0001e-03,  2.3902e-02,\n",
       "                       3.2457e-02, -5.2077e-03,  9.8942e-04, -4.0399e-02,  3.2432e-02,\n",
       "                      -1.2909e-02,  1.4007e-02, -3.2020e-02, -4.8050e-03, -3.3423e-02,\n",
       "                      -4.1341e-02,  6.1751e-03,  4.1554e-03,  1.3900e-03, -1.0714e-02,\n",
       "                      -1.5352e-02, -8.1560e-03, -1.1502e-02, -3.9939e-03, -5.7486e-03,\n",
       "                       2.3089e-02,  2.5494e-02,  1.0318e-02, -1.9635e-02,  2.2459e-03,\n",
       "                       9.7183e-04,  3.1627e-02,  3.9380e-02,  4.6293e-03, -2.3431e-02,\n",
       "                      -6.4072e-04,  1.7192e-02,  3.4350e-03,  1.8059e-02, -3.3915e-03,\n",
       "                       4.4415e-02, -1.1787e-02,  7.5837e-03,  2.0625e-02,  6.6643e-03,\n",
       "                       3.9681e-02, -6.9100e-03, -9.8066e-04,  1.2254e-02, -3.1923e-03,\n",
       "                      -2.6782e-02,  1.0342e-02,  7.6245e-03,  6.6009e-03,  4.7665e-03,\n",
       "                       3.3806e-03, -2.5340e-02,  2.9330e-03,  1.2422e-02, -2.2778e-03,\n",
       "                       1.9051e-02,  6.6136e-03, -8.4463e-03, -1.1495e-03,  2.9921e-02,\n",
       "                      -1.9857e-03,  2.7231e-02,  8.8962e-03,  2.2928e-03, -2.2536e-02])),\n",
       "             ('output_layer.weight',\n",
       "              tensor([[ 0.2939, -0.2228,  0.1210,  0.3289, -0.0352, -0.3343, -0.0591, -0.0817,\n",
       "                       -0.0450,  0.0258,  0.3629,  0.5068, -0.3440, -0.2298,  0.2521, -0.0714,\n",
       "                        0.7527,  0.4642,  0.0256, -0.0278, -0.2914, -0.1921,  0.0829,  0.1001,\n",
       "                       -0.0691,  0.3419, -0.0753, -0.1839,  0.4809, -0.3885, -0.3020, -0.0042,\n",
       "                       -0.2849,  0.0247,  0.3105,  0.4310, -0.1531,  0.3556, -0.2000,  0.3837,\n",
       "                       -0.1618,  0.0294, -0.2507,  0.3162,  0.0053, -0.3267,  0.3307, -0.3480,\n",
       "                       -0.2822, -0.2681, -0.0499,  0.0407, -0.0901, -0.0773,  0.1094, -0.3063,\n",
       "                        0.0742, -0.1212, -0.2877, -0.4275,  0.0700, -0.2026,  0.1905, -0.2103,\n",
       "                        0.3283, -0.1777, -0.0464,  0.1880,  0.4542, -0.5435,  0.1563,  0.1364,\n",
       "                        0.1653, -0.4026,  0.5448,  0.4998,  0.2283, -0.1559, -0.0145, -0.0631,\n",
       "                        0.0337,  0.0149,  0.0199,  0.2608,  0.1174,  0.1995,  0.0847,  0.4983,\n",
       "                        0.3143,  0.2701,  0.5119,  0.1649, -0.1268, -0.4148,  0.1884,  0.2228,\n",
       "                        0.4472,  0.3174, -0.5371,  0.0211,  0.3261,  0.4786,  0.2428,  0.1081,\n",
       "                        0.2421,  0.1920, -0.3775,  0.0521, -0.2263,  0.1821, -0.6240, -0.0618,\n",
       "                        0.0632,  0.3606, -0.5029, -0.0322, -0.2979, -0.2898,  0.2032, -0.4157,\n",
       "                       -0.0429, -0.3453, -0.2226,  0.2941,  0.2021,  0.3772, -0.1451,  0.2579,\n",
       "                       -0.3040,  0.1716,  0.1046, -0.1262, -0.0912,  0.0050, -0.1340, -0.2328,\n",
       "                        0.1758,  0.1554,  0.3311, -0.5538, -0.2668, -0.2962, -0.3939, -0.0148,\n",
       "                        0.3382, -0.4883,  0.2072,  0.0384, -0.1999,  0.3151, -0.2003,  0.4266,\n",
       "                       -0.0458, -0.1237, -0.1017, -0.4180, -0.2675,  0.0227,  0.1256,  0.0904,\n",
       "                        0.1983, -0.3269, -0.2565,  0.1813,  0.5241,  0.1061, -0.0870, -0.1346,\n",
       "                       -0.3246,  0.0340, -0.3858,  0.3074, -0.5361, -0.2167, -0.0034,  0.3393,\n",
       "                        0.2033,  0.0634,  0.3456, -0.0620, -0.2206, -0.2194,  0.2761, -0.2084,\n",
       "                        0.2864,  0.0725, -0.5597, -0.0915, -0.0608,  0.6874,  0.2615, -0.1175,\n",
       "                       -0.3494, -0.2407,  0.3185, -0.0948, -0.2349,  0.0807, -0.0987,  0.3589]])),\n",
       "             ('output_layer.bias', tensor([-0.0267]))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfbceb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze first three layers\n",
    "for name, param in my_network.named_parameters():\n",
    "    if  'hidden_layers.0' in name or 'hidden_layers.1' in name or 'hidden_layers.2' in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Unfreeze the final layer\n",
    "for name, param in my_network.named_parameters():\n",
    "    if 'hidden_layers.3' in name or 'output_layer'in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38db0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer for the final layer only\n",
    "parameters_to_optimize = filter(lambda p: p.requires_grad, my_network.parameters())\n",
    "if opt_type == \"ADAM\":\n",
    "    optimizer_ = optim.Adam(parameters_to_optimize, lr=0.001)  # Optimizer for the last layer\n",
    "elif opt_type == \"LBFGS\":\n",
    "    optimizer_ = optim.LBFGS(parameters_to_optimize, lr=0.1, max_iter=1, max_eval=50000, tolerance_change=1.0 * np.finfo(float).eps)\n",
    "else:\n",
    "    raise ValueError(\"Optimizer not recognized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adfeda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model definition\n",
    "# my_network = NeuralNet(input_dimension = init.shape[1], output_dimension = u_init.shape[1], n_hidden_layers=4, neurons=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a2fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if opt_type == \"ADAM\":\n",
    "#     optimizer_ = optim.Adam(my_network.parameters(), lr=0.001)\n",
    "# elif opt_type == \"LBFGS\":\n",
    "#     optimizer_ = optim.LBFGS(my_network.parameters(), lr=0.1, max_iter=1, max_eval=50000, tolerance_change=1.0 * np.finfo(float).eps)\n",
    "# else:\n",
    "#     raise ValueError(\"Optimizer not recognized\")\n",
    "\n",
    "\n",
    "def fit(model, training_set, interior, num_epochs, optimizer, p, verbose=True):\n",
    "    history = list()\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "\n",
    "        running_loss = list([0])\n",
    "\n",
    "        # Loop over batches\n",
    "        for j, (initial, u_initial, u_initial_t, bd_left, bd_right, ubl, ubr) in enumerate(training_set):\n",
    "            def closure():\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # for initial\n",
    "                initial.requires_grad = True\n",
    "                u_initial_pred_ = model(initial)\n",
    "                inputs = torch.ones(initial_pts, 1).to(device)\n",
    "                grad_u_init = torch.autograd.grad(u_initial_pred_, initial, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_init_t = grad_u_init[:, 1].reshape(-1, )\n",
    "\n",
    "                # for left boundary\n",
    "                bd_left.requires_grad = True\n",
    "                bd_left_pred_ = model(bd_left)\n",
    "                inputs = torch.ones(left_boundary_pts, 1).to(device)\n",
    "                grad_bd_left = torch.autograd.grad(bd_left_pred_, bd_left, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_x_left = grad_bd_left[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(left_boundary_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_bd_x_left = torch.autograd.grad(u_bd_x_left, bd_left, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_xx_left = grad_u_bd_x_left[:, 0].reshape(-1, )\n",
    "                #inputs = torch.ones(left_boundary_pts, 1).reshape(-1, )\n",
    "                #grad_u_bd_xx_left = torch.autograd.grad(u_bd_xx_left, bd_left, grad_outputs=inputs, create_graph=True)[0]\n",
    "                #u_bd_xxx_left = grad_u_bd_xx_left[:, 0].reshape(-1, )\n",
    "\n",
    "                # for right boundary\n",
    "                bd_right.requires_grad = True\n",
    "                bd_right_pred_ = model(bd_right)\n",
    "                inputs = torch.ones(right_boundary_pts, 1).to(device)\n",
    "                grad_bd_right = torch.autograd.grad(bd_right_pred_, bd_right, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_x_right = grad_bd_right[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(right_boundary_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_bd_x_right = torch.autograd.grad(u_bd_x_right, bd_right, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_xx_right = grad_u_bd_x_right[:, 0].reshape(-1, )\n",
    "\n",
    "                # residual calculation\n",
    "                interior.requires_grad = True\n",
    "                u_hat = model(interior)\n",
    "                inputs = torch.ones(residual_pts, 1).to(device)\n",
    "                grad_u_hat = torch.autograd.grad(u_hat, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "\n",
    "                u_x = grad_u_hat[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_x = torch.autograd.grad(u_x, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_xx = grad_u_x[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_xx = torch.autograd.grad(u_xx, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_xxx = grad_u_xx[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_xxx = torch.autograd.grad(u_xxx, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_xxxx = grad_u_xxx[:, 0].reshape(-1, )\n",
    "\n",
    "                u_t = grad_u_hat[:, 1]\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_t = torch.autograd.grad(u_t, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_tt = grad_u_t[:, 1].reshape(-1, )\n",
    "\n",
    "                pde_single_column = (u_tt.reshape(-1, ) + u_xxxx.reshape(-1, ) + \\\n",
    "                                     u_hat.reshape(-1, ) - 3*torch.sin(interior[:,0])*torch.exp(interior[:,1])) ** 2\n",
    "                pde_single_column = pde_single_column.reshape(-1, 1)\n",
    "\n",
    "                pde_matrix = pde_single_column.reshape(100, 100)\n",
    "\n",
    "                loss_at_time_steps = torch.mean(pde_matrix, 1)\n",
    "                loss_at_time_steps = loss_at_time_steps.reshape(-1, 1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    weighted_loss = torch.matmul(W, loss_at_time_steps)\n",
    "                weighted_loss = torch.exp(-eps * weighted_loss)\n",
    "\n",
    "                loss_pde = torch.mean(weighted_loss * loss_at_time_steps)\n",
    "\n",
    "                # Item 1. below\n",
    "\n",
    "                loss_ic = torch.mean((u_initial_pred_.reshape(-1, ) - u_initial.reshape(-1, )) ** p) + \\\n",
    "                          torch.mean((u_init_t.reshape(-1, ) - u_initial_t.reshape(-1, )) ** p)\n",
    "                #loss_pde = torch.mean((u_tt.reshape(-1, ) + u_xxxx.reshape(-1, )) ** p)\n",
    "                loss_left_b = torch.mean((bd_left_pred_.reshape(-1, )) ** p) + \\\n",
    "                              torch.mean((u_bd_xx_left.reshape(-1, )) ** p)\n",
    "                loss_right_b = torch.mean((bd_right_pred_.reshape(-1, )) ** p) + \\\n",
    "                               torch.mean((u_bd_xx_right.reshape(-1, )) ** p)\n",
    "\n",
    "                loss = loss_ic + loss_pde + loss_left_b + loss_right_b\n",
    "\n",
    "                # Item 2. below\n",
    "                loss.backward()\n",
    "                # Compute average training loss over batches for the current epoch\n",
    "                running_loss[0] += loss.item()\n",
    "                return loss\n",
    "\n",
    "            # Item 3. below\n",
    "            optimizer.step(closure=closure)\n",
    "\n",
    "        print('Loss: ', (running_loss[0] / len(training_set)))\n",
    "        history.append(running_loss[0])\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27e0cca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################  0  ################################\n",
      "Loss:  1.0265662670135498\n",
      "################################  1  ################################\n",
      "Loss:  1.0257219076156616\n",
      "################################  2  ################################\n",
      "Loss:  1.023619532585144\n",
      "################################  3  ################################\n",
      "Loss:  1.0216190814971924\n",
      "################################  4  ################################\n",
      "Loss:  1.0197855234146118\n",
      "################################  5  ################################\n",
      "Loss:  1.0181527137756348\n",
      "################################  6  ################################\n",
      "Loss:  1.016730546951294\n",
      "################################  7  ################################\n",
      "Loss:  1.015511155128479\n",
      "################################  8  ################################\n",
      "Loss:  1.0144758224487305\n",
      "################################  9  ################################\n",
      "Loss:  1.0135997533798218\n",
      "################################  10  ################################\n",
      "Loss:  1.0128569602966309\n",
      "################################  11  ################################\n",
      "Loss:  1.0122236013412476\n",
      "################################  12  ################################\n",
      "Loss:  1.0116785764694214\n",
      "################################  13  ################################\n",
      "Loss:  1.0112043619155884\n",
      "################################  14  ################################\n",
      "Loss:  1.0107860565185547\n",
      "################################  15  ################################\n",
      "Loss:  1.0104122161865234\n",
      "################################  16  ################################\n",
      "Loss:  1.010072946548462\n",
      "################################  17  ################################\n",
      "Loss:  1.009759783744812\n",
      "################################  18  ################################\n",
      "Loss:  1.0094659328460693\n",
      "################################  19  ################################\n",
      "Loss:  1.0091854333877563\n",
      "################################  20  ################################\n",
      "Loss:  1.0089125633239746\n",
      "################################  21  ################################\n",
      "Loss:  1.0086424350738525\n",
      "################################  22  ################################\n",
      "Loss:  1.0083703994750977\n",
      "################################  23  ################################\n",
      "Loss:  1.0080915689468384\n",
      "################################  24  ################################\n",
      "Loss:  1.007800817489624\n",
      "################################  25  ################################\n",
      "Loss:  1.0074928998947144\n",
      "################################  26  ################################\n",
      "Loss:  1.0071609020233154\n",
      "################################  27  ################################\n",
      "Loss:  1.006796956062317\n",
      "################################  28  ################################\n",
      "Loss:  1.0063906908035278\n",
      "################################  29  ################################\n",
      "Loss:  1.0059261322021484\n",
      "################################  30  ################################\n",
      "Loss:  1.0053789615631104\n",
      "################################  31  ################################\n",
      "Loss:  1.0047056674957275\n",
      "################################  32  ################################\n",
      "Loss:  1.00381600856781\n",
      "################################  33  ################################\n",
      "Loss:  1.0024683475494385\n",
      "################################  34  ################################\n",
      "Loss:  0.9998870491981506\n",
      "################################  35  ################################\n",
      "Loss:  0.9978933930397034\n",
      "################################  36  ################################\n",
      "Loss:  0.9974622130393982\n",
      "################################  37  ################################\n",
      "Loss:  0.9971763491630554\n",
      "################################  38  ################################\n",
      "Loss:  0.9968575239181519\n",
      "################################  39  ################################\n",
      "Loss:  0.9964487552642822\n",
      "################################  40  ################################\n",
      "Loss:  0.9959532022476196\n",
      "################################  41  ################################\n",
      "Loss:  0.9953833818435669\n",
      "################################  42  ################################\n",
      "Loss:  0.994768500328064\n",
      "################################  43  ################################\n",
      "Loss:  0.9941191077232361\n",
      "################################  44  ################################\n",
      "Loss:  0.9933990240097046\n",
      "################################  45  ################################\n",
      "Loss:  0.9925234913825989\n",
      "################################  46  ################################\n",
      "Loss:  0.9913524985313416\n",
      "################################  47  ################################\n",
      "Loss:  0.9895945191383362\n",
      "################################  48  ################################\n",
      "Loss:  0.9864033460617065\n",
      "################################  49  ################################\n",
      "Loss:  0.980119526386261\n",
      "################################  50  ################################\n",
      "Loss:  0.9755941033363342\n",
      "################################  51  ################################\n",
      "Loss:  0.9716536998748779\n",
      "################################  52  ################################\n",
      "Loss:  0.9683094024658203\n",
      "################################  53  ################################\n",
      "Loss:  0.9652859568595886\n",
      "################################  54  ################################\n",
      "Loss:  0.9632392525672913\n",
      "################################  55  ################################\n",
      "Loss:  0.9611274003982544\n",
      "################################  56  ################################\n",
      "Loss:  0.958685576915741\n",
      "################################  57  ################################\n",
      "Loss:  0.9562390446662903\n",
      "################################  58  ################################\n",
      "Loss:  0.953842043876648\n",
      "################################  59  ################################\n",
      "Loss:  0.9515724182128906\n",
      "################################  60  ################################\n",
      "Loss:  0.9493739008903503\n",
      "################################  61  ################################\n",
      "Loss:  0.9472484588623047\n",
      "################################  62  ################################\n",
      "Loss:  0.9451446533203125\n",
      "################################  63  ################################\n",
      "Loss:  0.9430497288703918\n",
      "################################  64  ################################\n",
      "Loss:  0.9409260749816895\n",
      "################################  65  ################################\n",
      "Loss:  0.9388479590415955\n",
      "################################  66  ################################\n",
      "Loss:  0.9369242787361145\n",
      "################################  67  ################################\n",
      "Loss:  0.9351649284362793\n",
      "################################  68  ################################\n",
      "Loss:  0.9333646893501282\n",
      "################################  69  ################################\n",
      "Loss:  0.9314571619033813\n",
      "################################  70  ################################\n",
      "Loss:  0.9293347001075745\n",
      "################################  71  ################################\n",
      "Loss:  0.927061915397644\n",
      "################################  72  ################################\n",
      "Loss:  0.9246332049369812\n",
      "################################  73  ################################\n",
      "Loss:  0.9221470952033997\n",
      "################################  74  ################################\n",
      "Loss:  0.9195865988731384\n",
      "################################  75  ################################\n",
      "Loss:  0.9170445799827576\n",
      "################################  76  ################################\n",
      "Loss:  0.9145152568817139\n",
      "################################  77  ################################\n",
      "Loss:  0.9120540618896484\n",
      "################################  78  ################################\n",
      "Loss:  0.9096871614456177\n",
      "################################  79  ################################\n",
      "Loss:  0.9074142575263977\n",
      "################################  80  ################################\n",
      "Loss:  0.9053156971931458\n",
      "################################  81  ################################\n",
      "Loss:  0.9032885432243347\n",
      "################################  82  ################################\n",
      "Loss:  0.9014267921447754\n",
      "################################  83  ################################\n",
      "Loss:  0.8997145891189575\n",
      "################################  84  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.8980964422225952\n",
      "################################  85  ################################\n",
      "Loss:  0.8965781331062317\n",
      "################################  86  ################################\n",
      "Loss:  0.8951871395111084\n",
      "################################  87  ################################\n",
      "Loss:  0.8939154744148254\n",
      "################################  88  ################################\n",
      "Loss:  0.8927591443061829\n",
      "################################  89  ################################\n",
      "Loss:  0.891670823097229\n",
      "################################  90  ################################\n",
      "Loss:  0.8906323313713074\n",
      "################################  91  ################################\n",
      "Loss:  0.8895996809005737\n",
      "################################  92  ################################\n",
      "Loss:  0.888567328453064\n",
      "################################  93  ################################\n",
      "Loss:  0.8874920010566711\n",
      "################################  94  ################################\n",
      "Loss:  0.8864089846611023\n",
      "################################  95  ################################\n",
      "Loss:  0.8851924538612366\n",
      "################################  96  ################################\n",
      "Loss:  0.8840010762214661\n",
      "################################  97  ################################\n",
      "Loss:  0.8825990557670593\n",
      "################################  98  ################################\n",
      "Loss:  0.8813183903694153\n",
      "################################  99  ################################\n",
      "Loss:  0.880011796951294\n",
      "################################  100  ################################\n",
      "Loss:  0.8789570927619934\n",
      "################################  101  ################################\n",
      "Loss:  0.8778637051582336\n",
      "################################  102  ################################\n",
      "Loss:  0.8768717050552368\n",
      "################################  103  ################################\n",
      "Loss:  0.8758333921432495\n",
      "################################  104  ################################\n",
      "Loss:  0.8748131394386292\n",
      "################################  105  ################################\n",
      "Loss:  0.8737155795097351\n",
      "################################  106  ################################\n",
      "Loss:  0.8725677132606506\n",
      "################################  107  ################################\n",
      "Loss:  0.8713585734367371\n",
      "################################  108  ################################\n",
      "Loss:  0.8701305389404297\n",
      "################################  109  ################################\n",
      "Loss:  0.8689056634902954\n",
      "################################  110  ################################\n",
      "Loss:  0.8677279353141785\n",
      "################################  111  ################################\n",
      "Loss:  0.8665996789932251\n",
      "################################  112  ################################\n",
      "Loss:  0.8655442595481873\n",
      "################################  113  ################################\n",
      "Loss:  0.8645347356796265\n",
      "################################  114  ################################\n",
      "Loss:  0.8635797500610352\n",
      "################################  115  ################################\n",
      "Loss:  0.8626409769058228\n",
      "################################  116  ################################\n",
      "Loss:  0.8617193102836609\n",
      "################################  117  ################################\n",
      "Loss:  0.8607797026634216\n",
      "################################  118  ################################\n",
      "Loss:  0.8598122000694275\n",
      "################################  119  ################################\n",
      "Loss:  0.8587812185287476\n",
      "################################  120  ################################\n",
      "Loss:  0.8576034307479858\n",
      "################################  121  ################################\n",
      "Loss:  0.856239914894104\n",
      "################################  122  ################################\n",
      "Loss:  0.8548054099082947\n",
      "################################  123  ################################\n",
      "Loss:  0.853118360042572\n",
      "################################  124  ################################\n",
      "Loss:  0.8514732122421265\n",
      "################################  125  ################################\n",
      "Loss:  0.8499665856361389\n",
      "################################  126  ################################\n",
      "Loss:  0.8483226299285889\n",
      "################################  127  ################################\n",
      "Loss:  0.8467625975608826\n",
      "################################  128  ################################\n",
      "Loss:  0.8452281951904297\n",
      "################################  129  ################################\n",
      "Loss:  0.8437908291816711\n",
      "################################  130  ################################\n",
      "Loss:  0.8425576686859131\n",
      "################################  131  ################################\n",
      "Loss:  0.8415018916130066\n",
      "################################  132  ################################\n",
      "Loss:  0.8406095504760742\n",
      "################################  133  ################################\n",
      "Loss:  0.8397492170333862\n",
      "################################  134  ################################\n",
      "Loss:  0.838924765586853\n",
      "################################  135  ################################\n",
      "Loss:  0.8380950093269348\n",
      "################################  136  ################################\n",
      "Loss:  0.837274432182312\n",
      "################################  137  ################################\n",
      "Loss:  0.8364457488059998\n",
      "################################  138  ################################\n",
      "Loss:  0.8356287479400635\n",
      "################################  139  ################################\n",
      "Loss:  0.8348011374473572\n",
      "################################  140  ################################\n",
      "Loss:  0.8339917063713074\n",
      "################################  141  ################################\n",
      "Loss:  0.8331699967384338\n",
      "################################  142  ################################\n",
      "Loss:  0.8323730230331421\n",
      "################################  143  ################################\n",
      "Loss:  0.8315647840499878\n",
      "################################  144  ################################\n",
      "Loss:  0.8307873010635376\n",
      "################################  145  ################################\n",
      "Loss:  0.8300038576126099\n",
      "################################  146  ################################\n",
      "Loss:  0.8292543888092041\n",
      "################################  147  ################################\n",
      "Loss:  0.8285022377967834\n",
      "################################  148  ################################\n",
      "Loss:  0.8277671933174133\n",
      "################################  149  ################################\n",
      "Loss:  0.8270096778869629\n",
      "################################  150  ################################\n",
      "Loss:  0.8262127041816711\n",
      "################################  151  ################################\n",
      "Loss:  0.8254120945930481\n",
      "################################  152  ################################\n",
      "Loss:  0.8247091174125671\n",
      "################################  153  ################################\n",
      "Loss:  0.8240696787834167\n",
      "################################  154  ################################\n",
      "Loss:  0.8234419822692871\n",
      "################################  155  ################################\n",
      "Loss:  0.8228370547294617\n",
      "################################  156  ################################\n",
      "Loss:  0.8222435116767883\n",
      "################################  157  ################################\n",
      "Loss:  0.821664035320282\n",
      "################################  158  ################################\n",
      "Loss:  0.8210845589637756\n",
      "################################  159  ################################\n",
      "Loss:  0.8204963207244873\n",
      "################################  160  ################################\n",
      "Loss:  0.8198822736740112\n",
      "################################  161  ################################\n",
      "Loss:  0.819221556186676\n",
      "################################  162  ################################\n",
      "Loss:  0.8184974789619446\n",
      "################################  163  ################################\n",
      "Loss:  0.8177024722099304\n",
      "################################  164  ################################\n",
      "Loss:  0.8168818950653076\n",
      "################################  165  ################################\n",
      "Loss:  0.8160728216171265\n",
      "################################  166  ################################\n",
      "Loss:  0.8152469396591187\n",
      "################################  167  ################################\n",
      "Loss:  0.8143805265426636\n",
      "################################  168  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.8134546875953674\n",
      "################################  169  ################################\n",
      "Loss:  0.8124920129776001\n",
      "################################  170  ################################\n",
      "Loss:  0.8115023970603943\n",
      "################################  171  ################################\n",
      "Loss:  0.8104898929595947\n",
      "################################  172  ################################\n",
      "Loss:  0.8094250559806824\n",
      "################################  173  ################################\n",
      "Loss:  0.8082942366600037\n",
      "################################  174  ################################\n",
      "Loss:  0.8070796132087708\n",
      "################################  175  ################################\n",
      "Loss:  0.8058375716209412\n",
      "################################  176  ################################\n",
      "Loss:  0.8045157194137573\n",
      "################################  177  ################################\n",
      "Loss:  0.8031500577926636\n",
      "################################  178  ################################\n",
      "Loss:  0.8017025589942932\n",
      "################################  179  ################################\n",
      "Loss:  0.8002874851226807\n",
      "################################  180  ################################\n",
      "Loss:  0.7990366816520691\n",
      "################################  181  ################################\n",
      "Loss:  0.7978306412696838\n",
      "################################  182  ################################\n",
      "Loss:  0.7968079447746277\n",
      "################################  183  ################################\n",
      "Loss:  0.7959964871406555\n",
      "################################  184  ################################\n",
      "Loss:  0.7952864170074463\n",
      "################################  185  ################################\n",
      "Loss:  0.7945419549942017\n",
      "################################  186  ################################\n",
      "Loss:  0.7937643527984619\n",
      "################################  187  ################################\n",
      "Loss:  0.7930456399917603\n",
      "################################  188  ################################\n",
      "Loss:  0.7923636436462402\n",
      "################################  189  ################################\n",
      "Loss:  0.7917007803916931\n",
      "################################  190  ################################\n",
      "Loss:  0.7909529209136963\n",
      "################################  191  ################################\n",
      "Loss:  0.7900989651679993\n",
      "################################  192  ################################\n",
      "Loss:  0.7891408801078796\n",
      "################################  193  ################################\n",
      "Loss:  0.788142204284668\n",
      "################################  194  ################################\n",
      "Loss:  0.7871055006980896\n",
      "################################  195  ################################\n",
      "Loss:  0.7860261797904968\n",
      "################################  196  ################################\n",
      "Loss:  0.7848876118659973\n",
      "################################  197  ################################\n",
      "Loss:  0.7836847901344299\n",
      "################################  198  ################################\n",
      "Loss:  0.7825543880462646\n",
      "################################  199  ################################\n",
      "Loss:  0.7815396189689636\n",
      "################################  200  ################################\n",
      "Loss:  0.7805542349815369\n",
      "################################  201  ################################\n",
      "Loss:  0.7796410918235779\n",
      "################################  202  ################################\n",
      "Loss:  0.7787257432937622\n",
      "################################  203  ################################\n",
      "Loss:  0.7779282927513123\n",
      "################################  204  ################################\n",
      "Loss:  0.7772409915924072\n",
      "################################  205  ################################\n",
      "Loss:  0.776565432548523\n",
      "################################  206  ################################\n",
      "Loss:  0.7759205102920532\n",
      "################################  207  ################################\n",
      "Loss:  0.7753491401672363\n",
      "################################  208  ################################\n",
      "Loss:  0.7748299837112427\n",
      "################################  209  ################################\n",
      "Loss:  0.7743656039237976\n",
      "################################  210  ################################\n",
      "Loss:  0.7739002108573914\n",
      "################################  211  ################################\n",
      "Loss:  0.7734585404396057\n",
      "################################  212  ################################\n",
      "Loss:  0.7730270624160767\n",
      "################################  213  ################################\n",
      "Loss:  0.7726268768310547\n",
      "################################  214  ################################\n",
      "Loss:  0.7722251415252686\n",
      "################################  215  ################################\n",
      "Loss:  0.7718302607536316\n",
      "################################  216  ################################\n",
      "Loss:  0.7714220285415649\n",
      "################################  217  ################################\n",
      "Loss:  0.7710105180740356\n",
      "################################  218  ################################\n",
      "Loss:  0.7706258296966553\n",
      "################################  219  ################################\n",
      "Loss:  0.7702805995941162\n",
      "################################  220  ################################\n",
      "Loss:  0.7699754238128662\n",
      "################################  221  ################################\n",
      "Loss:  0.7697000503540039\n",
      "################################  222  ################################\n",
      "Loss:  0.7694432735443115\n",
      "################################  223  ################################\n",
      "Loss:  0.7691909670829773\n",
      "################################  224  ################################\n",
      "Loss:  0.76894211769104\n",
      "################################  225  ################################\n",
      "Loss:  0.7686619758605957\n",
      "################################  226  ################################\n",
      "Loss:  0.7684251666069031\n",
      "################################  227  ################################\n",
      "Loss:  0.768234133720398\n",
      "################################  228  ################################\n",
      "Loss:  0.7680109143257141\n",
      "################################  229  ################################\n",
      "Loss:  0.7677698731422424\n",
      "################################  230  ################################\n",
      "Loss:  0.767497718334198\n",
      "################################  231  ################################\n",
      "Loss:  0.767164409160614\n",
      "################################  232  ################################\n",
      "Loss:  0.7667922973632812\n",
      "################################  233  ################################\n",
      "Loss:  0.7663975954055786\n",
      "################################  234  ################################\n",
      "Loss:  0.7659856677055359\n",
      "################################  235  ################################\n",
      "Loss:  0.7655609250068665\n",
      "################################  236  ################################\n",
      "Loss:  0.7651151418685913\n",
      "################################  237  ################################\n",
      "Loss:  0.7646743655204773\n",
      "################################  238  ################################\n",
      "Loss:  0.7642692923545837\n",
      "################################  239  ################################\n",
      "Loss:  0.7638770341873169\n",
      "################################  240  ################################\n",
      "Loss:  0.7634971141815186\n",
      "################################  241  ################################\n",
      "Loss:  0.7631056904792786\n",
      "################################  242  ################################\n",
      "Loss:  0.7627213001251221\n",
      "################################  243  ################################\n",
      "Loss:  0.7623472809791565\n",
      "################################  244  ################################\n",
      "Loss:  0.7619808316230774\n",
      "################################  245  ################################\n",
      "Loss:  0.7616106271743774\n",
      "################################  246  ################################\n",
      "Loss:  0.7612210512161255\n",
      "################################  247  ################################\n",
      "Loss:  0.7608620524406433\n",
      "################################  248  ################################\n",
      "Loss:  0.760523796081543\n",
      "################################  249  ################################\n",
      "Loss:  0.7602013349533081\n",
      "################################  250  ################################\n",
      "Loss:  0.759886622428894\n",
      "################################  251  ################################\n",
      "Loss:  0.759579062461853\n",
      "################################  252  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.7592853903770447\n",
      "################################  253  ################################\n",
      "Loss:  0.7590152621269226\n",
      "################################  254  ################################\n",
      "Loss:  0.7587578892707825\n",
      "################################  255  ################################\n",
      "Loss:  0.758503258228302\n",
      "################################  256  ################################\n",
      "Loss:  0.7582332491874695\n",
      "################################  257  ################################\n",
      "Loss:  0.7579532265663147\n",
      "################################  258  ################################\n",
      "Loss:  0.7576628923416138\n",
      "################################  259  ################################\n",
      "Loss:  0.7573668360710144\n",
      "################################  260  ################################\n",
      "Loss:  0.7570655345916748\n",
      "################################  261  ################################\n",
      "Loss:  0.7567574977874756\n",
      "################################  262  ################################\n",
      "Loss:  0.7564474940299988\n",
      "################################  263  ################################\n",
      "Loss:  0.756129264831543\n",
      "################################  264  ################################\n",
      "Loss:  0.7558131814002991\n",
      "################################  265  ################################\n",
      "Loss:  0.7554939985275269\n",
      "################################  266  ################################\n",
      "Loss:  0.7551818490028381\n",
      "################################  267  ################################\n",
      "Loss:  0.7548730969429016\n",
      "################################  268  ################################\n",
      "Loss:  0.7545735239982605\n",
      "################################  269  ################################\n",
      "Loss:  0.7542732954025269\n",
      "################################  270  ################################\n",
      "Loss:  0.7539721131324768\n",
      "################################  271  ################################\n",
      "Loss:  0.7536571621894836\n",
      "################################  272  ################################\n",
      "Loss:  0.7533330917358398\n",
      "################################  273  ################################\n",
      "Loss:  0.7529917359352112\n",
      "################################  274  ################################\n",
      "Loss:  0.7526401877403259\n",
      "################################  275  ################################\n",
      "Loss:  0.7522782683372498\n",
      "################################  276  ################################\n",
      "Loss:  0.7519276142120361\n",
      "################################  277  ################################\n",
      "Loss:  0.7516114115715027\n",
      "################################  278  ################################\n",
      "Loss:  0.7513136863708496\n",
      "################################  279  ################################\n",
      "Loss:  0.7509904503822327\n",
      "################################  280  ################################\n",
      "Loss:  0.7506520748138428\n",
      "################################  281  ################################\n",
      "Loss:  0.7502735257148743\n",
      "################################  282  ################################\n",
      "Loss:  0.7498335242271423\n",
      "################################  283  ################################\n",
      "Loss:  0.7493811249732971\n",
      "################################  284  ################################\n",
      "Loss:  0.7489045858383179\n",
      "################################  285  ################################\n",
      "Loss:  0.7483051419258118\n",
      "################################  286  ################################\n",
      "Loss:  0.7476760745048523\n",
      "################################  287  ################################\n",
      "Loss:  0.7470837831497192\n",
      "################################  288  ################################\n",
      "Loss:  0.746549665927887\n",
      "################################  289  ################################\n",
      "Loss:  0.7460458278656006\n",
      "################################  290  ################################\n",
      "Loss:  0.7455180883407593\n",
      "################################  291  ################################\n",
      "Loss:  0.7449133396148682\n",
      "################################  292  ################################\n",
      "Loss:  0.7442342042922974\n",
      "################################  293  ################################\n",
      "Loss:  0.7436333298683167\n",
      "################################  294  ################################\n",
      "Loss:  0.7430053353309631\n",
      "################################  295  ################################\n",
      "Loss:  0.7422924041748047\n",
      "################################  296  ################################\n",
      "Loss:  0.7415719628334045\n",
      "################################  297  ################################\n",
      "Loss:  0.7408767938613892\n",
      "################################  298  ################################\n",
      "Loss:  0.7402962446212769\n",
      "################################  299  ################################\n",
      "Loss:  0.739730954170227\n",
      "################################  300  ################################\n",
      "Loss:  0.7391788363456726\n",
      "################################  301  ################################\n",
      "Loss:  0.7385843992233276\n",
      "################################  302  ################################\n",
      "Loss:  0.7379600405693054\n",
      "################################  303  ################################\n",
      "Loss:  0.7373537421226501\n",
      "################################  304  ################################\n",
      "Loss:  0.7367679476737976\n",
      "################################  305  ################################\n",
      "Loss:  0.7362114787101746\n",
      "################################  306  ################################\n",
      "Loss:  0.7356336712837219\n",
      "################################  307  ################################\n",
      "Loss:  0.7350620627403259\n",
      "################################  308  ################################\n",
      "Loss:  0.7345081567764282\n",
      "################################  309  ################################\n",
      "Loss:  0.7339596152305603\n",
      "################################  310  ################################\n",
      "Loss:  0.7334089875221252\n",
      "################################  311  ################################\n",
      "Loss:  0.73285311460495\n",
      "################################  312  ################################\n",
      "Loss:  0.7322686314582825\n",
      "################################  313  ################################\n",
      "Loss:  0.7316197156906128\n",
      "################################  314  ################################\n",
      "Loss:  0.7308679223060608\n",
      "################################  315  ################################\n",
      "Loss:  0.730119526386261\n",
      "################################  316  ################################\n",
      "Loss:  0.729351282119751\n",
      "################################  317  ################################\n",
      "Loss:  0.7285361886024475\n",
      "################################  318  ################################\n",
      "Loss:  0.7277693152427673\n",
      "################################  319  ################################\n",
      "Loss:  0.7269815802574158\n",
      "################################  320  ################################\n",
      "Loss:  0.7262746095657349\n",
      "################################  321  ################################\n",
      "Loss:  0.7255638837814331\n",
      "################################  322  ################################\n",
      "Loss:  0.7249462604522705\n",
      "################################  323  ################################\n",
      "Loss:  0.7243375182151794\n",
      "################################  324  ################################\n",
      "Loss:  0.7238211035728455\n",
      "################################  325  ################################\n",
      "Loss:  0.7233285307884216\n",
      "################################  326  ################################\n",
      "Loss:  0.7229183912277222\n",
      "################################  327  ################################\n",
      "Loss:  0.7225266695022583\n",
      "################################  328  ################################\n",
      "Loss:  0.7221822738647461\n",
      "################################  329  ################################\n",
      "Loss:  0.7217999696731567\n",
      "################################  330  ################################\n",
      "Loss:  0.7213617563247681\n",
      "################################  331  ################################\n",
      "Loss:  0.7210114598274231\n",
      "################################  332  ################################\n",
      "Loss:  0.720636248588562\n",
      "################################  333  ################################\n",
      "Loss:  0.7202637195587158\n",
      "################################  334  ################################\n",
      "Loss:  0.7198554873466492\n",
      "################################  335  ################################\n",
      "Loss:  0.719447135925293\n",
      "################################  336  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.7189809083938599\n",
      "################################  337  ################################\n",
      "Loss:  0.718488872051239\n",
      "################################  338  ################################\n",
      "Loss:  0.7179228067398071\n",
      "################################  339  ################################\n",
      "Loss:  0.7173721194267273\n",
      "################################  340  ################################\n",
      "Loss:  0.7169073224067688\n",
      "################################  341  ################################\n",
      "Loss:  0.7165417671203613\n",
      "################################  342  ################################\n",
      "Loss:  0.7162637710571289\n",
      "################################  343  ################################\n",
      "Loss:  0.7159939408302307\n",
      "################################  344  ################################\n",
      "Loss:  0.7157623767852783\n",
      "################################  345  ################################\n",
      "Loss:  0.7154950499534607\n",
      "################################  346  ################################\n",
      "Loss:  0.7152418494224548\n",
      "################################  347  ################################\n",
      "Loss:  0.71497642993927\n",
      "################################  348  ################################\n",
      "Loss:  0.7147163152694702\n",
      "################################  349  ################################\n",
      "Loss:  0.714421272277832\n",
      "################################  350  ################################\n",
      "Loss:  0.7141132354736328\n",
      "################################  351  ################################\n",
      "Loss:  0.7137920260429382\n",
      "################################  352  ################################\n",
      "Loss:  0.7135065197944641\n",
      "################################  353  ################################\n",
      "Loss:  0.7132138013839722\n",
      "################################  354  ################################\n",
      "Loss:  0.7128728628158569\n",
      "################################  355  ################################\n",
      "Loss:  0.7124685645103455\n",
      "################################  356  ################################\n",
      "Loss:  0.7120496034622192\n",
      "################################  357  ################################\n",
      "Loss:  0.7116298675537109\n",
      "################################  358  ################################\n",
      "Loss:  0.7111824154853821\n",
      "################################  359  ################################\n",
      "Loss:  0.710756778717041\n",
      "################################  360  ################################\n",
      "Loss:  0.7103037238121033\n",
      "################################  361  ################################\n",
      "Loss:  0.7098751664161682\n",
      "################################  362  ################################\n",
      "Loss:  0.7094386219978333\n",
      "################################  363  ################################\n",
      "Loss:  0.7090040445327759\n",
      "################################  364  ################################\n",
      "Loss:  0.7084811925888062\n",
      "################################  365  ################################\n",
      "Loss:  0.7079522609710693\n",
      "################################  366  ################################\n",
      "Loss:  0.7073206305503845\n",
      "################################  367  ################################\n",
      "Loss:  0.7067649364471436\n",
      "################################  368  ################################\n",
      "Loss:  0.7062397599220276\n",
      "################################  369  ################################\n",
      "Loss:  0.7057142853736877\n",
      "################################  370  ################################\n",
      "Loss:  0.7051954865455627\n",
      "################################  371  ################################\n",
      "Loss:  0.704687237739563\n",
      "################################  372  ################################\n",
      "Loss:  0.7041845917701721\n",
      "################################  373  ################################\n",
      "Loss:  0.7036446928977966\n",
      "################################  374  ################################\n",
      "Loss:  0.7030916213989258\n",
      "################################  375  ################################\n",
      "Loss:  0.7025840878486633\n",
      "################################  376  ################################\n",
      "Loss:  0.7020492553710938\n",
      "################################  377  ################################\n",
      "Loss:  0.7015232443809509\n",
      "################################  378  ################################\n",
      "Loss:  0.7010571956634521\n",
      "################################  379  ################################\n",
      "Loss:  0.7005919218063354\n",
      "################################  380  ################################\n",
      "Loss:  0.7001681923866272\n",
      "################################  381  ################################\n",
      "Loss:  0.699779748916626\n",
      "################################  382  ################################\n",
      "Loss:  0.6994415521621704\n",
      "################################  383  ################################\n",
      "Loss:  0.6990877985954285\n",
      "################################  384  ################################\n",
      "Loss:  0.6987125277519226\n",
      "################################  385  ################################\n",
      "Loss:  0.6983003616333008\n",
      "################################  386  ################################\n",
      "Loss:  0.6979021430015564\n",
      "################################  387  ################################\n",
      "Loss:  0.6974730491638184\n",
      "################################  388  ################################\n",
      "Loss:  0.6969829201698303\n",
      "################################  389  ################################\n",
      "Loss:  0.6965330243110657\n",
      "################################  390  ################################\n",
      "Loss:  0.696098268032074\n",
      "################################  391  ################################\n",
      "Loss:  0.695688784122467\n",
      "################################  392  ################################\n",
      "Loss:  0.6952728033065796\n",
      "################################  393  ################################\n",
      "Loss:  0.6948102712631226\n",
      "################################  394  ################################\n",
      "Loss:  0.6944075226783752\n",
      "################################  395  ################################\n",
      "Loss:  0.6940531730651855\n",
      "################################  396  ################################\n",
      "Loss:  0.6936815977096558\n",
      "################################  397  ################################\n",
      "Loss:  0.693296492099762\n",
      "################################  398  ################################\n",
      "Loss:  0.692909300327301\n",
      "################################  399  ################################\n",
      "Loss:  0.6925398111343384\n",
      "################################  400  ################################\n",
      "Loss:  0.692197859287262\n",
      "################################  401  ################################\n",
      "Loss:  0.6918553709983826\n",
      "################################  402  ################################\n",
      "Loss:  0.6915103197097778\n",
      "################################  403  ################################\n",
      "Loss:  0.6911695599555969\n",
      "################################  404  ################################\n",
      "Loss:  0.6908234357833862\n",
      "################################  405  ################################\n",
      "Loss:  0.6904801726341248\n",
      "################################  406  ################################\n",
      "Loss:  0.6901276111602783\n",
      "################################  407  ################################\n",
      "Loss:  0.6897450089454651\n",
      "################################  408  ################################\n",
      "Loss:  0.6893283724784851\n",
      "################################  409  ################################\n",
      "Loss:  0.6888620853424072\n",
      "################################  410  ################################\n",
      "Loss:  0.6883013248443604\n",
      "################################  411  ################################\n",
      "Loss:  0.6876762509346008\n",
      "################################  412  ################################\n",
      "Loss:  0.6870067715644836\n",
      "################################  413  ################################\n",
      "Loss:  0.6862310171127319\n",
      "################################  414  ################################\n",
      "Loss:  0.6856939196586609\n",
      "################################  415  ################################\n",
      "Loss:  0.6851468682289124\n",
      "################################  416  ################################\n",
      "Loss:  0.6845327019691467\n",
      "################################  417  ################################\n",
      "Loss:  0.683953046798706\n",
      "################################  418  ################################\n",
      "Loss:  0.6833509206771851\n",
      "################################  419  ################################\n",
      "Loss:  0.682765007019043\n",
      "################################  420  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.6820400953292847\n",
      "################################  421  ################################\n",
      "Loss:  0.6812416315078735\n",
      "################################  422  ################################\n",
      "Loss:  0.6804775595664978\n",
      "################################  423  ################################\n",
      "Loss:  0.6796294450759888\n",
      "################################  424  ################################\n",
      "Loss:  0.6787418723106384\n",
      "################################  425  ################################\n",
      "Loss:  0.6779077053070068\n",
      "################################  426  ################################\n",
      "Loss:  0.6771001219749451\n",
      "################################  427  ################################\n",
      "Loss:  0.6762350797653198\n",
      "################################  428  ################################\n",
      "Loss:  0.6752864718437195\n",
      "################################  429  ################################\n",
      "Loss:  0.6744065284729004\n",
      "################################  430  ################################\n",
      "Loss:  0.6737210154533386\n",
      "################################  431  ################################\n",
      "Loss:  0.6729764342308044\n",
      "################################  432  ################################\n",
      "Loss:  0.6721376776695251\n",
      "################################  433  ################################\n",
      "Loss:  0.6713046431541443\n",
      "################################  434  ################################\n",
      "Loss:  0.6705052852630615\n",
      "################################  435  ################################\n",
      "Loss:  0.6697781085968018\n",
      "################################  436  ################################\n",
      "Loss:  0.6691789627075195\n",
      "################################  437  ################################\n",
      "Loss:  0.668584942817688\n",
      "################################  438  ################################\n",
      "Loss:  0.6678574085235596\n",
      "################################  439  ################################\n",
      "Loss:  0.6672865748405457\n",
      "################################  440  ################################\n",
      "Loss:  0.6668041348457336\n",
      "################################  441  ################################\n",
      "Loss:  0.6663481593132019\n",
      "################################  442  ################################\n",
      "Loss:  0.6658913493156433\n",
      "################################  443  ################################\n",
      "Loss:  0.6654760837554932\n",
      "################################  444  ################################\n",
      "Loss:  0.6650351881980896\n",
      "################################  445  ################################\n",
      "Loss:  0.6646006107330322\n",
      "################################  446  ################################\n",
      "Loss:  0.664182722568512\n",
      "################################  447  ################################\n",
      "Loss:  0.6637367606163025\n",
      "################################  448  ################################\n",
      "Loss:  0.6632764935493469\n",
      "################################  449  ################################\n",
      "Loss:  0.6628841757774353\n",
      "################################  450  ################################\n",
      "Loss:  0.6625022888183594\n",
      "################################  451  ################################\n",
      "Loss:  0.662136435508728\n",
      "################################  452  ################################\n",
      "Loss:  0.6617493033409119\n",
      "################################  453  ################################\n",
      "Loss:  0.6613681316375732\n",
      "################################  454  ################################\n",
      "Loss:  0.6609622836112976\n",
      "################################  455  ################################\n",
      "Loss:  0.6605560779571533\n",
      "################################  456  ################################\n",
      "Loss:  0.6601528525352478\n",
      "################################  457  ################################\n",
      "Loss:  0.6597744226455688\n",
      "################################  458  ################################\n",
      "Loss:  0.6594178676605225\n",
      "################################  459  ################################\n",
      "Loss:  0.6590538620948792\n",
      "################################  460  ################################\n",
      "Loss:  0.6586830615997314\n",
      "################################  461  ################################\n",
      "Loss:  0.6582925915718079\n",
      "################################  462  ################################\n",
      "Loss:  0.6579382419586182\n",
      "################################  463  ################################\n",
      "Loss:  0.6575921773910522\n",
      "################################  464  ################################\n",
      "Loss:  0.6572496294975281\n",
      "################################  465  ################################\n",
      "Loss:  0.6569178104400635\n",
      "################################  466  ################################\n",
      "Loss:  0.6565951704978943\n",
      "################################  467  ################################\n",
      "Loss:  0.6562786102294922\n",
      "################################  468  ################################\n",
      "Loss:  0.6559791564941406\n",
      "################################  469  ################################\n",
      "Loss:  0.655686616897583\n",
      "################################  470  ################################\n",
      "Loss:  0.6554257869720459\n",
      "################################  471  ################################\n",
      "Loss:  0.6551766395568848\n",
      "################################  472  ################################\n",
      "Loss:  0.6549409627914429\n",
      "################################  473  ################################\n",
      "Loss:  0.6546992659568787\n",
      "################################  474  ################################\n",
      "Loss:  0.6544602513313293\n",
      "################################  475  ################################\n",
      "Loss:  0.6542173027992249\n",
      "################################  476  ################################\n",
      "Loss:  0.6539731621742249\n",
      "################################  477  ################################\n",
      "Loss:  0.6536998152732849\n",
      "################################  478  ################################\n",
      "Loss:  0.653398334980011\n",
      "################################  479  ################################\n",
      "Loss:  0.6531064510345459\n",
      "################################  480  ################################\n",
      "Loss:  0.6528449058532715\n",
      "################################  481  ################################\n",
      "Loss:  0.6525869369506836\n",
      "################################  482  ################################\n",
      "Loss:  0.6523336172103882\n",
      "################################  483  ################################\n",
      "Loss:  0.652088463306427\n",
      "################################  484  ################################\n",
      "Loss:  0.6518611311912537\n",
      "################################  485  ################################\n",
      "Loss:  0.6516419649124146\n",
      "################################  486  ################################\n",
      "Loss:  0.6514309644699097\n",
      "################################  487  ################################\n",
      "Loss:  0.6512237191200256\n",
      "################################  488  ################################\n",
      "Loss:  0.6510220170021057\n",
      "################################  489  ################################\n",
      "Loss:  0.6508216261863708\n",
      "################################  490  ################################\n",
      "Loss:  0.6506274342536926\n",
      "################################  491  ################################\n",
      "Loss:  0.650436520576477\n",
      "################################  492  ################################\n",
      "Loss:  0.65024733543396\n",
      "################################  493  ################################\n",
      "Loss:  0.650057852268219\n",
      "################################  494  ################################\n",
      "Loss:  0.6498618721961975\n",
      "################################  495  ################################\n",
      "Loss:  0.6496566534042358\n",
      "################################  496  ################################\n",
      "Loss:  0.6494346857070923\n",
      "################################  497  ################################\n",
      "Loss:  0.6492100358009338\n",
      "################################  498  ################################\n",
      "Loss:  0.6489669680595398\n",
      "################################  499  ################################\n",
      "Loss:  0.6486985087394714\n",
      "################################  500  ################################\n",
      "Loss:  0.648388147354126\n",
      "################################  501  ################################\n",
      "Loss:  0.6480439305305481\n",
      "################################  502  ################################\n",
      "Loss:  0.6476474404335022\n",
      "################################  503  ################################\n",
      "Loss:  0.6472166180610657\n",
      "################################  504  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.6467629075050354\n",
      "################################  505  ################################\n",
      "Loss:  0.6463016867637634\n",
      "################################  506  ################################\n",
      "Loss:  0.645820677280426\n",
      "################################  507  ################################\n",
      "Loss:  0.6453148722648621\n",
      "################################  508  ################################\n",
      "Loss:  0.6448076367378235\n",
      "################################  509  ################################\n",
      "Loss:  0.644314169883728\n",
      "################################  510  ################################\n",
      "Loss:  0.643862783908844\n",
      "################################  511  ################################\n",
      "Loss:  0.6434260606765747\n",
      "################################  512  ################################\n",
      "Loss:  0.6430167555809021\n",
      "################################  513  ################################\n",
      "Loss:  0.6426098942756653\n",
      "################################  514  ################################\n",
      "Loss:  0.6422364711761475\n",
      "################################  515  ################################\n",
      "Loss:  0.6418736577033997\n",
      "################################  516  ################################\n",
      "Loss:  0.6415486931800842\n",
      "################################  517  ################################\n",
      "Loss:  0.641247034072876\n",
      "################################  518  ################################\n",
      "Loss:  0.6409735679626465\n",
      "################################  519  ################################\n",
      "Loss:  0.6407240629196167\n",
      "################################  520  ################################\n",
      "Loss:  0.640500009059906\n",
      "################################  521  ################################\n",
      "Loss:  0.6402959227561951\n",
      "################################  522  ################################\n",
      "Loss:  0.6401035785675049\n",
      "################################  523  ################################\n",
      "Loss:  0.6399061679840088\n",
      "################################  524  ################################\n",
      "Loss:  0.6396986842155457\n",
      "################################  525  ################################\n",
      "Loss:  0.6394649744033813\n",
      "################################  526  ################################\n",
      "Loss:  0.6392131447792053\n",
      "################################  527  ################################\n",
      "Loss:  0.6389657855033875\n",
      "################################  528  ################################\n",
      "Loss:  0.6386987566947937\n",
      "################################  529  ################################\n",
      "Loss:  0.6384177803993225\n",
      "################################  530  ################################\n",
      "Loss:  0.6381220817565918\n",
      "################################  531  ################################\n",
      "Loss:  0.6377225518226624\n",
      "################################  532  ################################\n",
      "Loss:  0.6374607086181641\n",
      "################################  533  ################################\n",
      "Loss:  0.6371763944625854\n",
      "################################  534  ################################\n",
      "Loss:  0.6368767023086548\n",
      "################################  535  ################################\n",
      "Loss:  0.6365481615066528\n",
      "################################  536  ################################\n",
      "Loss:  0.6362351775169373\n",
      "################################  537  ################################\n",
      "Loss:  0.6359055042266846\n",
      "################################  538  ################################\n",
      "Loss:  0.6355635523796082\n",
      "################################  539  ################################\n",
      "Loss:  0.6351909637451172\n",
      "################################  540  ################################\n",
      "Loss:  0.6347895264625549\n",
      "################################  541  ################################\n",
      "Loss:  0.6343992948532104\n",
      "################################  542  ################################\n",
      "Loss:  0.6339973211288452\n",
      "################################  543  ################################\n",
      "Loss:  0.633618950843811\n",
      "################################  544  ################################\n",
      "Loss:  0.6333045363426208\n",
      "################################  545  ################################\n",
      "Loss:  0.6330468654632568\n",
      "################################  546  ################################\n",
      "Loss:  0.632803738117218\n",
      "################################  547  ################################\n",
      "Loss:  0.6325486898422241\n",
      "################################  548  ################################\n",
      "Loss:  0.632291853427887\n",
      "################################  549  ################################\n",
      "Loss:  0.6320361495018005\n",
      "################################  550  ################################\n",
      "Loss:  0.6317607164382935\n",
      "################################  551  ################################\n",
      "Loss:  0.631512463092804\n",
      "################################  552  ################################\n",
      "Loss:  0.6312907338142395\n",
      "################################  553  ################################\n",
      "Loss:  0.6310703158378601\n",
      "################################  554  ################################\n",
      "Loss:  0.6308479905128479\n",
      "################################  555  ################################\n",
      "Loss:  0.63062584400177\n",
      "################################  556  ################################\n",
      "Loss:  0.6304044723510742\n",
      "################################  557  ################################\n",
      "Loss:  0.6301863193511963\n",
      "################################  558  ################################\n",
      "Loss:  0.629962682723999\n",
      "################################  559  ################################\n",
      "Loss:  0.6297183632850647\n",
      "################################  560  ################################\n",
      "Loss:  0.6294254064559937\n",
      "################################  561  ################################\n",
      "Loss:  0.6291570663452148\n",
      "################################  562  ################################\n",
      "Loss:  0.628888726234436\n",
      "################################  563  ################################\n",
      "Loss:  0.6285914778709412\n",
      "################################  564  ################################\n",
      "Loss:  0.6282615065574646\n",
      "################################  565  ################################\n",
      "Loss:  0.6279205083847046\n",
      "################################  566  ################################\n",
      "Loss:  0.6275777816772461\n",
      "################################  567  ################################\n",
      "Loss:  0.6272550225257874\n",
      "################################  568  ################################\n",
      "Loss:  0.6269224286079407\n",
      "################################  569  ################################\n",
      "Loss:  0.6265712380409241\n",
      "################################  570  ################################\n",
      "Loss:  0.6262297034263611\n",
      "################################  571  ################################\n",
      "Loss:  0.6258736252784729\n",
      "################################  572  ################################\n",
      "Loss:  0.6255387663841248\n",
      "################################  573  ################################\n",
      "Loss:  0.6252068281173706\n",
      "################################  574  ################################\n",
      "Loss:  0.6248878240585327\n",
      "################################  575  ################################\n",
      "Loss:  0.6245518922805786\n",
      "################################  576  ################################\n",
      "Loss:  0.6242172122001648\n",
      "################################  577  ################################\n",
      "Loss:  0.6238695979118347\n",
      "################################  578  ################################\n",
      "Loss:  0.6235129833221436\n",
      "################################  579  ################################\n",
      "Loss:  0.623151957988739\n",
      "################################  580  ################################\n",
      "Loss:  0.6227658987045288\n",
      "################################  581  ################################\n",
      "Loss:  0.62240070104599\n",
      "################################  582  ################################\n",
      "Loss:  0.6220188736915588\n",
      "################################  583  ################################\n",
      "Loss:  0.6216096878051758\n",
      "################################  584  ################################\n",
      "Loss:  0.6211803555488586\n",
      "################################  585  ################################\n",
      "Loss:  0.6206979155540466\n",
      "################################  586  ################################\n",
      "Loss:  0.6201786398887634\n",
      "################################  587  ################################\n",
      "Loss:  0.6196196675300598\n",
      "################################  588  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.6189970970153809\n",
      "################################  589  ################################\n",
      "Loss:  0.6183302402496338\n",
      "################################  590  ################################\n",
      "Loss:  0.6177139282226562\n",
      "################################  591  ################################\n",
      "Loss:  0.6170143485069275\n",
      "################################  592  ################################\n",
      "Loss:  0.6162799596786499\n",
      "################################  593  ################################\n",
      "Loss:  0.6154786944389343\n",
      "################################  594  ################################\n",
      "Loss:  0.614734947681427\n",
      "################################  595  ################################\n",
      "Loss:  0.6138845086097717\n",
      "################################  596  ################################\n",
      "Loss:  0.613014280796051\n",
      "################################  597  ################################\n",
      "Loss:  0.6120997667312622\n",
      "################################  598  ################################\n",
      "Loss:  0.6112939715385437\n",
      "################################  599  ################################\n",
      "Loss:  0.6105030179023743\n",
      "################################  600  ################################\n",
      "Loss:  0.6096480488777161\n",
      "################################  601  ################################\n",
      "Loss:  0.6088566184043884\n",
      "################################  602  ################################\n",
      "Loss:  0.6079493165016174\n",
      "################################  603  ################################\n",
      "Loss:  0.6071838736534119\n",
      "################################  604  ################################\n",
      "Loss:  0.6064885854721069\n",
      "################################  605  ################################\n",
      "Loss:  0.6056103110313416\n",
      "################################  606  ################################\n",
      "Loss:  0.6049842238426208\n",
      "################################  607  ################################\n",
      "Loss:  0.6044811010360718\n",
      "################################  608  ################################\n",
      "Loss:  0.604013979434967\n",
      "################################  609  ################################\n",
      "Loss:  0.6034523844718933\n",
      "################################  610  ################################\n",
      "Loss:  0.6029398441314697\n",
      "################################  611  ################################\n",
      "Loss:  0.6024635434150696\n",
      "################################  612  ################################\n",
      "Loss:  0.6020140051841736\n",
      "################################  613  ################################\n",
      "Loss:  0.6016489863395691\n",
      "################################  614  ################################\n",
      "Loss:  0.6012919545173645\n",
      "################################  615  ################################\n",
      "Loss:  0.6009414196014404\n",
      "################################  616  ################################\n",
      "Loss:  0.6006089448928833\n",
      "################################  617  ################################\n",
      "Loss:  0.6002703309059143\n",
      "################################  618  ################################\n",
      "Loss:  0.5999437570571899\n",
      "################################  619  ################################\n",
      "Loss:  0.5996190905570984\n",
      "################################  620  ################################\n",
      "Loss:  0.5992992520332336\n",
      "################################  621  ################################\n",
      "Loss:  0.5989854335784912\n",
      "################################  622  ################################\n",
      "Loss:  0.5986726880073547\n",
      "################################  623  ################################\n",
      "Loss:  0.5983853340148926\n",
      "################################  624  ################################\n",
      "Loss:  0.5981109738349915\n",
      "################################  625  ################################\n",
      "Loss:  0.5978735089302063\n",
      "################################  626  ################################\n",
      "Loss:  0.5976442098617554\n",
      "################################  627  ################################\n",
      "Loss:  0.5974084138870239\n",
      "################################  628  ################################\n",
      "Loss:  0.5971364378929138\n",
      "################################  629  ################################\n",
      "Loss:  0.5968812704086304\n",
      "################################  630  ################################\n",
      "Loss:  0.596634566783905\n",
      "################################  631  ################################\n",
      "Loss:  0.59637451171875\n",
      "################################  632  ################################\n",
      "Loss:  0.5960909128189087\n",
      "################################  633  ################################\n",
      "Loss:  0.5957736372947693\n",
      "################################  634  ################################\n",
      "Loss:  0.5954399704933167\n",
      "################################  635  ################################\n",
      "Loss:  0.5950781106948853\n",
      "################################  636  ################################\n",
      "Loss:  0.5947219133377075\n",
      "################################  637  ################################\n",
      "Loss:  0.5943273901939392\n",
      "################################  638  ################################\n",
      "Loss:  0.5939039587974548\n",
      "################################  639  ################################\n",
      "Loss:  0.5934719443321228\n",
      "################################  640  ################################\n",
      "Loss:  0.5930324792861938\n",
      "################################  641  ################################\n",
      "Loss:  0.5925894379615784\n",
      "################################  642  ################################\n",
      "Loss:  0.5921677947044373\n",
      "################################  643  ################################\n",
      "Loss:  0.5917275547981262\n",
      "################################  644  ################################\n",
      "Loss:  0.5913401246070862\n",
      "################################  645  ################################\n",
      "Loss:  0.5909678339958191\n",
      "################################  646  ################################\n",
      "Loss:  0.590679943561554\n",
      "################################  647  ################################\n",
      "Loss:  0.5903995633125305\n",
      "################################  648  ################################\n",
      "Loss:  0.5901303291320801\n",
      "################################  649  ################################\n",
      "Loss:  0.5898531675338745\n",
      "################################  650  ################################\n",
      "Loss:  0.5896056294441223\n",
      "################################  651  ################################\n",
      "Loss:  0.5894172787666321\n",
      "################################  652  ################################\n",
      "Loss:  0.5892395973205566\n",
      "################################  653  ################################\n",
      "Loss:  0.5890546441078186\n",
      "################################  654  ################################\n",
      "Loss:  0.5888600945472717\n",
      "################################  655  ################################\n",
      "Loss:  0.588653028011322\n",
      "################################  656  ################################\n",
      "Loss:  0.5884789228439331\n",
      "################################  657  ################################\n",
      "Loss:  0.5883027911186218\n",
      "################################  658  ################################\n",
      "Loss:  0.5880885124206543\n",
      "################################  659  ################################\n",
      "Loss:  0.587894082069397\n",
      "################################  660  ################################\n",
      "Loss:  0.5877026319503784\n",
      "################################  661  ################################\n",
      "Loss:  0.5875006318092346\n",
      "################################  662  ################################\n",
      "Loss:  0.5872822999954224\n",
      "################################  663  ################################\n",
      "Loss:  0.5870589017868042\n",
      "################################  664  ################################\n",
      "Loss:  0.5868149399757385\n",
      "################################  665  ################################\n",
      "Loss:  0.5865733623504639\n",
      "################################  666  ################################\n",
      "Loss:  0.5863091945648193\n",
      "################################  667  ################################\n",
      "Loss:  0.5860403180122375\n",
      "################################  668  ################################\n",
      "Loss:  0.5857574343681335\n",
      "################################  669  ################################\n",
      "Loss:  0.5854567885398865\n",
      "################################  670  ################################\n",
      "Loss:  0.5851361751556396\n",
      "################################  671  ################################\n",
      "Loss:  0.584805428981781\n",
      "################################  672  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5844774842262268\n",
      "################################  673  ################################\n",
      "Loss:  0.5841691493988037\n",
      "################################  674  ################################\n",
      "Loss:  0.5838601589202881\n",
      "################################  675  ################################\n",
      "Loss:  0.5835481286048889\n",
      "################################  676  ################################\n",
      "Loss:  0.5832157731056213\n",
      "################################  677  ################################\n",
      "Loss:  0.582902729511261\n",
      "################################  678  ################################\n",
      "Loss:  0.5826006531715393\n",
      "################################  679  ################################\n",
      "Loss:  0.5823211669921875\n",
      "################################  680  ################################\n",
      "Loss:  0.5820521116256714\n",
      "################################  681  ################################\n",
      "Loss:  0.5817986726760864\n",
      "################################  682  ################################\n",
      "Loss:  0.5815398097038269\n",
      "################################  683  ################################\n",
      "Loss:  0.5813056826591492\n",
      "################################  684  ################################\n",
      "Loss:  0.5811108350753784\n",
      "################################  685  ################################\n",
      "Loss:  0.5809334516525269\n",
      "################################  686  ################################\n",
      "Loss:  0.5807425379753113\n",
      "################################  687  ################################\n",
      "Loss:  0.5805274248123169\n",
      "################################  688  ################################\n",
      "Loss:  0.5803436636924744\n",
      "################################  689  ################################\n",
      "Loss:  0.5801594853401184\n",
      "################################  690  ################################\n",
      "Loss:  0.5799752473831177\n",
      "################################  691  ################################\n",
      "Loss:  0.5797943472862244\n",
      "################################  692  ################################\n",
      "Loss:  0.5796002745628357\n",
      "################################  693  ################################\n",
      "Loss:  0.5794025659561157\n",
      "################################  694  ################################\n",
      "Loss:  0.5792100429534912\n",
      "################################  695  ################################\n",
      "Loss:  0.579014241695404\n",
      "################################  696  ################################\n",
      "Loss:  0.5788100957870483\n",
      "################################  697  ################################\n",
      "Loss:  0.5785973072052002\n",
      "################################  698  ################################\n",
      "Loss:  0.5783662796020508\n",
      "################################  699  ################################\n",
      "Loss:  0.578149676322937\n",
      "################################  700  ################################\n",
      "Loss:  0.5779274106025696\n",
      "################################  701  ################################\n",
      "Loss:  0.5776810646057129\n",
      "################################  702  ################################\n",
      "Loss:  0.5774348378181458\n",
      "################################  703  ################################\n",
      "Loss:  0.5771822929382324\n",
      "################################  704  ################################\n",
      "Loss:  0.5769328474998474\n",
      "################################  705  ################################\n",
      "Loss:  0.576674222946167\n",
      "################################  706  ################################\n",
      "Loss:  0.5764405131340027\n",
      "################################  707  ################################\n",
      "Loss:  0.5762183666229248\n",
      "################################  708  ################################\n",
      "Loss:  0.5759983658790588\n",
      "################################  709  ################################\n",
      "Loss:  0.5757789015769958\n",
      "################################  710  ################################\n",
      "Loss:  0.5755558609962463\n",
      "################################  711  ################################\n",
      "Loss:  0.5753538012504578\n",
      "################################  712  ################################\n",
      "Loss:  0.5751537084579468\n",
      "################################  713  ################################\n",
      "Loss:  0.5749682784080505\n",
      "################################  714  ################################\n",
      "Loss:  0.5747815370559692\n",
      "################################  715  ################################\n",
      "Loss:  0.5746049284934998\n",
      "################################  716  ################################\n",
      "Loss:  0.5744422674179077\n",
      "################################  717  ################################\n",
      "Loss:  0.5742796659469604\n",
      "################################  718  ################################\n",
      "Loss:  0.5741205811500549\n",
      "################################  719  ################################\n",
      "Loss:  0.5739706754684448\n",
      "################################  720  ################################\n",
      "Loss:  0.5738335251808167\n",
      "################################  721  ################################\n",
      "Loss:  0.5737016201019287\n",
      "################################  722  ################################\n",
      "Loss:  0.5735738277435303\n",
      "################################  723  ################################\n",
      "Loss:  0.573453426361084\n",
      "################################  724  ################################\n",
      "Loss:  0.5733292698860168\n",
      "################################  725  ################################\n",
      "Loss:  0.573214590549469\n",
      "################################  726  ################################\n",
      "Loss:  0.5730973482131958\n",
      "################################  727  ################################\n",
      "Loss:  0.5729869604110718\n",
      "################################  728  ################################\n",
      "Loss:  0.5728710889816284\n",
      "################################  729  ################################\n",
      "Loss:  0.572762131690979\n",
      "################################  730  ################################\n",
      "Loss:  0.5726688504219055\n",
      "################################  731  ################################\n",
      "Loss:  0.5725758075714111\n",
      "################################  732  ################################\n",
      "Loss:  0.5724693536758423\n",
      "################################  733  ################################\n",
      "Loss:  0.5723761320114136\n",
      "################################  734  ################################\n",
      "Loss:  0.5722855925559998\n",
      "################################  735  ################################\n",
      "Loss:  0.5721970796585083\n",
      "################################  736  ################################\n",
      "Loss:  0.5721110701560974\n",
      "################################  737  ################################\n",
      "Loss:  0.5720267295837402\n",
      "################################  738  ################################\n",
      "Loss:  0.5719406008720398\n",
      "################################  739  ################################\n",
      "Loss:  0.5718547105789185\n",
      "################################  740  ################################\n",
      "Loss:  0.5717737078666687\n",
      "################################  741  ################################\n",
      "Loss:  0.5716965198516846\n",
      "################################  742  ################################\n",
      "Loss:  0.5716208815574646\n",
      "################################  743  ################################\n",
      "Loss:  0.5715455412864685\n",
      "################################  744  ################################\n",
      "Loss:  0.5714666843414307\n",
      "################################  745  ################################\n",
      "Loss:  0.5713904500007629\n",
      "################################  746  ################################\n",
      "Loss:  0.571315586566925\n",
      "################################  747  ################################\n",
      "Loss:  0.5712399482727051\n",
      "################################  748  ################################\n",
      "Loss:  0.5711626410484314\n",
      "################################  749  ################################\n",
      "Loss:  0.5710868239402771\n",
      "################################  750  ################################\n",
      "Loss:  0.5710037350654602\n",
      "################################  751  ################################\n",
      "Loss:  0.5709227919578552\n",
      "################################  752  ################################\n",
      "Loss:  0.5708366632461548\n",
      "################################  753  ################################\n",
      "Loss:  0.5707528591156006\n",
      "################################  754  ################################\n",
      "Loss:  0.570664644241333\n",
      "################################  755  ################################\n",
      "Loss:  0.5705780982971191\n",
      "################################  756  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5705006718635559\n",
      "################################  757  ################################\n",
      "Loss:  0.57041996717453\n",
      "################################  758  ################################\n",
      "Loss:  0.5703394412994385\n",
      "################################  759  ################################\n",
      "Loss:  0.5702582597732544\n",
      "################################  760  ################################\n",
      "Loss:  0.570171058177948\n",
      "################################  761  ################################\n",
      "Loss:  0.5700958967208862\n",
      "################################  762  ################################\n",
      "Loss:  0.5700181126594543\n",
      "################################  763  ################################\n",
      "Loss:  0.5699423551559448\n",
      "################################  764  ################################\n",
      "Loss:  0.569872260093689\n",
      "################################  765  ################################\n",
      "Loss:  0.5698025822639465\n",
      "################################  766  ################################\n",
      "Loss:  0.5697298645973206\n",
      "################################  767  ################################\n",
      "Loss:  0.5696565508842468\n",
      "################################  768  ################################\n",
      "Loss:  0.5695805549621582\n",
      "################################  769  ################################\n",
      "Loss:  0.5695005059242249\n",
      "################################  770  ################################\n",
      "Loss:  0.5694187879562378\n",
      "################################  771  ################################\n",
      "Loss:  0.5693326592445374\n",
      "################################  772  ################################\n",
      "Loss:  0.5692456960678101\n",
      "################################  773  ################################\n",
      "Loss:  0.5691561102867126\n",
      "################################  774  ################################\n",
      "Loss:  0.5690658688545227\n",
      "################################  775  ################################\n",
      "Loss:  0.5689743161201477\n",
      "################################  776  ################################\n",
      "Loss:  0.568877100944519\n",
      "################################  777  ################################\n",
      "Loss:  0.5687791109085083\n",
      "################################  778  ################################\n",
      "Loss:  0.5686779022216797\n",
      "################################  779  ################################\n",
      "Loss:  0.5685755610466003\n",
      "################################  780  ################################\n",
      "Loss:  0.5684652328491211\n",
      "################################  781  ################################\n",
      "Loss:  0.5683539509773254\n",
      "################################  782  ################################\n",
      "Loss:  0.5682403445243835\n",
      "################################  783  ################################\n",
      "Loss:  0.5681154131889343\n",
      "################################  784  ################################\n",
      "Loss:  0.5679931044578552\n",
      "################################  785  ################################\n",
      "Loss:  0.5678682327270508\n",
      "################################  786  ################################\n",
      "Loss:  0.5677338242530823\n",
      "################################  787  ################################\n",
      "Loss:  0.5676006078720093\n",
      "################################  788  ################################\n",
      "Loss:  0.5674610733985901\n",
      "################################  789  ################################\n",
      "Loss:  0.5673379898071289\n",
      "################################  790  ################################\n",
      "Loss:  0.5672164559364319\n",
      "################################  791  ################################\n",
      "Loss:  0.5670934319496155\n",
      "################################  792  ################################\n",
      "Loss:  0.5669806599617004\n",
      "################################  793  ################################\n",
      "Loss:  0.5668706297874451\n",
      "################################  794  ################################\n",
      "Loss:  0.5667763352394104\n",
      "################################  795  ################################\n",
      "Loss:  0.5666800737380981\n",
      "################################  796  ################################\n",
      "Loss:  0.5665892958641052\n",
      "################################  797  ################################\n",
      "Loss:  0.5665032267570496\n",
      "################################  798  ################################\n",
      "Loss:  0.566425621509552\n",
      "################################  799  ################################\n",
      "Loss:  0.5663549900054932\n",
      "################################  800  ################################\n",
      "Loss:  0.5662834644317627\n",
      "################################  801  ################################\n",
      "Loss:  0.5662147402763367\n",
      "################################  802  ################################\n",
      "Loss:  0.5661452412605286\n",
      "################################  803  ################################\n",
      "Loss:  0.5660735964775085\n",
      "################################  804  ################################\n",
      "Loss:  0.5660043358802795\n",
      "################################  805  ################################\n",
      "Loss:  0.565937340259552\n",
      "################################  806  ################################\n",
      "Loss:  0.565869927406311\n",
      "################################  807  ################################\n",
      "Loss:  0.5657998919487\n",
      "################################  808  ################################\n",
      "Loss:  0.5657303929328918\n",
      "################################  809  ################################\n",
      "Loss:  0.5656596422195435\n",
      "################################  810  ################################\n",
      "Loss:  0.5655777454376221\n",
      "################################  811  ################################\n",
      "Loss:  0.5655030608177185\n",
      "################################  812  ################################\n",
      "Loss:  0.5654232501983643\n",
      "################################  813  ################################\n",
      "Loss:  0.5653427243232727\n",
      "################################  814  ################################\n",
      "Loss:  0.5652515292167664\n",
      "################################  815  ################################\n",
      "Loss:  0.5651394724845886\n",
      "################################  816  ################################\n",
      "Loss:  0.5650250911712646\n",
      "################################  817  ################################\n",
      "Loss:  0.5649000406265259\n",
      "################################  818  ################################\n",
      "Loss:  0.5647706985473633\n",
      "################################  819  ################################\n",
      "Loss:  0.5646283626556396\n",
      "################################  820  ################################\n",
      "Loss:  0.5644845962524414\n",
      "################################  821  ################################\n",
      "Loss:  0.5643337368965149\n",
      "################################  822  ################################\n",
      "Loss:  0.5641679167747498\n",
      "################################  823  ################################\n",
      "Loss:  0.564019501209259\n",
      "################################  824  ################################\n",
      "Loss:  0.5638595223426819\n",
      "################################  825  ################################\n",
      "Loss:  0.5636926293373108\n",
      "################################  826  ################################\n",
      "Loss:  0.5635203719139099\n",
      "################################  827  ################################\n",
      "Loss:  0.563353955745697\n",
      "################################  828  ################################\n",
      "Loss:  0.5631778836250305\n",
      "################################  829  ################################\n",
      "Loss:  0.5630013346672058\n",
      "################################  830  ################################\n",
      "Loss:  0.5628113150596619\n",
      "################################  831  ################################\n",
      "Loss:  0.5626195669174194\n",
      "################################  832  ################################\n",
      "Loss:  0.5624313950538635\n",
      "################################  833  ################################\n",
      "Loss:  0.5622502565383911\n",
      "################################  834  ################################\n",
      "Loss:  0.5620769262313843\n",
      "################################  835  ################################\n",
      "Loss:  0.5619150400161743\n",
      "################################  836  ################################\n",
      "Loss:  0.5617589950561523\n",
      "################################  837  ################################\n",
      "Loss:  0.5616011023521423\n",
      "################################  838  ################################\n",
      "Loss:  0.5614487528800964\n",
      "################################  839  ################################\n",
      "Loss:  0.5612886548042297\n",
      "################################  840  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5611463785171509\n",
      "################################  841  ################################\n",
      "Loss:  0.5610076785087585\n",
      "################################  842  ################################\n",
      "Loss:  0.5608636736869812\n",
      "################################  843  ################################\n",
      "Loss:  0.5607162117958069\n",
      "################################  844  ################################\n",
      "Loss:  0.5605728626251221\n",
      "################################  845  ################################\n",
      "Loss:  0.5604318976402283\n",
      "################################  846  ################################\n",
      "Loss:  0.5602967143058777\n",
      "################################  847  ################################\n",
      "Loss:  0.5601606369018555\n",
      "################################  848  ################################\n",
      "Loss:  0.5600183010101318\n",
      "################################  849  ################################\n",
      "Loss:  0.5598651766777039\n",
      "################################  850  ################################\n",
      "Loss:  0.5597289800643921\n",
      "################################  851  ################################\n",
      "Loss:  0.5595946907997131\n",
      "################################  852  ################################\n",
      "Loss:  0.5594517588615417\n",
      "################################  853  ################################\n",
      "Loss:  0.5593265295028687\n",
      "################################  854  ################################\n",
      "Loss:  0.5592041015625\n",
      "################################  855  ################################\n",
      "Loss:  0.5590865612030029\n",
      "################################  856  ################################\n",
      "Loss:  0.5589570999145508\n",
      "################################  857  ################################\n",
      "Loss:  0.5588306784629822\n",
      "################################  858  ################################\n",
      "Loss:  0.5587238073348999\n",
      "################################  859  ################################\n",
      "Loss:  0.5586293935775757\n",
      "################################  860  ################################\n",
      "Loss:  0.5585293769836426\n",
      "################################  861  ################################\n",
      "Loss:  0.5584319233894348\n",
      "################################  862  ################################\n",
      "Loss:  0.5583373308181763\n",
      "################################  863  ################################\n",
      "Loss:  0.5582353472709656\n",
      "################################  864  ################################\n",
      "Loss:  0.558134913444519\n",
      "################################  865  ################################\n",
      "Loss:  0.5580229163169861\n",
      "################################  866  ################################\n",
      "Loss:  0.5579092502593994\n",
      "################################  867  ################################\n",
      "Loss:  0.557783842086792\n",
      "################################  868  ################################\n",
      "Loss:  0.5576421022415161\n",
      "################################  869  ################################\n",
      "Loss:  0.5574925541877747\n",
      "################################  870  ################################\n",
      "Loss:  0.5573287606239319\n",
      "################################  871  ################################\n",
      "Loss:  0.5571785569190979\n",
      "################################  872  ################################\n",
      "Loss:  0.5570252537727356\n",
      "################################  873  ################################\n",
      "Loss:  0.5568664073944092\n",
      "################################  874  ################################\n",
      "Loss:  0.5567041635513306\n",
      "################################  875  ################################\n",
      "Loss:  0.5565255284309387\n",
      "################################  876  ################################\n",
      "Loss:  0.5563449859619141\n",
      "################################  877  ################################\n",
      "Loss:  0.5561767220497131\n",
      "################################  878  ################################\n",
      "Loss:  0.5560196042060852\n",
      "################################  879  ################################\n",
      "Loss:  0.5558667182922363\n",
      "################################  880  ################################\n",
      "Loss:  0.5557047724723816\n",
      "################################  881  ################################\n",
      "Loss:  0.5555503368377686\n",
      "################################  882  ################################\n",
      "Loss:  0.5553873777389526\n",
      "################################  883  ################################\n",
      "Loss:  0.5552345514297485\n",
      "################################  884  ################################\n",
      "Loss:  0.5550709962844849\n",
      "################################  885  ################################\n",
      "Loss:  0.5549178719520569\n",
      "################################  886  ################################\n",
      "Loss:  0.5547555088996887\n",
      "################################  887  ################################\n",
      "Loss:  0.5546020269393921\n",
      "################################  888  ################################\n",
      "Loss:  0.5544441938400269\n",
      "################################  889  ################################\n",
      "Loss:  0.5542860627174377\n",
      "################################  890  ################################\n",
      "Loss:  0.5541273355484009\n",
      "################################  891  ################################\n",
      "Loss:  0.5539560914039612\n",
      "################################  892  ################################\n",
      "Loss:  0.5537999868392944\n",
      "################################  893  ################################\n",
      "Loss:  0.553642749786377\n",
      "################################  894  ################################\n",
      "Loss:  0.5534844398498535\n",
      "################################  895  ################################\n",
      "Loss:  0.5533300638198853\n",
      "################################  896  ################################\n",
      "Loss:  0.5531665682792664\n",
      "################################  897  ################################\n",
      "Loss:  0.553027868270874\n",
      "################################  898  ################################\n",
      "Loss:  0.5528824925422668\n",
      "################################  899  ################################\n",
      "Loss:  0.5527260899543762\n",
      "################################  900  ################################\n",
      "Loss:  0.5525560975074768\n",
      "################################  901  ################################\n",
      "Loss:  0.5523965358734131\n",
      "################################  902  ################################\n",
      "Loss:  0.5522286295890808\n",
      "################################  903  ################################\n",
      "Loss:  0.5520434379577637\n",
      "################################  904  ################################\n",
      "Loss:  0.5518550276756287\n",
      "################################  905  ################################\n",
      "Loss:  0.5516719818115234\n",
      "################################  906  ################################\n",
      "Loss:  0.5514883399009705\n",
      "################################  907  ################################\n",
      "Loss:  0.5512987375259399\n",
      "################################  908  ################################\n",
      "Loss:  0.5511088967323303\n",
      "################################  909  ################################\n",
      "Loss:  0.5509322285652161\n",
      "################################  910  ################################\n",
      "Loss:  0.5507649183273315\n",
      "################################  911  ################################\n",
      "Loss:  0.5506053566932678\n",
      "################################  912  ################################\n",
      "Loss:  0.5504387021064758\n",
      "################################  913  ################################\n",
      "Loss:  0.5502747297286987\n",
      "################################  914  ################################\n",
      "Loss:  0.5501139760017395\n",
      "################################  915  ################################\n",
      "Loss:  0.5499441027641296\n",
      "################################  916  ################################\n",
      "Loss:  0.5497814416885376\n",
      "################################  917  ################################\n",
      "Loss:  0.5496221780776978\n",
      "################################  918  ################################\n",
      "Loss:  0.5494634509086609\n",
      "################################  919  ################################\n",
      "Loss:  0.5493088960647583\n",
      "################################  920  ################################\n",
      "Loss:  0.5491307973861694\n",
      "################################  921  ################################\n",
      "Loss:  0.5489636659622192\n",
      "################################  922  ################################\n",
      "Loss:  0.5487802624702454\n",
      "################################  923  ################################\n",
      "Loss:  0.5485857725143433\n",
      "################################  924  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5483978390693665\n",
      "################################  925  ################################\n",
      "Loss:  0.5482151508331299\n",
      "################################  926  ################################\n",
      "Loss:  0.5480407476425171\n",
      "################################  927  ################################\n",
      "Loss:  0.5478693842887878\n",
      "################################  928  ################################\n",
      "Loss:  0.5476901531219482\n",
      "################################  929  ################################\n",
      "Loss:  0.5475015044212341\n",
      "################################  930  ################################\n",
      "Loss:  0.5473395586013794\n",
      "################################  931  ################################\n",
      "Loss:  0.547184944152832\n",
      "################################  932  ################################\n",
      "Loss:  0.5470260381698608\n",
      "################################  933  ################################\n",
      "Loss:  0.5468478202819824\n",
      "################################  934  ################################\n",
      "Loss:  0.5466620922088623\n",
      "################################  935  ################################\n",
      "Loss:  0.5464819073677063\n",
      "################################  936  ################################\n",
      "Loss:  0.5462864637374878\n",
      "################################  937  ################################\n",
      "Loss:  0.5460956692695618\n",
      "################################  938  ################################\n",
      "Loss:  0.5458884835243225\n",
      "################################  939  ################################\n",
      "Loss:  0.5456555485725403\n",
      "################################  940  ################################\n",
      "Loss:  0.5454268455505371\n",
      "################################  941  ################################\n",
      "Loss:  0.5452024340629578\n",
      "################################  942  ################################\n",
      "Loss:  0.5449809432029724\n",
      "################################  943  ################################\n",
      "Loss:  0.5447690486907959\n",
      "################################  944  ################################\n",
      "Loss:  0.5445613265037537\n",
      "################################  945  ################################\n",
      "Loss:  0.54437255859375\n",
      "################################  946  ################################\n",
      "Loss:  0.5441832542419434\n",
      "################################  947  ################################\n",
      "Loss:  0.543993353843689\n",
      "################################  948  ################################\n",
      "Loss:  0.5438154935836792\n",
      "################################  949  ################################\n",
      "Loss:  0.5436457991600037\n",
      "################################  950  ################################\n",
      "Loss:  0.5435041189193726\n",
      "################################  951  ################################\n",
      "Loss:  0.5433749556541443\n",
      "################################  952  ################################\n",
      "Loss:  0.5432575345039368\n",
      "################################  953  ################################\n",
      "Loss:  0.5431414842605591\n",
      "################################  954  ################################\n",
      "Loss:  0.5430257320404053\n",
      "################################  955  ################################\n",
      "Loss:  0.5429232716560364\n",
      "################################  956  ################################\n",
      "Loss:  0.5428194999694824\n",
      "################################  957  ################################\n",
      "Loss:  0.5427225828170776\n",
      "################################  958  ################################\n",
      "Loss:  0.5426166653633118\n",
      "################################  959  ################################\n",
      "Loss:  0.5424996614456177\n",
      "################################  960  ################################\n",
      "Loss:  0.5423943400382996\n",
      "################################  961  ################################\n",
      "Loss:  0.5422921180725098\n",
      "################################  962  ################################\n",
      "Loss:  0.5421837568283081\n",
      "################################  963  ################################\n",
      "Loss:  0.5420646071434021\n",
      "################################  964  ################################\n",
      "Loss:  0.5419387221336365\n",
      "################################  965  ################################\n",
      "Loss:  0.5418171882629395\n",
      "################################  966  ################################\n",
      "Loss:  0.541681170463562\n",
      "################################  967  ################################\n",
      "Loss:  0.5415570735931396\n",
      "################################  968  ################################\n",
      "Loss:  0.5414241552352905\n",
      "################################  969  ################################\n",
      "Loss:  0.5412852764129639\n",
      "################################  970  ################################\n",
      "Loss:  0.5411316752433777\n",
      "################################  971  ################################\n",
      "Loss:  0.540983259677887\n",
      "################################  972  ################################\n",
      "Loss:  0.5408240556716919\n",
      "################################  973  ################################\n",
      "Loss:  0.5406548380851746\n",
      "################################  974  ################################\n",
      "Loss:  0.540477991104126\n",
      "################################  975  ################################\n",
      "Loss:  0.5402927994728088\n",
      "################################  976  ################################\n",
      "Loss:  0.5401003956794739\n",
      "################################  977  ################################\n",
      "Loss:  0.5399060845375061\n",
      "################################  978  ################################\n",
      "Loss:  0.539716899394989\n",
      "################################  979  ################################\n",
      "Loss:  0.5395213961601257\n",
      "################################  980  ################################\n",
      "Loss:  0.5393303036689758\n",
      "################################  981  ################################\n",
      "Loss:  0.5391456484794617\n",
      "################################  982  ################################\n",
      "Loss:  0.5389711856842041\n",
      "################################  983  ################################\n",
      "Loss:  0.5387951731681824\n",
      "################################  984  ################################\n",
      "Loss:  0.5386149287223816\n",
      "################################  985  ################################\n",
      "Loss:  0.53843754529953\n",
      "################################  986  ################################\n",
      "Loss:  0.5382505059242249\n",
      "################################  987  ################################\n",
      "Loss:  0.5380646586418152\n",
      "################################  988  ################################\n",
      "Loss:  0.537869393825531\n",
      "################################  989  ################################\n",
      "Loss:  0.5376753211021423\n",
      "################################  990  ################################\n",
      "Loss:  0.5374753475189209\n",
      "################################  991  ################################\n",
      "Loss:  0.5372630953788757\n",
      "################################  992  ################################\n",
      "Loss:  0.5370472073554993\n",
      "################################  993  ################################\n",
      "Loss:  0.5368196964263916\n",
      "################################  994  ################################\n",
      "Loss:  0.5365850329399109\n",
      "################################  995  ################################\n",
      "Loss:  0.536329448223114\n",
      "################################  996  ################################\n",
      "Loss:  0.5360689759254456\n",
      "################################  997  ################################\n",
      "Loss:  0.5358018279075623\n",
      "################################  998  ################################\n",
      "Loss:  0.5355362892150879\n",
      "################################  999  ################################\n",
      "Loss:  0.5352423191070557\n",
      "################################  1000  ################################\n",
      "Loss:  0.5349039435386658\n",
      "################################  1001  ################################\n",
      "Loss:  0.5346081256866455\n",
      "################################  1002  ################################\n",
      "Loss:  0.5343050956726074\n",
      "################################  1003  ################################\n",
      "Loss:  0.533959686756134\n",
      "################################  1004  ################################\n",
      "Loss:  0.5335932970046997\n",
      "################################  1005  ################################\n",
      "Loss:  0.5331962704658508\n",
      "################################  1006  ################################\n",
      "Loss:  0.5327555537223816\n",
      "################################  1007  ################################\n",
      "Loss:  0.5322509407997131\n",
      "################################  1008  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5316991806030273\n",
      "################################  1009  ################################\n",
      "Loss:  0.5312334299087524\n",
      "################################  1010  ################################\n",
      "Loss:  0.5307754278182983\n",
      "################################  1011  ################################\n",
      "Loss:  0.5302948355674744\n",
      "################################  1012  ################################\n",
      "Loss:  0.5298410058021545\n",
      "################################  1013  ################################\n",
      "Loss:  0.5293803215026855\n",
      "################################  1014  ################################\n",
      "Loss:  0.5289435386657715\n",
      "################################  1015  ################################\n",
      "Loss:  0.5285077095031738\n",
      "################################  1016  ################################\n",
      "Loss:  0.5281097888946533\n",
      "################################  1017  ################################\n",
      "Loss:  0.5277301669120789\n",
      "################################  1018  ################################\n",
      "Loss:  0.5273922681808472\n",
      "################################  1019  ################################\n",
      "Loss:  0.5270769596099854\n",
      "################################  1020  ################################\n",
      "Loss:  0.5267918109893799\n",
      "################################  1021  ################################\n",
      "Loss:  0.5265169143676758\n",
      "################################  1022  ################################\n",
      "Loss:  0.5262576341629028\n",
      "################################  1023  ################################\n",
      "Loss:  0.5260009169578552\n",
      "################################  1024  ################################\n",
      "Loss:  0.5257312655448914\n",
      "################################  1025  ################################\n",
      "Loss:  0.5255109071731567\n",
      "################################  1026  ################################\n",
      "Loss:  0.5253030061721802\n",
      "################################  1027  ################################\n",
      "Loss:  0.525105357170105\n",
      "################################  1028  ################################\n",
      "Loss:  0.524903416633606\n",
      "################################  1029  ################################\n",
      "Loss:  0.5246712565422058\n",
      "################################  1030  ################################\n",
      "Loss:  0.5244327187538147\n",
      "################################  1031  ################################\n",
      "Loss:  0.5241855978965759\n",
      "################################  1032  ################################\n",
      "Loss:  0.523955762386322\n",
      "################################  1033  ################################\n",
      "Loss:  0.523727297782898\n",
      "################################  1034  ################################\n",
      "Loss:  0.5234795808792114\n",
      "################################  1035  ################################\n",
      "Loss:  0.5232452750205994\n",
      "################################  1036  ################################\n",
      "Loss:  0.5229924321174622\n",
      "################################  1037  ################################\n",
      "Loss:  0.5227314829826355\n",
      "################################  1038  ################################\n",
      "Loss:  0.5224243402481079\n",
      "################################  1039  ################################\n",
      "Loss:  0.5221343636512756\n",
      "################################  1040  ################################\n",
      "Loss:  0.5218519568443298\n",
      "################################  1041  ################################\n",
      "Loss:  0.5215625762939453\n",
      "################################  1042  ################################\n",
      "Loss:  0.5212695598602295\n",
      "################################  1043  ################################\n",
      "Loss:  0.5209718942642212\n",
      "################################  1044  ################################\n",
      "Loss:  0.5206729769706726\n",
      "################################  1045  ################################\n",
      "Loss:  0.520370364189148\n",
      "################################  1046  ################################\n",
      "Loss:  0.5200644135475159\n",
      "################################  1047  ################################\n",
      "Loss:  0.519749104976654\n",
      "################################  1048  ################################\n",
      "Loss:  0.5194299817085266\n",
      "################################  1049  ################################\n",
      "Loss:  0.5191459655761719\n",
      "################################  1050  ################################\n",
      "Loss:  0.5188838243484497\n",
      "################################  1051  ################################\n",
      "Loss:  0.5186406373977661\n",
      "################################  1052  ################################\n",
      "Loss:  0.5183839797973633\n",
      "################################  1053  ################################\n",
      "Loss:  0.5181363821029663\n",
      "################################  1054  ################################\n",
      "Loss:  0.5179008841514587\n",
      "################################  1055  ################################\n",
      "Loss:  0.5176653265953064\n",
      "################################  1056  ################################\n",
      "Loss:  0.5174185037612915\n",
      "################################  1057  ################################\n",
      "Loss:  0.51719069480896\n",
      "################################  1058  ################################\n",
      "Loss:  0.5169575214385986\n",
      "################################  1059  ################################\n",
      "Loss:  0.516692042350769\n",
      "################################  1060  ################################\n",
      "Loss:  0.5164410471916199\n",
      "################################  1061  ################################\n",
      "Loss:  0.5161963105201721\n",
      "################################  1062  ################################\n",
      "Loss:  0.5159162282943726\n",
      "################################  1063  ################################\n",
      "Loss:  0.5156291127204895\n",
      "################################  1064  ################################\n",
      "Loss:  0.5153340697288513\n",
      "################################  1065  ################################\n",
      "Loss:  0.5150594115257263\n",
      "################################  1066  ################################\n",
      "Loss:  0.5147757530212402\n",
      "################################  1067  ################################\n",
      "Loss:  0.5144873857498169\n",
      "################################  1068  ################################\n",
      "Loss:  0.514172375202179\n",
      "################################  1069  ################################\n",
      "Loss:  0.5138612389564514\n",
      "################################  1070  ################################\n",
      "Loss:  0.5135588049888611\n",
      "################################  1071  ################################\n",
      "Loss:  0.5132701992988586\n",
      "################################  1072  ################################\n",
      "Loss:  0.5129767656326294\n",
      "################################  1073  ################################\n",
      "Loss:  0.5126959681510925\n",
      "################################  1074  ################################\n",
      "Loss:  0.5124247670173645\n",
      "################################  1075  ################################\n",
      "Loss:  0.5121504664421082\n",
      "################################  1076  ################################\n",
      "Loss:  0.511902391910553\n",
      "################################  1077  ################################\n",
      "Loss:  0.5116710066795349\n",
      "################################  1078  ################################\n",
      "Loss:  0.5114798545837402\n",
      "################################  1079  ################################\n",
      "Loss:  0.5112725496292114\n",
      "################################  1080  ################################\n",
      "Loss:  0.5110886693000793\n",
      "################################  1081  ################################\n",
      "Loss:  0.5109528303146362\n",
      "################################  1082  ################################\n",
      "Loss:  0.5108319520950317\n",
      "################################  1083  ################################\n",
      "Loss:  0.5107067227363586\n",
      "################################  1084  ################################\n",
      "Loss:  0.5105803608894348\n",
      "################################  1085  ################################\n",
      "Loss:  0.5104308128356934\n",
      "################################  1086  ################################\n",
      "Loss:  0.5102969408035278\n",
      "################################  1087  ################################\n",
      "Loss:  0.5101926922798157\n",
      "################################  1088  ################################\n",
      "Loss:  0.5100850462913513\n",
      "################################  1089  ################################\n",
      "Loss:  0.5099717974662781\n",
      "################################  1090  ################################\n",
      "Loss:  0.5098631978034973\n",
      "################################  1091  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5097497701644897\n",
      "################################  1092  ################################\n",
      "Loss:  0.5096336007118225\n",
      "################################  1093  ################################\n",
      "Loss:  0.5095228552818298\n",
      "################################  1094  ################################\n",
      "Loss:  0.5094190239906311\n",
      "################################  1095  ################################\n",
      "Loss:  0.5093259215354919\n",
      "################################  1096  ################################\n",
      "Loss:  0.5092344284057617\n",
      "################################  1097  ################################\n",
      "Loss:  0.5091431736946106\n",
      "################################  1098  ################################\n",
      "Loss:  0.5090494155883789\n",
      "################################  1099  ################################\n",
      "Loss:  0.5089613795280457\n",
      "################################  1100  ################################\n",
      "Loss:  0.5088722705841064\n",
      "################################  1101  ################################\n",
      "Loss:  0.5087782740592957\n",
      "################################  1102  ################################\n",
      "Loss:  0.5086793303489685\n",
      "################################  1103  ################################\n",
      "Loss:  0.5085893869400024\n",
      "################################  1104  ################################\n",
      "Loss:  0.5085049867630005\n",
      "################################  1105  ################################\n",
      "Loss:  0.5084236860275269\n",
      "################################  1106  ################################\n",
      "Loss:  0.508340060710907\n",
      "################################  1107  ################################\n",
      "Loss:  0.50825434923172\n",
      "################################  1108  ################################\n",
      "Loss:  0.508173406124115\n",
      "################################  1109  ################################\n",
      "Loss:  0.5080968737602234\n",
      "################################  1110  ################################\n",
      "Loss:  0.5080218315124512\n",
      "################################  1111  ################################\n",
      "Loss:  0.5079476237297058\n",
      "################################  1112  ################################\n",
      "Loss:  0.5078696012496948\n",
      "################################  1113  ################################\n",
      "Loss:  0.5077897906303406\n",
      "################################  1114  ################################\n",
      "Loss:  0.5077068209648132\n",
      "################################  1115  ################################\n",
      "Loss:  0.5076202750205994\n",
      "################################  1116  ################################\n",
      "Loss:  0.5075304508209229\n",
      "################################  1117  ################################\n",
      "Loss:  0.5074349045753479\n",
      "################################  1118  ################################\n",
      "Loss:  0.5073347687721252\n",
      "################################  1119  ################################\n",
      "Loss:  0.5072328448295593\n",
      "################################  1120  ################################\n",
      "Loss:  0.5071210861206055\n",
      "################################  1121  ################################\n",
      "Loss:  0.5070195198059082\n",
      "################################  1122  ################################\n",
      "Loss:  0.5069041848182678\n",
      "################################  1123  ################################\n",
      "Loss:  0.5067934989929199\n",
      "################################  1124  ################################\n",
      "Loss:  0.5066747069358826\n",
      "################################  1125  ################################\n",
      "Loss:  0.5065436959266663\n",
      "################################  1126  ################################\n",
      "Loss:  0.5063984990119934\n",
      "################################  1127  ################################\n",
      "Loss:  0.5062733888626099\n",
      "################################  1128  ################################\n",
      "Loss:  0.5061520338058472\n",
      "################################  1129  ################################\n",
      "Loss:  0.5060187578201294\n",
      "################################  1130  ################################\n",
      "Loss:  0.505893349647522\n",
      "################################  1131  ################################\n",
      "Loss:  0.505779504776001\n",
      "################################  1132  ################################\n",
      "Loss:  0.5056567192077637\n",
      "################################  1133  ################################\n",
      "Loss:  0.5055525898933411\n",
      "################################  1134  ################################\n",
      "Loss:  0.505458414554596\n",
      "################################  1135  ################################\n",
      "Loss:  0.5053685903549194\n",
      "################################  1136  ################################\n",
      "Loss:  0.505281925201416\n",
      "################################  1137  ################################\n",
      "Loss:  0.5051988959312439\n",
      "################################  1138  ################################\n",
      "Loss:  0.5051150321960449\n",
      "################################  1139  ################################\n",
      "Loss:  0.5050332546234131\n",
      "################################  1140  ################################\n",
      "Loss:  0.5049444437026978\n",
      "################################  1141  ################################\n",
      "Loss:  0.5048556327819824\n",
      "################################  1142  ################################\n",
      "Loss:  0.5047393441200256\n",
      "################################  1143  ################################\n",
      "Loss:  0.5046524405479431\n",
      "################################  1144  ################################\n",
      "Loss:  0.5045477747917175\n",
      "################################  1145  ################################\n",
      "Loss:  0.5044256448745728\n",
      "################################  1146  ################################\n",
      "Loss:  0.5042914152145386\n",
      "################################  1147  ################################\n",
      "Loss:  0.504160463809967\n",
      "################################  1148  ################################\n",
      "Loss:  0.5040162801742554\n",
      "################################  1149  ################################\n",
      "Loss:  0.5038862228393555\n",
      "################################  1150  ################################\n",
      "Loss:  0.5037478804588318\n",
      "################################  1151  ################################\n",
      "Loss:  0.5036006569862366\n",
      "################################  1152  ################################\n",
      "Loss:  0.5034552216529846\n",
      "################################  1153  ################################\n",
      "Loss:  0.5033125877380371\n",
      "################################  1154  ################################\n",
      "Loss:  0.5031718611717224\n",
      "################################  1155  ################################\n",
      "Loss:  0.5030397176742554\n",
      "################################  1156  ################################\n",
      "Loss:  0.5029088258743286\n",
      "################################  1157  ################################\n",
      "Loss:  0.5027880668640137\n",
      "################################  1158  ################################\n",
      "Loss:  0.502677321434021\n",
      "################################  1159  ################################\n",
      "Loss:  0.5025784969329834\n",
      "################################  1160  ################################\n",
      "Loss:  0.5024857521057129\n",
      "################################  1161  ################################\n",
      "Loss:  0.5023953914642334\n",
      "################################  1162  ################################\n",
      "Loss:  0.5023027658462524\n",
      "################################  1163  ################################\n",
      "Loss:  0.502216637134552\n",
      "################################  1164  ################################\n",
      "Loss:  0.5021411180496216\n",
      "################################  1165  ################################\n",
      "Loss:  0.5020684599876404\n",
      "################################  1166  ################################\n",
      "Loss:  0.5019984841346741\n",
      "################################  1167  ################################\n",
      "Loss:  0.5019254684448242\n",
      "################################  1168  ################################\n",
      "Loss:  0.5018426179885864\n",
      "################################  1169  ################################\n",
      "Loss:  0.5017642974853516\n",
      "################################  1170  ################################\n",
      "Loss:  0.501683235168457\n",
      "################################  1171  ################################\n",
      "Loss:  0.5015971660614014\n",
      "################################  1172  ################################\n",
      "Loss:  0.5015013813972473\n",
      "################################  1173  ################################\n",
      "Loss:  0.5014005303382874\n",
      "################################  1174  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5012817978858948\n",
      "################################  1175  ################################\n",
      "Loss:  0.5011630058288574\n",
      "################################  1176  ################################\n",
      "Loss:  0.5010312795639038\n",
      "################################  1177  ################################\n",
      "Loss:  0.5008828639984131\n",
      "################################  1178  ################################\n",
      "Loss:  0.5007226467132568\n",
      "################################  1179  ################################\n",
      "Loss:  0.5005661845207214\n",
      "################################  1180  ################################\n",
      "Loss:  0.5003926753997803\n",
      "################################  1181  ################################\n",
      "Loss:  0.500232994556427\n",
      "################################  1182  ################################\n",
      "Loss:  0.5000497102737427\n",
      "################################  1183  ################################\n",
      "Loss:  0.4998798966407776\n",
      "################################  1184  ################################\n",
      "Loss:  0.4997014105319977\n",
      "################################  1185  ################################\n",
      "Loss:  0.4995337724685669\n",
      "################################  1186  ################################\n",
      "Loss:  0.49936774373054504\n",
      "################################  1187  ################################\n",
      "Loss:  0.49920418858528137\n",
      "################################  1188  ################################\n",
      "Loss:  0.4990382790565491\n",
      "################################  1189  ################################\n",
      "Loss:  0.49888110160827637\n",
      "################################  1190  ################################\n",
      "Loss:  0.4987250566482544\n",
      "################################  1191  ################################\n",
      "Loss:  0.49857866764068604\n",
      "################################  1192  ################################\n",
      "Loss:  0.49842125177383423\n",
      "################################  1193  ################################\n",
      "Loss:  0.49830105900764465\n",
      "################################  1194  ################################\n",
      "Loss:  0.4981876313686371\n",
      "################################  1195  ################################\n",
      "Loss:  0.4980689287185669\n",
      "################################  1196  ################################\n",
      "Loss:  0.4979647397994995\n",
      "################################  1197  ################################\n",
      "Loss:  0.49786895513534546\n",
      "################################  1198  ################################\n",
      "Loss:  0.49777328968048096\n",
      "################################  1199  ################################\n",
      "Loss:  0.49767524003982544\n",
      "################################  1200  ################################\n",
      "Loss:  0.49758267402648926\n",
      "################################  1201  ################################\n",
      "Loss:  0.49748852849006653\n",
      "################################  1202  ################################\n",
      "Loss:  0.49739742279052734\n",
      "################################  1203  ################################\n",
      "Loss:  0.4973054528236389\n",
      "################################  1204  ################################\n",
      "Loss:  0.4972158670425415\n",
      "################################  1205  ################################\n",
      "Loss:  0.49712324142456055\n",
      "################################  1206  ################################\n",
      "Loss:  0.4970247447490692\n",
      "################################  1207  ################################\n",
      "Loss:  0.49693137407302856\n",
      "################################  1208  ################################\n",
      "Loss:  0.49684128165245056\n",
      "################################  1209  ################################\n",
      "Loss:  0.4967646300792694\n",
      "################################  1210  ################################\n",
      "Loss:  0.4966849386692047\n",
      "################################  1211  ################################\n",
      "Loss:  0.496603399515152\n",
      "################################  1212  ################################\n",
      "Loss:  0.49651476740837097\n",
      "################################  1213  ################################\n",
      "Loss:  0.49641871452331543\n",
      "################################  1214  ################################\n",
      "Loss:  0.49631720781326294\n",
      "################################  1215  ################################\n",
      "Loss:  0.49620768427848816\n",
      "################################  1216  ################################\n",
      "Loss:  0.49609413743019104\n",
      "################################  1217  ################################\n",
      "Loss:  0.49596837162971497\n",
      "################################  1218  ################################\n",
      "Loss:  0.4958600401878357\n",
      "################################  1219  ################################\n",
      "Loss:  0.4957410991191864\n",
      "################################  1220  ################################\n",
      "Loss:  0.49560612440109253\n",
      "################################  1221  ################################\n",
      "Loss:  0.49546992778778076\n",
      "################################  1222  ################################\n",
      "Loss:  0.4953254163265228\n",
      "################################  1223  ################################\n",
      "Loss:  0.4951823651790619\n",
      "################################  1224  ################################\n",
      "Loss:  0.4950404465198517\n",
      "################################  1225  ################################\n",
      "Loss:  0.49490588903427124\n",
      "################################  1226  ################################\n",
      "Loss:  0.4947735071182251\n",
      "################################  1227  ################################\n",
      "Loss:  0.4946410059928894\n",
      "################################  1228  ################################\n",
      "Loss:  0.4945087134838104\n",
      "################################  1229  ################################\n",
      "Loss:  0.49437540769577026\n",
      "################################  1230  ################################\n",
      "Loss:  0.4942413568496704\n",
      "################################  1231  ################################\n",
      "Loss:  0.49410486221313477\n",
      "################################  1232  ################################\n",
      "Loss:  0.493961364030838\n",
      "################################  1233  ################################\n",
      "Loss:  0.49383091926574707\n",
      "################################  1234  ################################\n",
      "Loss:  0.4936898648738861\n",
      "################################  1235  ################################\n",
      "Loss:  0.49357253313064575\n",
      "################################  1236  ################################\n",
      "Loss:  0.4934294521808624\n",
      "################################  1237  ################################\n",
      "Loss:  0.4932737946510315\n",
      "################################  1238  ################################\n",
      "Loss:  0.49310845136642456\n",
      "################################  1239  ################################\n",
      "Loss:  0.4929100275039673\n",
      "################################  1240  ################################\n",
      "Loss:  0.4927217662334442\n",
      "################################  1241  ################################\n",
      "Loss:  0.49254289269447327\n",
      "################################  1242  ################################\n",
      "Loss:  0.49237510561943054\n",
      "################################  1243  ################################\n",
      "Loss:  0.49221354722976685\n",
      "################################  1244  ################################\n",
      "Loss:  0.49204692244529724\n",
      "################################  1245  ################################\n",
      "Loss:  0.49189504981040955\n",
      "################################  1246  ################################\n",
      "Loss:  0.4917648732662201\n",
      "################################  1247  ################################\n",
      "Loss:  0.4916408061981201\n",
      "################################  1248  ################################\n",
      "Loss:  0.49152618646621704\n",
      "################################  1249  ################################\n",
      "Loss:  0.49142986536026\n",
      "################################  1250  ################################\n",
      "Loss:  0.49134138226509094\n",
      "################################  1251  ################################\n",
      "Loss:  0.49125781655311584\n",
      "################################  1252  ################################\n",
      "Loss:  0.4911787509918213\n",
      "################################  1253  ################################\n",
      "Loss:  0.4910987615585327\n",
      "################################  1254  ################################\n",
      "Loss:  0.49102818965911865\n",
      "################################  1255  ################################\n",
      "Loss:  0.4909617602825165\n",
      "################################  1256  ################################\n",
      "Loss:  0.49089673161506653\n",
      "################################  1257  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.49083200097084045\n",
      "################################  1258  ################################\n",
      "Loss:  0.4907675087451935\n",
      "################################  1259  ################################\n",
      "Loss:  0.4907021224498749\n",
      "################################  1260  ################################\n",
      "Loss:  0.4906350076198578\n",
      "################################  1261  ################################\n",
      "Loss:  0.4905652105808258\n",
      "################################  1262  ################################\n",
      "Loss:  0.4904891550540924\n",
      "################################  1263  ################################\n",
      "Loss:  0.49041056632995605\n",
      "################################  1264  ################################\n",
      "Loss:  0.4903339147567749\n",
      "################################  1265  ################################\n",
      "Loss:  0.49025097489356995\n",
      "################################  1266  ################################\n",
      "Loss:  0.49016663432121277\n",
      "################################  1267  ################################\n",
      "Loss:  0.4900820851325989\n",
      "################################  1268  ################################\n",
      "Loss:  0.4899953603744507\n",
      "################################  1269  ################################\n",
      "Loss:  0.4899054765701294\n",
      "################################  1270  ################################\n",
      "Loss:  0.4898175597190857\n",
      "################################  1271  ################################\n",
      "Loss:  0.4897235631942749\n",
      "################################  1272  ################################\n",
      "Loss:  0.48962879180908203\n",
      "################################  1273  ################################\n",
      "Loss:  0.4895254969596863\n",
      "################################  1274  ################################\n",
      "Loss:  0.4894236624240875\n",
      "################################  1275  ################################\n",
      "Loss:  0.4893275797367096\n",
      "################################  1276  ################################\n",
      "Loss:  0.48922836780548096\n",
      "################################  1277  ################################\n",
      "Loss:  0.4891315996646881\n",
      "################################  1278  ################################\n",
      "Loss:  0.4890397787094116\n",
      "################################  1279  ################################\n",
      "Loss:  0.4889531433582306\n",
      "################################  1280  ################################\n",
      "Loss:  0.4888695478439331\n",
      "################################  1281  ################################\n",
      "Loss:  0.4887898862361908\n",
      "################################  1282  ################################\n",
      "Loss:  0.4887124300003052\n",
      "################################  1283  ################################\n",
      "Loss:  0.4886412024497986\n",
      "################################  1284  ################################\n",
      "Loss:  0.48857012391090393\n",
      "################################  1285  ################################\n",
      "Loss:  0.4885074198246002\n",
      "################################  1286  ################################\n",
      "Loss:  0.4884500801563263\n",
      "################################  1287  ################################\n",
      "Loss:  0.488394170999527\n",
      "################################  1288  ################################\n",
      "Loss:  0.4883427619934082\n",
      "################################  1289  ################################\n",
      "Loss:  0.48829442262649536\n",
      "################################  1290  ################################\n",
      "Loss:  0.4882447123527527\n",
      "################################  1291  ################################\n",
      "Loss:  0.4881892800331116\n",
      "################################  1292  ################################\n",
      "Loss:  0.4881381690502167\n",
      "################################  1293  ################################\n",
      "Loss:  0.4880937933921814\n",
      "################################  1294  ################################\n",
      "Loss:  0.4880473017692566\n",
      "################################  1295  ################################\n",
      "Loss:  0.487998902797699\n",
      "################################  1296  ################################\n",
      "Loss:  0.4879535734653473\n",
      "################################  1297  ################################\n",
      "Loss:  0.48789864778518677\n",
      "################################  1298  ################################\n",
      "Loss:  0.4878532588481903\n",
      "################################  1299  ################################\n",
      "Loss:  0.48780784010887146\n",
      "################################  1300  ################################\n",
      "Loss:  0.4877545237541199\n",
      "################################  1301  ################################\n",
      "Loss:  0.4876871705055237\n",
      "################################  1302  ################################\n",
      "Loss:  0.4876392185688019\n",
      "################################  1303  ################################\n",
      "Loss:  0.48758649826049805\n",
      "################################  1304  ################################\n",
      "Loss:  0.48753076791763306\n",
      "################################  1305  ################################\n",
      "Loss:  0.48747578263282776\n",
      "################################  1306  ################################\n",
      "Loss:  0.48742109537124634\n",
      "################################  1307  ################################\n",
      "Loss:  0.4873654544353485\n",
      "################################  1308  ################################\n",
      "Loss:  0.4873076379299164\n",
      "################################  1309  ################################\n",
      "Loss:  0.48725074529647827\n",
      "################################  1310  ################################\n",
      "Loss:  0.4871966242790222\n",
      "################################  1311  ################################\n",
      "Loss:  0.48714447021484375\n",
      "################################  1312  ################################\n",
      "Loss:  0.4870860278606415\n",
      "################################  1313  ################################\n",
      "Loss:  0.4870327115058899\n",
      "################################  1314  ################################\n",
      "Loss:  0.48698335886001587\n",
      "################################  1315  ################################\n",
      "Loss:  0.48692765831947327\n",
      "################################  1316  ################################\n",
      "Loss:  0.4868735373020172\n",
      "################################  1317  ################################\n",
      "Loss:  0.48680585622787476\n",
      "################################  1318  ################################\n",
      "Loss:  0.48673874139785767\n",
      "################################  1319  ################################\n",
      "Loss:  0.486649751663208\n",
      "################################  1320  ################################\n",
      "Loss:  0.4865725338459015\n",
      "################################  1321  ################################\n",
      "Loss:  0.48648276925086975\n",
      "################################  1322  ################################\n",
      "Loss:  0.48636674880981445\n",
      "################################  1323  ################################\n",
      "Loss:  0.48623594641685486\n",
      "################################  1324  ################################\n",
      "Loss:  0.4861133098602295\n",
      "################################  1325  ################################\n",
      "Loss:  0.4859679639339447\n",
      "################################  1326  ################################\n",
      "Loss:  0.48584070801734924\n",
      "################################  1327  ################################\n",
      "Loss:  0.4857077896595001\n",
      "################################  1328  ################################\n",
      "Loss:  0.4855833053588867\n",
      "################################  1329  ################################\n",
      "Loss:  0.4854501783847809\n",
      "################################  1330  ################################\n",
      "Loss:  0.4853123724460602\n",
      "################################  1331  ################################\n",
      "Loss:  0.48517128825187683\n",
      "################################  1332  ################################\n",
      "Loss:  0.485025554895401\n",
      "################################  1333  ################################\n",
      "Loss:  0.48489269614219666\n",
      "################################  1334  ################################\n",
      "Loss:  0.484760046005249\n",
      "################################  1335  ################################\n",
      "Loss:  0.48464131355285645\n",
      "################################  1336  ################################\n",
      "Loss:  0.4845200777053833\n",
      "################################  1337  ################################\n",
      "Loss:  0.48440465331077576\n",
      "################################  1338  ################################\n",
      "Loss:  0.4842865765094757\n",
      "################################  1339  ################################\n",
      "Loss:  0.4841747283935547\n",
      "################################  1340  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.48406216502189636\n",
      "################################  1341  ################################\n",
      "Loss:  0.4839443266391754\n",
      "################################  1342  ################################\n",
      "Loss:  0.48383909463882446\n",
      "################################  1343  ################################\n",
      "Loss:  0.4837336540222168\n",
      "################################  1344  ################################\n",
      "Loss:  0.48362594842910767\n",
      "################################  1345  ################################\n",
      "Loss:  0.4835166037082672\n",
      "################################  1346  ################################\n",
      "Loss:  0.48339882493019104\n",
      "################################  1347  ################################\n",
      "Loss:  0.4832743704319\n",
      "################################  1348  ################################\n",
      "Loss:  0.4831559360027313\n",
      "################################  1349  ################################\n",
      "Loss:  0.4830363690853119\n",
      "################################  1350  ################################\n",
      "Loss:  0.482914537191391\n",
      "################################  1351  ################################\n",
      "Loss:  0.48277831077575684\n",
      "################################  1352  ################################\n",
      "Loss:  0.48263561725616455\n",
      "################################  1353  ################################\n",
      "Loss:  0.4824802279472351\n",
      "################################  1354  ################################\n",
      "Loss:  0.4823506772518158\n",
      "################################  1355  ################################\n",
      "Loss:  0.4822070300579071\n",
      "################################  1356  ################################\n",
      "Loss:  0.4820576012134552\n",
      "################################  1357  ################################\n",
      "Loss:  0.48188596963882446\n",
      "################################  1358  ################################\n",
      "Loss:  0.4817183315753937\n",
      "################################  1359  ################################\n",
      "Loss:  0.48155006766319275\n",
      "################################  1360  ################################\n",
      "Loss:  0.4813827574253082\n",
      "################################  1361  ################################\n",
      "Loss:  0.48121729493141174\n",
      "################################  1362  ################################\n",
      "Loss:  0.48105528950691223\n",
      "################################  1363  ################################\n",
      "Loss:  0.48088935017585754\n",
      "################################  1364  ################################\n",
      "Loss:  0.48073235154151917\n",
      "################################  1365  ################################\n",
      "Loss:  0.48056453466415405\n",
      "################################  1366  ################################\n",
      "Loss:  0.4804275631904602\n",
      "################################  1367  ################################\n",
      "Loss:  0.4802924692630768\n",
      "################################  1368  ################################\n",
      "Loss:  0.48016709089279175\n",
      "################################  1369  ################################\n",
      "Loss:  0.48004642128944397\n",
      "################################  1370  ################################\n",
      "Loss:  0.47993555665016174\n",
      "################################  1371  ################################\n",
      "Loss:  0.4798261225223541\n",
      "################################  1372  ################################\n",
      "Loss:  0.4797149896621704\n",
      "################################  1373  ################################\n",
      "Loss:  0.4796113073825836\n",
      "################################  1374  ################################\n",
      "Loss:  0.4795130491256714\n",
      "################################  1375  ################################\n",
      "Loss:  0.47941911220550537\n",
      "################################  1376  ################################\n",
      "Loss:  0.47932231426239014\n",
      "################################  1377  ################################\n",
      "Loss:  0.4792422652244568\n",
      "################################  1378  ################################\n",
      "Loss:  0.47916486859321594\n",
      "################################  1379  ################################\n",
      "Loss:  0.4790899157524109\n",
      "################################  1380  ################################\n",
      "Loss:  0.4790164828300476\n",
      "################################  1381  ################################\n",
      "Loss:  0.47895509004592896\n",
      "################################  1382  ################################\n",
      "Loss:  0.47889888286590576\n",
      "################################  1383  ################################\n",
      "Loss:  0.4788426458835602\n",
      "################################  1384  ################################\n",
      "Loss:  0.4787940979003906\n",
      "################################  1385  ################################\n",
      "Loss:  0.4787510633468628\n",
      "################################  1386  ################################\n",
      "Loss:  0.4787054657936096\n",
      "################################  1387  ################################\n",
      "Loss:  0.4786665439605713\n",
      "################################  1388  ################################\n",
      "Loss:  0.4786261320114136\n",
      "################################  1389  ################################\n",
      "Loss:  0.47859111428260803\n",
      "################################  1390  ################################\n",
      "Loss:  0.47855624556541443\n",
      "################################  1391  ################################\n",
      "Loss:  0.4785236120223999\n",
      "################################  1392  ################################\n",
      "Loss:  0.47849103808403015\n",
      "################################  1393  ################################\n",
      "Loss:  0.4784574806690216\n",
      "################################  1394  ################################\n",
      "Loss:  0.47842442989349365\n",
      "################################  1395  ################################\n",
      "Loss:  0.47839316725730896\n",
      "################################  1396  ################################\n",
      "Loss:  0.47836440801620483\n",
      "################################  1397  ################################\n",
      "Loss:  0.47833433747291565\n",
      "################################  1398  ################################\n",
      "Loss:  0.4783068597316742\n",
      "################################  1399  ################################\n",
      "Loss:  0.4782765209674835\n",
      "################################  1400  ################################\n",
      "Loss:  0.4782521724700928\n",
      "################################  1401  ################################\n",
      "Loss:  0.47822874784469604\n",
      "################################  1402  ################################\n",
      "Loss:  0.4781990051269531\n",
      "################################  1403  ################################\n",
      "Loss:  0.4781677722930908\n",
      "################################  1404  ################################\n",
      "Loss:  0.4781404733657837\n",
      "################################  1405  ################################\n",
      "Loss:  0.47811159491539\n",
      "################################  1406  ################################\n",
      "Loss:  0.4780811369419098\n",
      "################################  1407  ################################\n",
      "Loss:  0.4780504107475281\n",
      "################################  1408  ################################\n",
      "Loss:  0.4780186712741852\n",
      "################################  1409  ################################\n",
      "Loss:  0.4779800474643707\n",
      "################################  1410  ################################\n",
      "Loss:  0.4779423773288727\n",
      "################################  1411  ################################\n",
      "Loss:  0.47789937257766724\n",
      "################################  1412  ################################\n",
      "Loss:  0.47786077857017517\n",
      "################################  1413  ################################\n",
      "Loss:  0.4778147041797638\n",
      "################################  1414  ################################\n",
      "Loss:  0.47775998711586\n",
      "################################  1415  ################################\n",
      "Loss:  0.4777013957500458\n",
      "################################  1416  ################################\n",
      "Loss:  0.4776443541049957\n",
      "################################  1417  ################################\n",
      "Loss:  0.47758734226226807\n",
      "################################  1418  ################################\n",
      "Loss:  0.47752052545547485\n",
      "################################  1419  ################################\n",
      "Loss:  0.47745198011398315\n",
      "################################  1420  ################################\n",
      "Loss:  0.47737643122673035\n",
      "################################  1421  ################################\n",
      "Loss:  0.47730568051338196\n",
      "################################  1422  ################################\n",
      "Loss:  0.47723957896232605\n",
      "################################  1423  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.47716477513313293\n",
      "################################  1424  ################################\n",
      "Loss:  0.47709426283836365\n",
      "################################  1425  ################################\n",
      "Loss:  0.4770243465900421\n",
      "################################  1426  ################################\n",
      "Loss:  0.4769568145275116\n",
      "################################  1427  ################################\n",
      "Loss:  0.47688546776771545\n",
      "################################  1428  ################################\n",
      "Loss:  0.47680896520614624\n",
      "################################  1429  ################################\n",
      "Loss:  0.4767457842826843\n",
      "################################  1430  ################################\n",
      "Loss:  0.4766848087310791\n",
      "################################  1431  ################################\n",
      "Loss:  0.47662392258644104\n",
      "################################  1432  ################################\n",
      "Loss:  0.4765687584877014\n",
      "################################  1433  ################################\n",
      "Loss:  0.47651511430740356\n",
      "################################  1434  ################################\n",
      "Loss:  0.47645363211631775\n",
      "################################  1435  ################################\n",
      "Loss:  0.4763987362384796\n",
      "################################  1436  ################################\n",
      "Loss:  0.4763437509536743\n",
      "################################  1437  ################################\n",
      "Loss:  0.4762900173664093\n",
      "################################  1438  ################################\n",
      "Loss:  0.47623950242996216\n",
      "################################  1439  ################################\n",
      "Loss:  0.4761900007724762\n",
      "################################  1440  ################################\n",
      "Loss:  0.4761393368244171\n",
      "################################  1441  ################################\n",
      "Loss:  0.4760899841785431\n",
      "################################  1442  ################################\n",
      "Loss:  0.47604045271873474\n",
      "################################  1443  ################################\n",
      "Loss:  0.47599074244499207\n",
      "################################  1444  ################################\n",
      "Loss:  0.4759367108345032\n",
      "################################  1445  ################################\n",
      "Loss:  0.4758860766887665\n",
      "################################  1446  ################################\n",
      "Loss:  0.4758317470550537\n",
      "################################  1447  ################################\n",
      "Loss:  0.4757751226425171\n",
      "################################  1448  ################################\n",
      "Loss:  0.47571486234664917\n",
      "################################  1449  ################################\n",
      "Loss:  0.4756501615047455\n",
      "################################  1450  ################################\n",
      "Loss:  0.4755783677101135\n",
      "################################  1451  ################################\n",
      "Loss:  0.4755009710788727\n",
      "################################  1452  ################################\n",
      "Loss:  0.47541406750679016\n",
      "################################  1453  ################################\n",
      "Loss:  0.47532105445861816\n",
      "################################  1454  ################################\n",
      "Loss:  0.475223183631897\n",
      "################################  1455  ################################\n",
      "Loss:  0.47511953115463257\n",
      "################################  1456  ################################\n",
      "Loss:  0.4750136137008667\n",
      "################################  1457  ################################\n",
      "Loss:  0.4749022126197815\n",
      "################################  1458  ################################\n",
      "Loss:  0.4747914969921112\n",
      "################################  1459  ################################\n",
      "Loss:  0.47467169165611267\n",
      "################################  1460  ################################\n",
      "Loss:  0.47455069422721863\n",
      "################################  1461  ################################\n",
      "Loss:  0.4744124710559845\n",
      "################################  1462  ################################\n",
      "Loss:  0.4742538034915924\n",
      "################################  1463  ################################\n",
      "Loss:  0.47408923506736755\n",
      "################################  1464  ################################\n",
      "Loss:  0.4739029109477997\n",
      "################################  1465  ################################\n",
      "Loss:  0.4737217128276825\n",
      "################################  1466  ################################\n",
      "Loss:  0.4735357165336609\n",
      "################################  1467  ################################\n",
      "Loss:  0.47334587574005127\n",
      "################################  1468  ################################\n",
      "Loss:  0.4731343984603882\n",
      "################################  1469  ################################\n",
      "Loss:  0.47293621301651\n",
      "################################  1470  ################################\n",
      "Loss:  0.4727364480495453\n",
      "################################  1471  ################################\n",
      "Loss:  0.47253453731536865\n",
      "################################  1472  ################################\n",
      "Loss:  0.4723302721977234\n",
      "################################  1473  ################################\n",
      "Loss:  0.47213077545166016\n",
      "################################  1474  ################################\n",
      "Loss:  0.47193649411201477\n",
      "################################  1475  ################################\n",
      "Loss:  0.4717472791671753\n",
      "################################  1476  ################################\n",
      "Loss:  0.47159475088119507\n",
      "################################  1477  ################################\n",
      "Loss:  0.47144100069999695\n",
      "################################  1478  ################################\n",
      "Loss:  0.4712935984134674\n",
      "################################  1479  ################################\n",
      "Loss:  0.47115492820739746\n",
      "################################  1480  ################################\n",
      "Loss:  0.47102615237236023\n",
      "################################  1481  ################################\n",
      "Loss:  0.47091373801231384\n",
      "################################  1482  ################################\n",
      "Loss:  0.4708002507686615\n",
      "################################  1483  ################################\n",
      "Loss:  0.4707021117210388\n",
      "################################  1484  ################################\n",
      "Loss:  0.4706054627895355\n",
      "################################  1485  ################################\n",
      "Loss:  0.4705163836479187\n",
      "################################  1486  ################################\n",
      "Loss:  0.47043490409851074\n",
      "################################  1487  ################################\n",
      "Loss:  0.4703480005264282\n",
      "################################  1488  ################################\n",
      "Loss:  0.4702751636505127\n",
      "################################  1489  ################################\n",
      "Loss:  0.4701903760433197\n",
      "################################  1490  ################################\n",
      "Loss:  0.47009801864624023\n",
      "################################  1491  ################################\n",
      "Loss:  0.470002681016922\n",
      "################################  1492  ################################\n",
      "Loss:  0.4698981046676636\n",
      "################################  1493  ################################\n",
      "Loss:  0.4698096513748169\n",
      "################################  1494  ################################\n",
      "Loss:  0.4697173833847046\n",
      "################################  1495  ################################\n",
      "Loss:  0.4696248769760132\n",
      "################################  1496  ################################\n",
      "Loss:  0.46953320503234863\n",
      "################################  1497  ################################\n",
      "Loss:  0.4694359004497528\n",
      "################################  1498  ################################\n",
      "Loss:  0.4693370461463928\n",
      "################################  1499  ################################\n",
      "Loss:  0.4692375957965851\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m history \u001b[38;5;241m=\u001b[39m fit(my_network, training_set, interior, n_epochs, optimizer_, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[1;32m      5\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m total_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m \u001b[43mstart_time\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start_time' is not defined"
     ]
    }
   ],
   "source": [
    "# n_epochs = 10000\n",
    "# start_time = time.time()\n",
    "n_epochs = 1500\n",
    "history = fit(my_network, training_set, interior, n_epochs, optimizer_, p=2, verbose=True )\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# print(\"Training time: {:.2f} seconds\".format(total_time))\n",
    "\n",
    "# with open('p_ic.pkl', 'wb') as f:\n",
    "#     pickle.dump(history, f)\n",
    "\n",
    "# f.close()\n",
    "\n",
    "# model_state_dict = my_network.state_dict()\n",
    "\n",
    "# # Save the model state dictionary to a file\n",
    "# torch.save(model_state_dict, 'p_ic.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the history from the pickle file\n",
    "# with open('p_ic_fixed_param.pkl', 'rb') as f:\n",
    "#     history = pickle.load(f)\n",
    "\n",
    "# # Load the model architecture\n",
    "# my_network = your_model_module.YourModelClass()  # Instantiate your model class\n",
    "\n",
    "# # Load the saved model state dictionary\n",
    "# model_state_dict = torch.load('p_ic_fixed_param.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# # Load the model weights\n",
    "# my_network.load_state_dict(model_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a0f31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Error Test:  200.79827308654785 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test = torch.linspace(0, 8*pi, 10000).reshape(-1,1)\n",
    "t_test = torch.ones((10000,1))\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution(x_test, t_test).reshape(-1,1)\n",
    "\n",
    "my_network = my_network.cpu()\n",
    "u_test_pred = my_network(test).reshape(-1,1)\n",
    "\n",
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((u_test_pred - u_test)**2)/torch.mean(u_test**2)\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")\n",
    "\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.plot(x_test, u_test, label=\"Ground Truth\",lw=2)\n",
    "plt.plot(x_test, u_test_pred.detach(), label=\"Network Prediction\",lw=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"u\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "046255e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('input_layer.weight',\n",
       "              tensor([[-4.1217e-02,  3.9779e-01],\n",
       "                      [-2.3341e-02, -3.8373e-01],\n",
       "                      [ 4.8723e-02, -3.4235e-01],\n",
       "                      [-3.5261e-01, -2.9153e-01],\n",
       "                      [-3.9347e-01,  3.3110e-01],\n",
       "                      [ 9.3517e-03,  3.3976e-01],\n",
       "                      [ 2.7382e-01,  2.0674e-01],\n",
       "                      [ 6.1502e-01, -1.5997e-01],\n",
       "                      [-5.4564e-02,  2.5594e-01],\n",
       "                      [ 4.2477e-01,  1.1277e-01],\n",
       "                      [ 3.7104e-02,  4.4382e-02],\n",
       "                      [ 1.8099e-02,  1.8369e-01],\n",
       "                      [ 2.7865e-02,  1.2604e-02],\n",
       "                      [ 3.8196e-01,  8.4665e-02],\n",
       "                      [-6.0569e-02,  3.5445e-01],\n",
       "                      [-6.0163e-01,  1.3410e-01],\n",
       "                      [ 7.5129e-01, -5.2868e-02],\n",
       "                      [ 2.8001e-01, -1.3908e-01],\n",
       "                      [-2.7274e-01,  1.8141e-01],\n",
       "                      [-1.9149e-01, -3.5476e-01],\n",
       "                      [ 4.6174e-01,  1.2505e-01],\n",
       "                      [-2.4511e-01,  4.2722e-01],\n",
       "                      [-4.8669e-01,  1.1729e-02],\n",
       "                      [-1.6723e-01, -1.9353e-01],\n",
       "                      [-4.1253e-01, -2.4686e-02],\n",
       "                      [ 3.9500e-02, -3.5713e-01],\n",
       "                      [-4.6559e-02,  2.7641e-01],\n",
       "                      [-6.3170e-01,  2.1280e-01],\n",
       "                      [-7.2085e-02, -1.4225e-01],\n",
       "                      [-3.1086e-02,  2.2689e-01],\n",
       "                      [-6.8181e-03, -4.5411e-01],\n",
       "                      [ 2.4233e-01, -2.5821e-02],\n",
       "                      [-2.8100e-03,  1.2145e-01],\n",
       "                      [ 4.1894e-02,  3.8224e-02],\n",
       "                      [ 4.9067e-02, -2.0873e-01],\n",
       "                      [-4.8383e-02, -1.4559e-01],\n",
       "                      [ 3.5225e-02, -2.0981e-01],\n",
       "                      [-5.8618e-01,  6.5794e-03],\n",
       "                      [-4.6643e-02, -2.0037e-01],\n",
       "                      [ 5.6809e-01, -1.7785e-01],\n",
       "                      [ 6.6562e-01, -1.3489e-01],\n",
       "                      [ 4.2863e-03,  4.1933e-01],\n",
       "                      [-2.1951e-01, -2.1738e-01],\n",
       "                      [-3.9811e-01,  3.4010e-02],\n",
       "                      [-3.9777e-02,  4.6239e-03],\n",
       "                      [-1.9568e-01,  1.7738e-01],\n",
       "                      [-4.0585e-01, -1.6228e-01],\n",
       "                      [-2.8730e-01,  4.6067e-02],\n",
       "                      [-2.5726e-01, -1.4369e-02],\n",
       "                      [ 3.5788e-02,  2.3394e-01],\n",
       "                      [-3.1758e-02, -2.0211e-01],\n",
       "                      [-3.8039e-02,  3.5584e-01],\n",
       "                      [ 1.4248e-01, -4.9290e-01],\n",
       "                      [ 4.4903e-02, -1.4727e-01],\n",
       "                      [ 2.7225e-01, -3.0961e-02],\n",
       "                      [-3.1575e-02,  1.3025e-01],\n",
       "                      [-6.9980e-02, -6.2239e-02],\n",
       "                      [-2.5311e-02, -1.4930e-01],\n",
       "                      [-6.0606e-02, -2.4102e-01],\n",
       "                      [ 4.1108e-01,  1.9573e-01],\n",
       "                      [-4.6147e-02,  2.2244e-01],\n",
       "                      [ 3.9334e-02,  1.0663e-01],\n",
       "                      [ 1.1555e-02, -1.7851e-01],\n",
       "                      [ 3.3834e-02, -2.7209e-01],\n",
       "                      [ 2.6403e-02,  1.0515e-02],\n",
       "                      [ 1.1704e-02, -4.1784e-01],\n",
       "                      [ 2.4131e-02,  6.9545e-01],\n",
       "                      [-2.0656e-01, -2.2821e-01],\n",
       "                      [-2.1532e-02, -3.3835e-01],\n",
       "                      [ 3.0560e-02,  8.5622e-02],\n",
       "                      [-1.2992e-01,  2.8727e-01],\n",
       "                      [-2.1642e-02, -2.0754e-01],\n",
       "                      [ 8.0928e-03,  2.8404e-01],\n",
       "                      [ 3.9001e-01,  2.8057e-01],\n",
       "                      [ 1.9866e-01,  1.6347e-01],\n",
       "                      [ 5.3986e-02, -4.1433e-01],\n",
       "                      [ 3.7637e-01,  3.2144e-01],\n",
       "                      [-4.7030e-01, -1.7988e-01],\n",
       "                      [ 1.5604e-01,  1.5744e-01],\n",
       "                      [-4.0021e-02, -4.1384e-01],\n",
       "                      [ 1.0097e-02,  6.5644e-01],\n",
       "                      [-3.6802e-01,  4.5706e-03],\n",
       "                      [ 2.1057e-02,  4.2514e-01],\n",
       "                      [ 3.5748e-02, -4.8626e-01],\n",
       "                      [ 2.4565e-01, -2.0448e-01],\n",
       "                      [ 3.6275e-01,  3.0165e-01],\n",
       "                      [ 1.6481e-01, -2.0982e-01],\n",
       "                      [-1.6249e-01, -7.9746e-02],\n",
       "                      [-2.7843e-01,  2.5795e-01],\n",
       "                      [-5.3555e-02,  1.9849e-01],\n",
       "                      [ 6.5039e-03, -5.8650e-01],\n",
       "                      [ 4.9239e-01,  1.6427e-02],\n",
       "                      [ 2.8743e-01, -1.7366e-01],\n",
       "                      [-5.6797e-01, -4.2091e-01],\n",
       "                      [ 3.3088e-01, -3.7846e-01],\n",
       "                      [ 4.0502e-01,  2.4048e-01],\n",
       "                      [-2.0139e-02, -1.4969e-01],\n",
       "                      [ 2.0222e-01,  8.3686e-02],\n",
       "                      [-8.0337e-02,  1.0946e-01],\n",
       "                      [ 1.2415e-01,  1.7106e-01],\n",
       "                      [-3.0502e-02, -1.2840e-01],\n",
       "                      [-2.0811e-01, -4.7043e-02],\n",
       "                      [ 1.5928e-01, -6.9818e-02],\n",
       "                      [-4.2328e-02, -1.0259e-01],\n",
       "                      [ 1.4497e-01, -2.8796e-01],\n",
       "                      [ 3.4602e-01, -2.9564e-04],\n",
       "                      [ 4.5200e-01,  3.7942e-01],\n",
       "                      [ 2.6171e-01, -2.7719e-01],\n",
       "                      [ 1.5029e-02,  3.2004e-01],\n",
       "                      [-6.8147e-02, -2.3644e-01],\n",
       "                      [-4.1699e-01,  2.1064e-01],\n",
       "                      [-2.5701e-02, -9.4169e-02],\n",
       "                      [-4.9964e-02,  1.7334e-01],\n",
       "                      [ 4.3165e-02, -2.5521e-01],\n",
       "                      [-2.9713e-01,  2.4636e-01],\n",
       "                      [-6.4405e-02, -1.6614e-01],\n",
       "                      [-3.7972e-02, -8.1339e-02],\n",
       "                      [ 2.5633e-01, -1.6075e-01],\n",
       "                      [ 2.8655e-02, -2.5345e-02],\n",
       "                      [-2.5893e-02, -1.2861e-01],\n",
       "                      [-6.7889e-02,  4.2429e-01],\n",
       "                      [ 2.8816e-01, -2.9555e-01],\n",
       "                      [-3.5302e-02,  1.3389e-01],\n",
       "                      [-3.9641e-02, -1.3444e-02],\n",
       "                      [-4.4824e-02,  1.6968e-01],\n",
       "                      [ 3.7713e-01,  1.9642e-01],\n",
       "                      [-1.2800e-01,  2.7784e-01],\n",
       "                      [ 4.8252e-02,  4.3848e-01],\n",
       "                      [-2.6857e-02, -1.1736e-01],\n",
       "                      [ 4.3344e-01,  4.1167e-02],\n",
       "                      [ 4.5806e-01,  4.7304e-02],\n",
       "                      [-3.5935e-02,  4.1379e-01],\n",
       "                      [-3.4109e-02, -9.8958e-02],\n",
       "                      [-2.7530e-01, -2.4914e-01],\n",
       "                      [ 3.8893e-02,  2.0025e-01],\n",
       "                      [ 2.8964e-01,  1.9516e-01],\n",
       "                      [ 5.7908e-02, -1.0350e-01],\n",
       "                      [ 3.2573e-01, -5.8778e-03],\n",
       "                      [-3.9831e-01, -5.1991e-02],\n",
       "                      [ 1.6503e-02,  2.8208e-01],\n",
       "                      [ 3.4217e-01, -2.7890e-01],\n",
       "                      [-7.2294e-01,  3.0575e-03],\n",
       "                      [ 2.8760e-01, -1.7334e-01],\n",
       "                      [-4.1363e-02,  3.2807e-01],\n",
       "                      [-1.7320e-02, -4.0780e-01],\n",
       "                      [ 4.6566e-02, -2.5003e-01],\n",
       "                      [-3.3061e-01,  1.4935e-01],\n",
       "                      [-3.4249e-01, -1.1910e-01],\n",
       "                      [-1.7602e-01,  1.5950e-01],\n",
       "                      [ 9.6573e-02,  1.8741e-02],\n",
       "                      [ 2.7061e-02,  4.4818e-02],\n",
       "                      [ 3.8797e-02, -2.2516e-01],\n",
       "                      [ 1.1806e-01, -3.7929e-01],\n",
       "                      [-1.6282e-02, -1.6106e-01],\n",
       "                      [ 2.4179e-01,  2.4065e-01],\n",
       "                      [ 6.2500e-02,  5.1036e-02],\n",
       "                      [-2.4012e-01,  1.0461e-01],\n",
       "                      [ 3.9607e-01,  1.2707e-01],\n",
       "                      [ 2.3780e-02, -9.1050e-03],\n",
       "                      [-1.3205e-01, -3.7648e-02],\n",
       "                      [-7.1658e-02,  2.1408e-01],\n",
       "                      [-3.0440e-01, -4.9980e-01],\n",
       "                      [-5.2117e-01, -1.6421e-01],\n",
       "                      [-2.6110e-02, -4.5892e-02],\n",
       "                      [-1.0479e-02,  2.3486e-01],\n",
       "                      [ 4.7447e-01, -4.1185e-02],\n",
       "                      [-2.3189e-01, -6.3095e-02],\n",
       "                      [ 3.5089e-02, -4.8430e-01],\n",
       "                      [-1.4420e-01, -2.6387e-01],\n",
       "                      [ 5.1226e-02, -3.5832e-01],\n",
       "                      [-1.0846e-01, -6.4740e-02],\n",
       "                      [-4.7289e-01,  1.3297e-01],\n",
       "                      [-3.2235e-01, -9.6128e-03],\n",
       "                      [-3.4909e-02,  9.9754e-02],\n",
       "                      [ 5.1952e-01, -8.3315e-04],\n",
       "                      [-4.1042e-01, -2.1776e-01],\n",
       "                      [-5.2232e-01,  2.8318e-02],\n",
       "                      [-7.9435e-02,  1.1258e-02],\n",
       "                      [-5.3452e-02,  2.6692e-01],\n",
       "                      [ 1.5247e-02,  3.7069e-01],\n",
       "                      [-2.4496e-02,  9.0354e-04],\n",
       "                      [ 2.6098e-01,  9.0072e-02],\n",
       "                      [ 3.8542e-01, -2.2830e-01],\n",
       "                      [-4.5532e-01, -1.9881e-02],\n",
       "                      [ 5.2727e-01,  8.5771e-02],\n",
       "                      [-3.4884e-02,  3.5815e-01],\n",
       "                      [-3.6168e-01, -6.7152e-02],\n",
       "                      [ 3.0858e-01,  1.5708e-01],\n",
       "                      [ 1.5423e-02, -4.3855e-01],\n",
       "                      [ 3.6842e-01, -1.4252e-02],\n",
       "                      [ 1.2282e-02, -5.6946e-01],\n",
       "                      [-3.9593e-02, -7.8049e-02],\n",
       "                      [-3.4269e-01,  3.6368e-01],\n",
       "                      [-1.0312e-02,  3.5882e-01],\n",
       "                      [-4.8118e-02, -2.9450e-01],\n",
       "                      [-1.0887e-01,  4.3025e-01],\n",
       "                      [ 3.1034e-01,  1.7513e-01],\n",
       "                      [-5.6803e-03, -3.7004e-01],\n",
       "                      [-9.1692e-02, -1.8487e-01],\n",
       "                      [-2.1476e-02, -1.6170e-01]])),\n",
       "             ('input_layer.bias',\n",
       "              tensor([ 1.1399e-01,  4.6516e-02, -3.2062e-01, -1.3022e-02,  7.0050e-02,\n",
       "                      -7.8019e-02,  2.5992e-01,  2.9908e-01,  2.3370e-01,  1.9475e-01,\n",
       "                      -2.4838e-01, -7.6040e-03, -1.2898e-01,  2.9970e-01,  8.0686e-02,\n",
       "                      -1.5371e-01,  9.3012e-02,  2.0074e-01,  2.8236e-01, -1.1484e-01,\n",
       "                      -2.1111e-02,  1.1120e-01, -2.5023e-01, -2.9942e-02, -1.2053e-01,\n",
       "                      -6.8871e-01, -4.1319e-02, -4.3854e-02, -1.4002e-01,  5.8159e-02,\n",
       "                      -6.7497e-02, -1.2985e-01,  2.5272e-01, -1.8737e-01, -2.2138e-01,\n",
       "                       3.2317e-01,  3.2238e-02, -3.5430e-02,  3.4743e-01, -7.3868e-02,\n",
       "                       3.7165e-02, -2.4507e-01, -3.5530e-01,  6.0298e-02, -7.5614e-02,\n",
       "                       1.2838e-01, -2.8307e-02, -2.5597e-01,  2.9742e-02, -3.6079e-01,\n",
       "                       3.3738e-01,  1.7871e-01, -2.4402e-02, -1.5992e-01,  1.6794e-01,\n",
       "                       1.2723e-01,  2.8106e-01,  2.6735e-01, -8.4313e-02, -5.6183e-02,\n",
       "                       3.3386e-01, -1.4864e-01,  6.9333e-02, -7.4027e-02, -1.1526e-01,\n",
       "                      -9.6543e-02, -1.7196e-02,  2.3542e-02,  3.7121e-01,  1.6582e-01,\n",
       "                      -1.4469e-01,  9.3352e-02, -2.6680e-01, -8.5833e-02,  1.4876e-01,\n",
       "                      -2.4162e-01,  1.8168e-03, -1.4943e-01, -2.9746e-01,  1.6352e-01,\n",
       "                      -1.0346e-01, -7.7627e-02, -4.7660e-02, -5.3944e-02, -1.4093e-01,\n",
       "                       2.2223e-01, -1.2472e-01, -8.8147e-02,  1.2794e-01,  4.1066e-01,\n",
       "                       2.7080e-01, -1.5500e-01,  1.9302e-02, -1.2888e-01, -3.0344e-02,\n",
       "                       1.8288e-01,  1.1150e-01, -5.0712e-02, -6.1872e-03,  1.8758e-02,\n",
       "                      -1.1210e-01, -1.5215e-01, -1.6737e-01, -6.8905e-02,  7.3146e-02,\n",
       "                       1.5315e-02,  1.4351e-01,  3.8561e-02,  9.6672e-02, -1.7412e-01,\n",
       "                       1.4118e-01,  3.9494e-04,  1.8059e-01, -3.1655e-01,  6.7743e-02,\n",
       "                      -1.4690e-01,  1.7247e-01,  1.6251e-01, -3.4499e-01,  3.0785e-03,\n",
       "                       3.1424e-01, -8.7466e-02,  2.6715e-01,  3.2555e-01,  1.3568e-01,\n",
       "                       1.1899e-01,  2.2165e-01,  2.2013e-01,  1.3074e-01,  3.2594e-01,\n",
       "                       4.4550e-02,  2.0181e-01, -1.0451e-01,  3.8734e-01, -1.1916e-01,\n",
       "                       2.7593e-01, -2.5899e-01,  2.0240e-02,  1.0639e-01, -2.5462e-01,\n",
       "                       1.4220e-01, -1.6081e-01, -2.3023e-01,  3.5077e-01,  3.2438e-01,\n",
       "                      -3.9216e-01,  2.6401e-02, -2.1136e-01, -5.9285e-03, -1.6435e-01,\n",
       "                      -2.7226e-01, -1.6991e-01, -1.2969e-01, -1.4290e-01,  1.4436e-01,\n",
       "                      -5.2102e-02,  1.5636e-01,  4.5986e-02,  2.5968e-02,  1.4916e-01,\n",
       "                       3.4043e-01, -2.0730e-01,  4.6178e-02,  1.9954e-01,  4.4741e-02,\n",
       "                       8.6960e-02, -2.4156e-01, -2.5200e-01,  4.0628e-02, -3.6574e-01,\n",
       "                       3.3522e-01, -1.2390e-01, -6.6621e-02,  8.8886e-02,  3.9949e-02,\n",
       "                      -6.7897e-03,  4.8035e-02,  8.7073e-02,  3.2982e-01, -1.5834e-01,\n",
       "                       1.2622e-01,  7.2167e-02,  2.0680e-01, -5.1646e-02,  7.0026e-02,\n",
       "                       3.3928e-01, -4.3489e-02,  8.8472e-02, -2.2025e-01,  6.8946e-02,\n",
       "                      -1.8475e-01,  3.1519e-01, -1.2166e-01, -1.0845e-01,  5.2138e-02,\n",
       "                       2.4851e-01,  1.7561e-01,  2.2940e-02, -3.8928e-02,  2.0833e-01])),\n",
       "             ('hidden_layers.0.weight',\n",
       "              tensor([[ 0.1046,  0.1380, -0.1711,  ...,  0.1795,  0.0283, -0.1115],\n",
       "                      [ 0.0224, -0.1112, -0.0845,  ...,  0.0931, -0.0420, -0.0206],\n",
       "                      [-0.0495,  0.0994, -0.2440,  ...,  0.2113,  0.1670,  0.0428],\n",
       "                      ...,\n",
       "                      [ 0.1746, -0.0402,  0.1064,  ..., -0.2202,  0.0013, -0.1176],\n",
       "                      [ 0.0657, -0.1022, -0.1538,  ..., -0.0768, -0.0824, -0.1321],\n",
       "                      [ 0.0862,  0.0672,  0.1007,  ..., -0.0552, -0.1308, -0.0730]])),\n",
       "             ('hidden_layers.0.bias',\n",
       "              tensor([ 0.0859,  0.0370,  0.1581, -0.0307,  0.0730,  0.0878,  0.0025,  0.1019,\n",
       "                       0.0133, -0.1046, -0.0049, -0.0100, -0.0372, -0.0150, -0.0674,  0.0488,\n",
       "                       0.1539, -0.0637,  0.0279,  0.1186,  0.0425,  0.0464,  0.0220,  0.0162,\n",
       "                       0.0244,  0.1203, -0.0380, -0.0557,  0.0432,  0.0605, -0.0737,  0.0999,\n",
       "                      -0.0631,  0.1222, -0.0399, -0.0418, -0.0719,  0.0188, -0.0767, -0.0720,\n",
       "                      -0.1341,  0.0077,  0.0331, -0.0403, -0.0533, -0.0165,  0.0701,  0.0853,\n",
       "                       0.0808, -0.0101,  0.0282,  0.0830,  0.0974,  0.0959,  0.0863, -0.0261,\n",
       "                      -0.0386, -0.0251, -0.0180,  0.0486, -0.0213, -0.0081, -0.0073, -0.0455,\n",
       "                       0.1353,  0.0327, -0.0804,  0.0612,  0.0463,  0.0922, -0.0790, -0.0644,\n",
       "                      -0.0507, -0.1374,  0.0420,  0.0376,  0.0444, -0.0558,  0.1714, -0.0738,\n",
       "                       0.0376, -0.0909,  0.1366,  0.0195,  0.1019, -0.0150, -0.1276, -0.0051,\n",
       "                       0.0864, -0.0306, -0.0168, -0.1609,  0.0819,  0.0580,  0.0397,  0.0609,\n",
       "                      -0.1516, -0.1348,  0.0250, -0.1665,  0.1425,  0.2215, -0.0656,  0.1992,\n",
       "                      -0.0352, -0.0712,  0.0050,  0.1456, -0.0301, -0.0198,  0.0629,  0.0265,\n",
       "                       0.0141,  0.1078,  0.1109,  0.1140, -0.0964,  0.0590,  0.0359, -0.0760,\n",
       "                      -0.0399,  0.0013, -0.1016,  0.0746,  0.1057, -0.0467, -0.0959,  0.0880,\n",
       "                      -0.0981,  0.0224, -0.0762,  0.0159,  0.1716,  0.0763,  0.0721, -0.0188,\n",
       "                      -0.0183, -0.0146,  0.0537,  0.0333,  0.0139,  0.0574,  0.0727, -0.0535,\n",
       "                       0.0481, -0.1007, -0.0096, -0.0523,  0.0374,  0.0096,  0.0260, -0.0265,\n",
       "                      -0.0726, -0.0347,  0.0963,  0.0644, -0.0585, -0.0617, -0.0068,  0.2055,\n",
       "                      -0.0902, -0.0233, -0.0007, -0.0415,  0.1549,  0.0047,  0.0176, -0.0976,\n",
       "                      -0.1124,  0.1425, -0.1401,  0.0573,  0.0212,  0.1264,  0.0944,  0.0090,\n",
       "                      -0.0319,  0.0205,  0.0624, -0.0174, -0.0158,  0.0298, -0.0427,  0.0441,\n",
       "                      -0.0081,  0.0686,  0.0011,  0.1176, -0.0235,  0.0520,  0.0902,  0.0139,\n",
       "                      -0.0187, -0.0121, -0.1043,  0.0207,  0.0367,  0.2250,  0.0481,  0.0406])),\n",
       "             ('hidden_layers.1.weight',\n",
       "              tensor([[ 0.0669,  0.1198,  0.1354,  ...,  0.1364, -0.0822,  0.1016],\n",
       "                      [ 0.1108,  0.0365,  0.0481,  ..., -0.0198,  0.1121,  0.1655],\n",
       "                      [-0.1105, -0.2037,  0.0129,  ..., -0.1091,  0.0341,  0.0044],\n",
       "                      ...,\n",
       "                      [-0.0923,  0.1746,  0.0253,  ...,  0.1253, -0.0535,  0.1234],\n",
       "                      [ 0.1913, -0.0975,  0.1498,  ...,  0.1105, -0.1108,  0.0033],\n",
       "                      [ 0.1459, -0.1569,  0.0815,  ...,  0.1526, -0.1881,  0.1119]])),\n",
       "             ('hidden_layers.1.bias',\n",
       "              tensor([-0.0068,  0.0034, -0.0416,  0.0385, -0.0275,  0.0704,  0.0467,  0.0266,\n",
       "                       0.0269, -0.0341,  0.0380,  0.0312,  0.0138, -0.0384, -0.0044,  0.0413,\n",
       "                      -0.0097,  0.0624, -0.0423, -0.0200,  0.0303, -0.0188,  0.0064,  0.0463,\n",
       "                       0.0522,  0.0168,  0.0136,  0.0025, -0.0071,  0.0100, -0.0043, -0.0522,\n",
       "                      -0.0100, -0.0250,  0.0289, -0.0736, -0.0181, -0.0377, -0.0185,  0.0458,\n",
       "                      -0.0121, -0.0119,  0.0271, -0.0290,  0.0337,  0.0090, -0.0164,  0.0007,\n",
       "                      -0.0222, -0.0050, -0.0362,  0.0011,  0.0569,  0.0475,  0.0086, -0.0160,\n",
       "                       0.0707, -0.0350,  0.0767, -0.0747,  0.0021, -0.0140, -0.0049, -0.0529,\n",
       "                      -0.0443,  0.0715,  0.0628,  0.0398,  0.0872,  0.0369,  0.0076,  0.0255,\n",
       "                       0.0219, -0.0176, -0.0395, -0.0247,  0.0695, -0.0936, -0.0361,  0.0124,\n",
       "                      -0.0195, -0.0413, -0.0018,  0.0267,  0.0604,  0.0236, -0.0277, -0.0418,\n",
       "                       0.0340,  0.0162,  0.0420, -0.0037,  0.0537, -0.0016,  0.0726,  0.0341,\n",
       "                      -0.0432, -0.0478,  0.0305, -0.0792,  0.0630, -0.0591,  0.0733,  0.0140,\n",
       "                      -0.0268,  0.0416, -0.0082, -0.0008, -0.0244,  0.0540, -0.0629, -0.0343,\n",
       "                      -0.0191, -0.0593, -0.0160,  0.0166, -0.0331,  0.0546,  0.0143,  0.0583,\n",
       "                      -0.0365, -0.0711, -0.0011,  0.0217,  0.0022, -0.0579,  0.0066,  0.0115,\n",
       "                      -0.0390,  0.0509,  0.0194, -0.0442, -0.0127, -0.0793, -0.0099, -0.0671,\n",
       "                      -0.0326, -0.0110,  0.0317, -0.0255, -0.0059, -0.0235,  0.0930, -0.0278,\n",
       "                       0.0030, -0.0436, -0.0354,  0.0572, -0.0161,  0.0343, -0.1172, -0.0293,\n",
       "                      -0.0452, -0.0089,  0.0012, -0.0777, -0.0360, -0.0258,  0.0075, -0.0606,\n",
       "                       0.0086,  0.0198, -0.0860, -0.0309, -0.0518,  0.0069,  0.0046, -0.0965,\n",
       "                       0.0375, -0.0091,  0.0350, -0.0275, -0.0586, -0.0053,  0.0498, -0.0112,\n",
       "                       0.0669,  0.0145,  0.0046,  0.0801,  0.0568,  0.0140,  0.0113,  0.0004,\n",
       "                      -0.0349,  0.0106, -0.0015,  0.0034,  0.0409, -0.0605,  0.0411,  0.0201,\n",
       "                       0.0323,  0.0449, -0.0587,  0.0123, -0.0584,  0.0179,  0.0258,  0.0379])),\n",
       "             ('hidden_layers.2.weight',\n",
       "              tensor([[ 1.0669e-01, -3.6015e-02, -2.0791e-01,  ...,  6.4242e-02,\n",
       "                        9.5805e-02,  1.4890e-01],\n",
       "                      [ 2.3483e-01,  1.1727e-05,  2.6225e-02,  ...,  9.7407e-02,\n",
       "                        1.6595e-01, -1.6545e-01],\n",
       "                      [ 1.6204e-02,  1.6020e-01,  1.0227e-01,  ..., -8.5098e-02,\n",
       "                        8.7885e-02,  1.0688e-01],\n",
       "                      ...,\n",
       "                      [-1.7858e-01,  1.0609e-01,  7.8679e-02,  ..., -4.8720e-02,\n",
       "                       -2.0938e-01,  1.7960e-01],\n",
       "                      [-7.2042e-02,  2.3014e-01,  1.7415e-01,  ..., -2.0387e-01,\n",
       "                        5.0393e-02, -3.5900e-01],\n",
       "                      [ 1.4170e-01, -6.0703e-02, -1.3969e-01,  ...,  2.0856e-01,\n",
       "                       -2.5380e-02,  1.2153e-01]])),\n",
       "             ('hidden_layers.2.bias',\n",
       "              tensor([ 3.6504e-02, -2.8098e-02, -7.1477e-03, -2.4411e-02,  1.0232e-02,\n",
       "                       4.4537e-02,  6.2278e-02,  3.8125e-02, -8.4810e-03, -1.0122e-02,\n",
       "                      -4.9010e-02,  2.5521e-02, -9.1122e-03, -7.4446e-03, -1.0165e-02,\n",
       "                      -4.9772e-02, -1.3430e-02, -6.9862e-03, -3.5357e-03, -1.6643e-02,\n",
       "                       3.6019e-02,  1.7969e-02, -1.5131e-02,  1.6306e-02,  3.2599e-02,\n",
       "                      -2.7890e-02, -7.6404e-03,  6.4267e-03,  4.1275e-02, -4.8222e-02,\n",
       "                      -5.2228e-02, -1.5545e-02,  8.1409e-03, -3.6359e-02,  1.9891e-02,\n",
       "                       7.8213e-03,  2.1738e-02,  1.5062e-02, -4.3710e-02, -3.0381e-02,\n",
       "                       2.0938e-02, -8.0168e-03, -3.6647e-03, -2.1198e-02, -5.0556e-03,\n",
       "                      -2.3409e-02, -1.5787e-02,  6.2248e-02,  4.5024e-02,  2.4008e-02,\n",
       "                       1.5776e-02,  7.8768e-03,  4.5201e-02,  8.9695e-03,  1.7959e-02,\n",
       "                      -5.4980e-03, -3.0477e-02,  1.2220e-02, -3.4844e-02,  1.2250e-02,\n",
       "                       1.7637e-02, -3.1364e-03, -8.7676e-03,  1.1380e-02,  5.3991e-02,\n",
       "                       2.9269e-02,  1.4721e-02, -3.2749e-02, -1.2797e-02,  6.1287e-03,\n",
       "                       1.0117e-02, -2.8284e-02,  2.4471e-03,  3.5667e-02, -3.5015e-02,\n",
       "                       1.3505e-02, -3.8055e-02, -2.5849e-02, -7.3466e-02,  4.5309e-02,\n",
       "                       6.0525e-03,  1.3245e-02, -1.2108e-02, -4.8644e-03,  3.1135e-02,\n",
       "                       1.8998e-02,  6.3470e-02,  9.4041e-02,  1.4250e-02,  3.8794e-03,\n",
       "                       3.7627e-03,  3.1618e-02, -2.8686e-02,  3.7121e-02,  9.9887e-03,\n",
       "                      -5.5001e-03, -4.0625e-04,  2.4997e-02, -1.4325e-02,  1.7431e-02,\n",
       "                      -5.9822e-02, -2.4269e-02,  1.7158e-02,  3.7550e-03, -2.1960e-02,\n",
       "                       6.8619e-03, -2.7061e-02,  6.2446e-03, -5.1875e-02,  2.4225e-02,\n",
       "                       1.0985e-02,  5.2308e-02,  9.0097e-03, -2.1031e-02,  3.0119e-02,\n",
       "                      -2.9034e-02,  2.8870e-02, -4.4382e-02,  1.4247e-02, -1.2376e-02,\n",
       "                       1.0838e-03, -2.1833e-02,  1.0499e-03, -5.2194e-02,  4.5860e-02,\n",
       "                      -3.4840e-03,  1.2114e-03, -1.7969e-02,  4.9999e-02,  2.5788e-02,\n",
       "                       6.7380e-05,  5.6189e-04,  4.8026e-02, -4.0138e-03, -9.1845e-03,\n",
       "                      -8.8705e-03,  2.9779e-02,  7.9078e-02,  1.4115e-02, -2.0660e-03,\n",
       "                       1.4423e-02,  7.3830e-03, -2.1082e-02, -4.1733e-03,  5.4031e-02,\n",
       "                       5.2243e-02,  2.2225e-02, -1.8947e-02, -3.3379e-02, -1.9673e-02,\n",
       "                      -2.4277e-02,  4.3823e-03, -1.5992e-02, -1.4926e-02, -2.4227e-02,\n",
       "                       4.1926e-02, -9.9414e-03, -3.8015e-02,  8.1449e-03,  1.9436e-02,\n",
       "                       4.1810e-02,  2.5298e-02,  3.1678e-03,  4.9693e-02, -9.1436e-03,\n",
       "                      -5.9609e-02,  2.0264e-03, -1.7424e-02,  1.1667e-02, -8.2125e-03,\n",
       "                       1.8592e-02, -5.8449e-03,  5.3216e-02, -1.1645e-02, -4.8877e-02,\n",
       "                       2.0721e-02,  1.1116e-02, -7.4691e-04,  2.4755e-03, -1.2394e-03,\n",
       "                      -3.8950e-03,  3.3417e-02,  3.6384e-02,  4.8268e-03, -3.1038e-02,\n",
       "                       1.1632e-02, -4.2585e-02,  5.1360e-02, -6.9920e-03,  5.8776e-05,\n",
       "                      -1.6191e-02,  8.8216e-03, -6.0393e-04, -9.7349e-04, -3.6574e-03,\n",
       "                      -3.4019e-02,  5.7901e-02, -1.0106e-02, -5.5662e-02,  4.0831e-02])),\n",
       "             ('hidden_layers.3.weight',\n",
       "              tensor([[ 0.2306,  0.1342, -0.0065,  ..., -0.1369, -0.1301,  0.2162],\n",
       "                      [-0.1099,  0.1701,  0.0928,  ..., -0.0747, -0.2164, -0.1666],\n",
       "                      [ 0.1390,  0.0234,  0.1420,  ..., -0.1973,  0.0728,  0.1698],\n",
       "                      ...,\n",
       "                      [ 0.1594,  0.1191, -0.0619,  ..., -0.1113,  0.1705,  0.2199],\n",
       "                      [-0.1357, -0.0392,  0.1682,  ..., -0.0102,  0.0003, -0.0931],\n",
       "                      [ 0.0156, -0.0330,  0.0466,  ..., -0.1079,  0.0728,  0.1685]])),\n",
       "             ('hidden_layers.3.bias',\n",
       "              tensor([ 3.3447e-02, -3.8660e-02, -1.2533e-02,  1.2788e-02,  1.6778e-03,\n",
       "                      -3.7189e-02,  6.4795e-04, -1.5213e-03,  1.4601e-03,  7.0366e-03,\n",
       "                       1.1121e-02,  2.1210e-02,  1.5623e-02,  2.9893e-02, -2.9132e-03,\n",
       "                       1.0178e-03, -7.4711e-03, -4.1016e-02,  1.8596e-03,  2.2179e-03,\n",
       "                       1.1019e-02,  3.9081e-03,  1.9522e-02, -2.8879e-02,  8.0518e-04,\n",
       "                       3.1983e-02, -4.2813e-03, -2.5839e-03, -3.0953e-03, -4.7962e-03,\n",
       "                       3.6698e-03,  1.6717e-02,  5.2598e-02,  2.9724e-03, -2.3639e-02,\n",
       "                       4.3021e-02, -2.9178e-02, -7.2890e-03,  2.1537e-03,  3.2850e-02,\n",
       "                       2.5197e-02,  1.9671e-03, -1.4895e-02,  4.2268e-03, -1.9777e-02,\n",
       "                       1.6920e-02,  2.8397e-02,  4.5355e-02, -4.0931e-03, -4.7944e-02,\n",
       "                       4.0135e-03, -9.5669e-03, -4.3167e-03, -1.3496e-02,  8.3057e-03,\n",
       "                       2.0702e-02, -7.5420e-03, -5.6880e-03, -1.2631e-03, -2.0860e-02,\n",
       "                      -9.1193e-03,  1.5357e-02, -3.3587e-04,  2.1950e-02, -7.0168e-02,\n",
       "                      -4.1861e-03, -1.5469e-03,  5.4882e-03,  5.6950e-02, -3.8052e-02,\n",
       "                       6.1744e-03,  3.7141e-02,  7.5256e-03, -2.2492e-03,  7.6432e-03,\n",
       "                      -4.1085e-02, -1.6299e-02,  3.6101e-02, -2.5729e-03, -2.5318e-03,\n",
       "                      -7.3287e-03, -1.5556e-04,  4.2220e-03, -5.5370e-02, -4.1906e-02,\n",
       "                       4.7348e-03,  1.2096e-03,  2.5366e-02,  2.1930e-02,  1.0550e-02,\n",
       "                      -2.9742e-02,  1.2005e-02,  7.7752e-03,  6.5660e-03,  1.1124e-02,\n",
       "                       1.7817e-02,  2.6589e-02,  3.2336e-02,  4.7062e-02, -1.1054e-05,\n",
       "                       3.5695e-03, -2.2400e-02, -3.3147e-02,  4.6832e-02, -3.5998e-05,\n",
       "                      -2.1141e-02, -1.4055e-02,  5.2349e-04,  1.4587e-02, -3.2192e-02,\n",
       "                       4.5866e-03, -1.3688e-03,  1.3990e-03,  2.9474e-02,  2.2674e-02,\n",
       "                       8.0396e-04, -4.9868e-04,  3.6494e-02, -9.4537e-03, -3.3017e-02,\n",
       "                      -4.3751e-02, -3.6302e-02, -1.2391e-02,  2.0586e-02, -1.0237e-03,\n",
       "                      -3.1543e-03,  1.9227e-02,  3.4075e-02, -2.2771e-02, -4.5378e-02,\n",
       "                      -7.9686e-03, -7.1836e-03, -3.9284e-03, -1.0001e-03,  2.3902e-02,\n",
       "                       3.2457e-02, -5.2077e-03,  9.8942e-04, -4.0399e-02,  3.2432e-02,\n",
       "                      -1.2909e-02,  1.4007e-02, -3.2020e-02, -4.8050e-03, -3.3423e-02,\n",
       "                      -4.1341e-02,  6.1751e-03,  4.1554e-03,  1.3900e-03, -1.0714e-02,\n",
       "                      -1.5352e-02, -8.1560e-03, -1.1502e-02, -3.9939e-03, -5.7486e-03,\n",
       "                       2.3089e-02,  2.5494e-02,  1.0318e-02, -1.9635e-02,  2.2459e-03,\n",
       "                       9.7183e-04,  3.1627e-02,  3.9380e-02,  4.6293e-03, -2.3431e-02,\n",
       "                      -6.4072e-04,  1.7192e-02,  3.4350e-03,  1.8059e-02, -3.3915e-03,\n",
       "                       4.4415e-02, -1.1787e-02,  7.5837e-03,  2.0625e-02,  6.6643e-03,\n",
       "                       3.9681e-02, -6.9100e-03, -9.8066e-04,  1.2254e-02, -3.1923e-03,\n",
       "                      -2.6782e-02,  1.0342e-02,  7.6245e-03,  6.6009e-03,  4.7665e-03,\n",
       "                       3.3806e-03, -2.5340e-02,  2.9330e-03,  1.2422e-02, -2.2778e-03,\n",
       "                       1.9051e-02,  6.6136e-03, -8.4463e-03, -1.1495e-03,  2.9921e-02,\n",
       "                      -1.9857e-03,  2.7231e-02,  8.8962e-03,  2.2928e-03, -2.2536e-02])),\n",
       "             ('output_layer.weight',\n",
       "              tensor([[ 0.2939, -0.2228,  0.1210,  0.3289, -0.0352, -0.3343, -0.0591, -0.0817,\n",
       "                       -0.0450,  0.0258,  0.3629,  0.5068, -0.3440, -0.2298,  0.2521, -0.0714,\n",
       "                        0.7527,  0.4642,  0.0256, -0.0278, -0.2914, -0.1921,  0.0829,  0.1001,\n",
       "                       -0.0691,  0.3419, -0.0753, -0.1839,  0.4809, -0.3885, -0.3020, -0.0042,\n",
       "                       -0.2849,  0.0247,  0.3105,  0.4310, -0.1531,  0.3556, -0.2000,  0.3837,\n",
       "                       -0.1618,  0.0294, -0.2507,  0.3162,  0.0053, -0.3267,  0.3307, -0.3480,\n",
       "                       -0.2822, -0.2681, -0.0499,  0.0407, -0.0901, -0.0773,  0.1094, -0.3063,\n",
       "                        0.0742, -0.1212, -0.2877, -0.4275,  0.0700, -0.2026,  0.1905, -0.2103,\n",
       "                        0.3283, -0.1777, -0.0464,  0.1880,  0.4542, -0.5435,  0.1563,  0.1364,\n",
       "                        0.1653, -0.4026,  0.5448,  0.4998,  0.2283, -0.1559, -0.0145, -0.0631,\n",
       "                        0.0337,  0.0149,  0.0199,  0.2608,  0.1174,  0.1995,  0.0847,  0.4983,\n",
       "                        0.3143,  0.2701,  0.5119,  0.1649, -0.1268, -0.4148,  0.1884,  0.2228,\n",
       "                        0.4472,  0.3174, -0.5371,  0.0211,  0.3261,  0.4786,  0.2428,  0.1081,\n",
       "                        0.2421,  0.1920, -0.3775,  0.0521, -0.2263,  0.1821, -0.6240, -0.0618,\n",
       "                        0.0632,  0.3606, -0.5029, -0.0322, -0.2979, -0.2898,  0.2032, -0.4157,\n",
       "                       -0.0429, -0.3453, -0.2226,  0.2941,  0.2021,  0.3772, -0.1451,  0.2579,\n",
       "                       -0.3040,  0.1716,  0.1046, -0.1262, -0.0912,  0.0050, -0.1340, -0.2328,\n",
       "                        0.1758,  0.1554,  0.3311, -0.5538, -0.2668, -0.2962, -0.3939, -0.0148,\n",
       "                        0.3382, -0.4883,  0.2072,  0.0384, -0.1999,  0.3151, -0.2003,  0.4266,\n",
       "                       -0.0458, -0.1237, -0.1017, -0.4180, -0.2675,  0.0227,  0.1256,  0.0904,\n",
       "                        0.1983, -0.3269, -0.2565,  0.1813,  0.5241,  0.1061, -0.0870, -0.1346,\n",
       "                       -0.3246,  0.0340, -0.3858,  0.3074, -0.5361, -0.2167, -0.0034,  0.3393,\n",
       "                        0.2033,  0.0634,  0.3456, -0.0620, -0.2206, -0.2194,  0.2761, -0.2084,\n",
       "                        0.2864,  0.0725, -0.5597, -0.0915, -0.0608,  0.6874,  0.2615, -0.1175,\n",
       "                       -0.3494, -0.2407,  0.3185, -0.0948, -0.2349,  0.0807, -0.0987,  0.3589]])),\n",
       "             ('output_layer.bias', tensor([-0.0267]))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5006ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_test = torch.linspace(0, 8*pi, 10000).reshape(-1,1)\n",
    "t_test = torch.ones((10000,1))\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution(x_test, t_test).reshape(-1,1)\n",
    "\n",
    "my_network = my_network.cpu()\n",
    "u_test_pred = my_network(test).reshape(-1,1)\n",
    "\n",
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((u_test_pred - u_test)**2)/torch.mean(u_test**2)\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")\n",
    "\n",
    "#plt.grid(True, which=\"both\", ls=\":\")\n",
    "\n",
    "# Set Times New Roman font for axes labels and ticks\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "font_path = 'times-new-roman.ttf'\n",
    "ticks_font = FontProperties(fname=font_path, size=14)\n",
    "plt.xlabel(\"x\", fontsize=20, fontproperties=ticks_font)\n",
    "plt.ylabel(\"u\", fontsize=20, fontproperties=ticks_font)\n",
    "\n",
    "# Set Times New Roman font for ticks\n",
    "plt.xticks(fontsize=20, fontproperties=ticks_font)\n",
    "plt.yticks(fontsize=20, fontproperties=ticks_font)\n",
    "#plt.plot(x_test, u_test, label=\"Ground Truth\",lw=2)\n",
    "plt.scatter(x_test[::100], u_test[::100], label=\"Ground Truth\", color='blue', lw=2)\n",
    "plt.plot(x_test, u_test_pred.detach(), label=\"Prediction\",color='red', lw=2)\n",
    "legend_font = FontProperties(family='Times New Roman', style='normal', size=30)\n",
    "\n",
    "#\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.savefig('diff_ic_dotted.pdf', dpi = 500, bbox_inches = \"tight\", format='pdf', backend='cairo')\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(history)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa273157",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37806fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(0, 8*pi, 10000).reshape(-1,1)\n",
    "t_test = torch.ones((10000,1))\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution(x_test, t_test).reshape(-1,1)\n",
    "\n",
    "my_network = my_network.cpu()\n",
    "u_test_pred = my_network(test).reshape(-1,1)\n",
    "\n",
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((u_test_pred - u_test)**2)/torch.mean(u_test**2)\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026556dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate data\n",
    "x_test = torch.linspace(0, 8 * np.pi, 10000).reshape(-1, 1)\n",
    "t_test = torch.ones((10000, 1))\n",
    "test = torch.cat([x_test, t_test], 1)\n",
    "u_test = exact_solution(x_test, t_test).reshape(-1, 1)\n",
    "\n",
    "my_network = my_network.cpu()\n",
    "u_test_pred = my_network(test).reshape(-1, 1)\n",
    "\n",
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((u_test_pred - u_test) ** 2) / torch.mean(u_test ** 2)\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")\n",
    "\n",
    "# Create a plot with a white background\n",
    "plt.figure(figsize=(10, 6), facecolor='white')\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "\n",
    "# Create the plot for Ground Truth\n",
    "plt.scatter(x_test[::200], u_test[::200], label=\"Ground Truth\", color='blue', lw=2)\n",
    "\n",
    "# Create the plot for Network Prediction using dots\n",
    "plt.plot(x_test, u_test_pred.detach(), 'r', label=\"Network Prediction\")\n",
    "\n",
    "# Set Times New Roman font for axes labels and ticks\n",
    "font_path = 'times-new-roman.ttf'\n",
    "ticks_font = FontProperties(fname=font_path, size=14)\n",
    "plt.xlabel(\"x\", fontsize=14, fontproperties=ticks_font)\n",
    "plt.ylabel(\"u\", fontsize=14, fontproperties=ticks_font)\n",
    "\n",
    "# Set Times New Roman font for ticks\n",
    "plt.xticks(fontsize=14, fontproperties=ticks_font)\n",
    "plt.yticks(fontsize=14, fontproperties=ticks_font)\n",
    "\n",
    "# Labels and legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f1beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
