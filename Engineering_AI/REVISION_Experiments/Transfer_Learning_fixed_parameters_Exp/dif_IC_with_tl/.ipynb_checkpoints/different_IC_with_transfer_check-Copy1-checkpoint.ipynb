{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad512cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import pi\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb58902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causality param\n",
    "eps = 5\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Define the exact solution\n",
    "def exact_solution(x, t):\n",
    "    return torch.sin(x)*torch.exp(t)\n",
    "\n",
    "def initial_condition(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "def initial_condition_t(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "# assigning number of points\n",
    "initial_pts = 500\n",
    "left_boundary_pts = 500\n",
    "right_boundary_pts = 500\n",
    "residual_pts = 10000\n",
    "\n",
    "# Type of optimizer (ADAM or LBFGS)\n",
    "opt_type = \"LBFGS\"\n",
    "\n",
    "x_init = 8*pi*torch.rand((initial_pts,1)) # initial pts\n",
    "t_init = 0*x_init\n",
    "init = torch.cat([x_init, t_init],1).to(device)\n",
    "u_init = initial_condition(init[:,0]).reshape(-1, 1).to(device)\n",
    "u_init_t = initial_condition(init[:,0]).reshape(-1, 1).to(device)\n",
    "\n",
    "xb_left = torch.zeros((left_boundary_pts, 1)) # left spatial boundary\n",
    "tb_left = torch.rand((left_boundary_pts, 1)) #\n",
    "b_left = torch.cat([xb_left, tb_left ],1).to(device)\n",
    "u_b_l = 0*torch.sin(tb_left).to(device)\n",
    "\n",
    "xb_right = 8*pi*torch.ones((right_boundary_pts, 1)) # right spatial boundary\n",
    "tb_right = torch.rand((right_boundary_pts, 1)) # right boundary pts\n",
    "b_right = torch.cat([xb_right, tb_right ],1).to(device)\n",
    "u_b_r = 0*torch.sin(2*pi - tb_right).to(device)\n",
    "\n",
    "x_int = torch.linspace(0, 8*pi, 102)\n",
    "x_int = x_int[1:-1]\n",
    "\n",
    "t_int = torch.linspace(0, 1, 102)\n",
    "t_int = t_int[1:-1]\n",
    "\n",
    "x_interior = x_int.tile((100,))\n",
    "x_interior = x_interior.reshape(-1,1)\n",
    "\n",
    "t_interior = t_int.repeat_interleave(100)\n",
    "t_interior = t_interior.reshape(-1,1)\n",
    "\n",
    "# torch.set_printoptions(threshold=10_000)\n",
    "\n",
    "interior = torch.cat([x_interior, t_interior],1).to(device)\n",
    "\n",
    "n = 100  # size of matrix\n",
    "W = torch.tril(torch.ones(n, n), diagonal=-1).to(device)  # create a lower triangular matrix of ones\n",
    "W -= torch.diag(torch.diag(W)).to(device)  # set the diagonal elements to zero\n",
    "\n",
    "training_set = DataLoader(torch.utils.data.TensorDataset(init.to(device), u_init.to(device), u_init_t.to(device), b_left.to(device),  b_right.to(device), u_b_l.to(device), u_b_r.to(device)), batch_size=500, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79235936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension, output_dimension, n_hidden_layers, neurons):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer\n",
    "        self.neurons = neurons\n",
    "        # Number of hidden layers\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        # Activation function\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.neurons, self.neurons) for _ in range(n_hidden_layers)])\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear transformations defining the network\n",
    "        # (see equation above)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x = self.activation(l(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11a572e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNet_Seq(input_dimension, output_dimension, n_hidden_layers, neurons):\n",
    "    modules = list()\n",
    "    modules.append(nn.Linear(input_dimension, neurons))\n",
    "    modules.append(nn.Tanh())\n",
    "    for _ in range(n_hidden_layers):\n",
    "        modules.append(nn.Linear(neurons, neurons))\n",
    "        modules.append(nn.Tanh())\n",
    "    modules.append(nn.Linear(neurons, output_dimension))\n",
    "    model =  nn.Sequential(*modules)\n",
    "    return model\n",
    "\n",
    "# Model definition\n",
    "my_network = NeuralNet(input_dimension = init.shape[1], output_dimension = u_init.shape[1], n_hidden_layers=4, neurons=200)\n",
    "model_state_dict = torch.load('causal_eb1.pth', map_location=torch.device('cpu'))\n",
    "my_network = my_network.to(device)\n",
    "\n",
    "# # after defining my network - also dont forget to comment xavier\n",
    "# my_network.load_state_dict(model_state_dict)\n",
    "# my_network = my_network.to(device)\n",
    "# # after defining my network - also dont forget to comment xavier\n",
    "# my_network.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f411c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('input_layer.weight',\n",
       "              tensor([[-0.1483, -0.1306],\n",
       "                      [-0.5120,  0.4005],\n",
       "                      [ 0.2212,  0.4857],\n",
       "                      [ 0.2502,  0.1851],\n",
       "                      [-0.2606, -0.3258],\n",
       "                      [ 0.6749, -0.5799],\n",
       "                      [-0.4390, -0.5032],\n",
       "                      [-0.4062, -0.2990],\n",
       "                      [ 0.0892,  0.1295],\n",
       "                      [ 0.0749,  0.2775],\n",
       "                      [-0.0104, -0.6196],\n",
       "                      [-0.0484,  0.6508],\n",
       "                      [ 0.4884,  0.6554],\n",
       "                      [ 0.2299, -0.2209],\n",
       "                      [ 0.2819, -0.6447],\n",
       "                      [-0.4191,  0.4049],\n",
       "                      [ 0.1176,  0.2600],\n",
       "                      [-0.5179, -0.0029],\n",
       "                      [ 0.0064,  0.0503],\n",
       "                      [ 0.5350,  0.3187],\n",
       "                      [ 0.2102,  0.2128],\n",
       "                      [ 0.4654, -0.5798],\n",
       "                      [ 0.3573, -0.1273],\n",
       "                      [ 0.3342,  0.4510],\n",
       "                      [ 0.5456,  0.2701],\n",
       "                      [-0.3403,  0.4177],\n",
       "                      [-0.5440,  0.2664],\n",
       "                      [-0.1718,  0.0969],\n",
       "                      [ 0.2275,  0.4339],\n",
       "                      [ 0.4488, -0.5014],\n",
       "                      [ 0.5432,  0.5888],\n",
       "                      [-0.5891, -0.0668],\n",
       "                      [-0.1009,  0.3511],\n",
       "                      [-0.2217,  0.0127],\n",
       "                      [ 0.2510,  0.0492],\n",
       "                      [ 0.0358, -0.0309],\n",
       "                      [-0.4430, -0.6152],\n",
       "                      [ 0.1709, -0.0629],\n",
       "                      [-0.3300,  0.4140],\n",
       "                      [-0.6244, -0.5756],\n",
       "                      [-0.5066, -0.0881],\n",
       "                      [-0.1858,  0.1366],\n",
       "                      [ 0.2712,  0.1137],\n",
       "                      [-0.3332, -0.1508],\n",
       "                      [ 0.3770,  0.2865],\n",
       "                      [-0.3016,  0.2813],\n",
       "                      [ 0.6754,  0.5748],\n",
       "                      [ 0.0883,  0.0266],\n",
       "                      [ 0.2809, -0.2470],\n",
       "                      [-0.0243,  0.6041],\n",
       "                      [-0.0195,  0.6626],\n",
       "                      [ 0.5221,  0.3159],\n",
       "                      [ 0.5750,  0.0065],\n",
       "                      [ 0.4596,  0.6835],\n",
       "                      [ 0.6052,  0.2694],\n",
       "                      [ 0.6957, -0.6941],\n",
       "                      [-0.5450,  0.2070],\n",
       "                      [ 0.6514,  0.0768],\n",
       "                      [-0.4785,  0.5999],\n",
       "                      [-0.3608,  0.4903],\n",
       "                      [-0.3778,  0.0299],\n",
       "                      [ 0.1268, -0.0037],\n",
       "                      [ 0.0467, -0.5485],\n",
       "                      [ 0.1423,  0.0461],\n",
       "                      [-0.5199,  0.6882],\n",
       "                      [ 0.6506, -0.6265],\n",
       "                      [-0.4703, -0.1571],\n",
       "                      [ 0.6106, -0.0023],\n",
       "                      [-0.1114, -0.6645],\n",
       "                      [ 0.4913, -0.3331],\n",
       "                      [-0.1641,  0.3544],\n",
       "                      [-0.1861,  0.1005],\n",
       "                      [ 0.2087,  0.6281],\n",
       "                      [-0.2358, -0.3374],\n",
       "                      [ 0.2537, -0.3783],\n",
       "                      [-0.1631, -0.5323],\n",
       "                      [-0.3726,  0.1024],\n",
       "                      [ 0.5559,  0.2291],\n",
       "                      [-0.7025, -0.4727],\n",
       "                      [ 0.2643, -0.6620],\n",
       "                      [-0.5791, -0.2102],\n",
       "                      [-0.5163, -0.1170],\n",
       "                      [-0.0844, -0.0634],\n",
       "                      [ 0.5799, -0.3762],\n",
       "                      [-0.5282,  0.1026],\n",
       "                      [-0.0649, -0.0108],\n",
       "                      [-0.3142,  0.1393],\n",
       "                      [ 0.6587, -0.5282],\n",
       "                      [-0.0565,  0.0882],\n",
       "                      [ 0.3747, -0.1281],\n",
       "                      [-0.4120,  0.3234],\n",
       "                      [ 0.5221,  0.0816],\n",
       "                      [-0.5455,  0.0235],\n",
       "                      [-0.2379,  0.1134],\n",
       "                      [ 0.1439,  0.1991],\n",
       "                      [ 0.1324,  0.5185],\n",
       "                      [-0.4656,  0.2738],\n",
       "                      [ 0.3678,  0.0402],\n",
       "                      [ 0.6851,  0.6206],\n",
       "                      [-0.6678, -0.5394],\n",
       "                      [-0.5147,  0.0983],\n",
       "                      [-0.0795, -0.1084],\n",
       "                      [-0.4654,  0.5808],\n",
       "                      [-0.2740, -0.6193],\n",
       "                      [ 0.5894, -0.2070],\n",
       "                      [-0.6964,  0.2098],\n",
       "                      [-0.5551, -0.0514],\n",
       "                      [ 0.5153, -0.2330],\n",
       "                      [ 0.7004, -0.3417],\n",
       "                      [-0.1587, -0.2885],\n",
       "                      [ 0.1706,  0.4560],\n",
       "                      [ 0.1604,  0.2600],\n",
       "                      [-0.6978, -0.5155],\n",
       "                      [-0.1340, -0.2615],\n",
       "                      [-0.3385, -0.3701],\n",
       "                      [-0.1924,  0.2814],\n",
       "                      [-0.0166,  0.5138],\n",
       "                      [ 0.3565, -0.0333],\n",
       "                      [ 0.0025, -0.5967],\n",
       "                      [-0.2036, -0.2741],\n",
       "                      [-0.4663,  0.2494],\n",
       "                      [-0.4011, -0.4249],\n",
       "                      [ 0.5539,  0.5708],\n",
       "                      [ 0.0179, -0.6532],\n",
       "                      [ 0.4048,  0.2189],\n",
       "                      [ 0.3582,  0.6027],\n",
       "                      [-0.4551, -0.1171],\n",
       "                      [-0.0309, -0.1949],\n",
       "                      [-0.1709, -0.0939],\n",
       "                      [ 0.0394,  0.3866],\n",
       "                      [ 0.2414,  0.4313],\n",
       "                      [ 0.3194,  0.6523],\n",
       "                      [ 0.1869,  0.0856],\n",
       "                      [ 0.3921,  0.2386],\n",
       "                      [ 0.5985,  0.0935],\n",
       "                      [ 0.3475, -0.4379],\n",
       "                      [ 0.2842,  0.7004],\n",
       "                      [-0.3363,  0.6467],\n",
       "                      [-0.2655, -0.0832],\n",
       "                      [ 0.1669, -0.5232],\n",
       "                      [ 0.5675,  0.2963],\n",
       "                      [ 0.2857,  0.1988],\n",
       "                      [ 0.6015,  0.4307],\n",
       "                      [ 0.3413,  0.1755],\n",
       "                      [-0.5797, -0.6048],\n",
       "                      [-0.4289, -0.2817],\n",
       "                      [-0.4938, -0.5622],\n",
       "                      [-0.3877, -0.3249],\n",
       "                      [ 0.5890, -0.0093],\n",
       "                      [-0.6513, -0.6858],\n",
       "                      [ 0.0381, -0.6388],\n",
       "                      [-0.5360, -0.6137],\n",
       "                      [-0.3383, -0.6342],\n",
       "                      [ 0.1036,  0.2838],\n",
       "                      [ 0.2311, -0.1637],\n",
       "                      [ 0.0057,  0.3627],\n",
       "                      [ 0.0490,  0.5269],\n",
       "                      [ 0.2068,  0.2127],\n",
       "                      [ 0.1849, -0.2454],\n",
       "                      [-0.1484, -0.0513],\n",
       "                      [ 0.4675,  0.3701],\n",
       "                      [ 0.6813,  0.5213],\n",
       "                      [-0.1283,  0.5548],\n",
       "                      [ 0.6946, -0.5967],\n",
       "                      [ 0.6019, -0.3779],\n",
       "                      [-0.3203,  0.5500],\n",
       "                      [ 0.0102, -0.0842],\n",
       "                      [-0.3070, -0.3910],\n",
       "                      [ 0.3970,  0.7009],\n",
       "                      [-0.5686,  0.5892],\n",
       "                      [ 0.6153, -0.6475],\n",
       "                      [-0.2528, -0.3329],\n",
       "                      [ 0.2122,  0.6484],\n",
       "                      [-0.4283, -0.2046],\n",
       "                      [-0.3218, -0.0425],\n",
       "                      [-0.6493,  0.3796],\n",
       "                      [ 0.5976, -0.2825],\n",
       "                      [-0.1223, -0.3061],\n",
       "                      [ 0.6969, -0.0967],\n",
       "                      [ 0.1891, -0.1466],\n",
       "                      [-0.5609, -0.4839],\n",
       "                      [-0.6238, -0.3559],\n",
       "                      [-0.0564,  0.6291],\n",
       "                      [ 0.2296, -0.6300],\n",
       "                      [-0.6059, -0.4248],\n",
       "                      [ 0.4047, -0.1588],\n",
       "                      [ 0.3105, -0.5239],\n",
       "                      [ 0.2563,  0.1800],\n",
       "                      [ 0.1942, -0.3006],\n",
       "                      [ 0.4574,  0.0444],\n",
       "                      [-0.3573, -0.1419],\n",
       "                      [-0.6918, -0.1654],\n",
       "                      [ 0.3127, -0.5469],\n",
       "                      [-0.1833, -0.0676],\n",
       "                      [ 0.0971,  0.4837],\n",
       "                      [ 0.6516,  0.4001],\n",
       "                      [ 0.5198,  0.6251],\n",
       "                      [-0.3944,  0.2742],\n",
       "                      [-0.2075,  0.4385],\n",
       "                      [ 0.1844, -0.4983]])),\n",
       "             ('input_layer.bias',\n",
       "              tensor([ 0.1496, -0.1365,  0.1339, -0.5513,  0.2302, -0.0919,  0.5560,  0.3760,\n",
       "                      -0.2914,  0.3174, -0.3311,  0.3270, -0.6497,  0.1040,  0.0780,  0.2809,\n",
       "                       0.1003,  0.4022,  0.2796, -0.6856, -0.0741,  0.0817, -0.3798,  0.3806,\n",
       "                       0.4513, -0.0997, -0.3310, -0.4556,  0.1071,  0.3831, -0.5869, -0.6596,\n",
       "                       0.1521, -0.3900,  0.1390,  0.5285, -0.3751, -0.6930, -0.5328,  0.2527,\n",
       "                       0.6238,  0.5359, -0.5936,  0.3250, -0.1846,  0.3218, -0.2685,  0.5398,\n",
       "                       0.3851,  0.6192,  0.5736,  0.3231,  0.3176,  0.0767,  0.1798,  0.6714,\n",
       "                      -0.5030, -0.5793, -0.0419,  0.2964, -0.5639, -0.3600,  0.0204, -0.5722,\n",
       "                       0.5008,  0.4811, -0.2528,  0.3742, -0.2363,  0.0465, -0.6365, -0.5986,\n",
       "                       0.4601, -0.5981,  0.4758, -0.6400,  0.6593, -0.6161,  0.2907,  0.5927,\n",
       "                       0.3662, -0.2453, -0.5317, -0.1103,  0.3737, -0.3201,  0.5386,  0.2726,\n",
       "                      -0.0799, -0.1086,  0.6380, -0.6601,  0.2407, -0.6259,  0.6895,  0.5947,\n",
       "                      -0.1003, -0.3357,  0.5107, -0.4977,  0.2178,  0.5930,  0.2610,  0.6788,\n",
       "                      -0.4414, -0.5462, -0.5163,  0.3277,  0.4815, -0.2858,  0.2479,  0.1441,\n",
       "                      -0.0296, -0.3971,  0.4863,  0.6361, -0.4585, -0.1712, -0.3674, -0.1302,\n",
       "                      -0.3013,  0.2892,  0.2545,  0.6950,  0.0866,  0.0172, -0.6249,  0.0774,\n",
       "                       0.5449, -0.0498,  0.3793, -0.1045, -0.0816, -0.4768, -0.2719, -0.5926,\n",
       "                      -0.5350, -0.5197, -0.4019,  0.6204,  0.1639,  0.3388, -0.3188,  0.4366,\n",
       "                      -0.0843, -0.4426, -0.3850,  0.6404,  0.1450,  0.2172,  0.4629, -0.5557,\n",
       "                      -0.2263, -0.1064, -0.1643,  0.4926,  0.2762, -0.2068, -0.5144,  0.3175,\n",
       "                       0.3177,  0.3187, -0.5998, -0.4329,  0.0918, -0.2734,  0.7024,  0.3711,\n",
       "                       0.3103, -0.1983,  0.6916, -0.6767,  0.0059, -0.1530,  0.0928, -0.6912,\n",
       "                      -0.1872, -0.3836,  0.1463,  0.1134, -0.1014,  0.6160,  0.6692, -0.5411,\n",
       "                       0.6066, -0.3707,  0.6038,  0.5498, -0.2600,  0.5400, -0.2329, -0.6490,\n",
       "                       0.1260, -0.2027,  0.6065, -0.3431,  0.3219,  0.5050, -0.2449, -0.0077])),\n",
       "             ('hidden_layers.0.weight',\n",
       "              tensor([[ 0.0551,  0.0177, -0.0338,  ..., -0.0623, -0.0042, -0.0172],\n",
       "                      [-0.0052,  0.0586, -0.0278,  ..., -0.0564, -0.0500, -0.0266],\n",
       "                      [ 0.0486,  0.0444, -0.0147,  ..., -0.0118,  0.0625,  0.0626],\n",
       "                      ...,\n",
       "                      [ 0.0247, -0.0005, -0.0132,  ...,  0.0410,  0.0592, -0.0131],\n",
       "                      [-0.0214, -0.0469,  0.0286,  ..., -0.0138,  0.0119, -0.0529],\n",
       "                      [-0.0420,  0.0274,  0.0177,  ...,  0.0396,  0.0655,  0.0672]])),\n",
       "             ('hidden_layers.0.bias',\n",
       "              tensor([ 0.0531, -0.0304,  0.0059, -0.0025, -0.0218,  0.0487, -0.0562, -0.0054,\n",
       "                      -0.0204, -0.0239,  0.0160, -0.0274,  0.0412, -0.0277,  0.0260,  0.0467,\n",
       "                       0.0174, -0.0134, -0.0062, -0.0536, -0.0023, -0.0099,  0.0212,  0.0015,\n",
       "                       0.0333,  0.0441, -0.0590, -0.0096,  0.0519, -0.0408,  0.0421, -0.0035,\n",
       "                      -0.0531,  0.0411,  0.0198,  0.0075, -0.0517,  0.0530, -0.0323,  0.0555,\n",
       "                      -0.0349,  0.0339, -0.0429,  0.0154, -0.0037,  0.0037,  0.0096, -0.0550,\n",
       "                       0.0490, -0.0053, -0.0468, -0.0022,  0.0053,  0.0037, -0.0124,  0.0246,\n",
       "                      -0.0072,  0.0482,  0.0499,  0.0407, -0.0026,  0.0525,  0.0335,  0.0049,\n",
       "                      -0.0292, -0.0495, -0.0147, -0.0477,  0.0636, -0.0601,  0.0132, -0.0410,\n",
       "                      -0.0339, -0.0414, -0.0071, -0.0176, -0.0290, -0.0515, -0.0096, -0.0092,\n",
       "                      -0.0337,  0.0468,  0.0629,  0.0092, -0.0556, -0.0481,  0.0655,  0.0301,\n",
       "                      -0.0288, -0.0416,  0.0110, -0.0205, -0.0618, -0.0568, -0.0226,  0.0449,\n",
       "                      -0.0406, -0.0575, -0.0253, -0.0181, -0.0665,  0.0189, -0.0414,  0.0337,\n",
       "                      -0.0653,  0.0607, -0.0369, -0.0012,  0.0300,  0.0030, -0.0014, -0.0580,\n",
       "                       0.0252,  0.0086,  0.0058,  0.0193,  0.0564, -0.0095, -0.0292, -0.0181,\n",
       "                      -0.0567,  0.0386,  0.0080, -0.0019, -0.0283, -0.0659,  0.0015, -0.0604,\n",
       "                       0.0564, -0.0040,  0.0474, -0.0162,  0.0585, -0.0254,  0.0042,  0.0006,\n",
       "                      -0.0369,  0.0282,  0.0547,  0.0237,  0.0247, -0.0401, -0.0444, -0.0234,\n",
       "                       0.0547,  0.0232, -0.0315,  0.0179, -0.0699,  0.0311, -0.0239,  0.0327,\n",
       "                       0.0090,  0.0664, -0.0258, -0.0644, -0.0523,  0.0139,  0.0510,  0.0047,\n",
       "                      -0.0361,  0.0060, -0.0338,  0.0564,  0.0557, -0.0038,  0.0063, -0.0287,\n",
       "                       0.0496,  0.0450, -0.0035,  0.0596,  0.0426,  0.0410,  0.0087, -0.0529,\n",
       "                      -0.0009,  0.0200, -0.0520, -0.0476,  0.0198,  0.0288, -0.0218,  0.0378,\n",
       "                      -0.0450, -0.0107, -0.0031,  0.0118,  0.0445,  0.0568,  0.0440,  0.0104,\n",
       "                       0.0113, -0.0023, -0.0256, -0.0328, -0.0192,  0.0305,  0.0024,  0.0104])),\n",
       "             ('hidden_layers.1.weight',\n",
       "              tensor([[ 0.0453, -0.0694, -0.0669,  ..., -0.0007, -0.0474,  0.0702],\n",
       "                      [ 0.0689,  0.0700, -0.0381,  ..., -0.0378,  0.0186, -0.0455],\n",
       "                      [ 0.0085,  0.0224,  0.0158,  ..., -0.0443, -0.0522,  0.0352],\n",
       "                      ...,\n",
       "                      [ 0.0272, -0.0329, -0.0171,  ...,  0.0223,  0.0672, -0.0161],\n",
       "                      [ 0.0098,  0.0289, -0.0047,  ...,  0.0407, -0.0216, -0.0240],\n",
       "                      [ 0.0408,  0.0464,  0.0681,  ..., -0.0385, -0.0260,  0.0034]])),\n",
       "             ('hidden_layers.1.bias',\n",
       "              tensor([-0.0367, -0.0411, -0.0469, -0.0684,  0.0470,  0.0269,  0.0605,  0.0561,\n",
       "                      -0.0185,  0.0074, -0.0303, -0.0324, -0.0378,  0.0651, -0.0052,  0.0254,\n",
       "                      -0.0052,  0.0445, -0.0107,  0.0149, -0.0620,  0.0343, -0.0442,  0.0579,\n",
       "                       0.0044, -0.0270,  0.0304, -0.0191,  0.0146, -0.0272, -0.0180, -0.0142,\n",
       "                      -0.0405, -0.0295, -0.0490, -0.0385, -0.0566, -0.0537, -0.0705, -0.0537,\n",
       "                      -0.0508, -0.0463, -0.0505, -0.0632, -0.0397,  0.0461, -0.0253,  0.0632,\n",
       "                      -0.0229, -0.0290,  0.0466,  0.0599,  0.0457,  0.0509, -0.0325, -0.0422,\n",
       "                       0.0008,  0.0501, -0.0080, -0.0688,  0.0644, -0.0473, -0.0461, -0.0043,\n",
       "                      -0.0335,  0.0386, -0.0675, -0.0326, -0.0500, -0.0696,  0.0173, -0.0001,\n",
       "                       0.0544, -0.0462, -0.0662, -0.0681,  0.0230, -0.0119, -0.0557, -0.0270,\n",
       "                      -0.0491,  0.0388,  0.0558,  0.0406,  0.0224,  0.0281,  0.0556,  0.0183,\n",
       "                       0.0027, -0.0130, -0.0533,  0.0461, -0.0684,  0.0400,  0.0172,  0.0584,\n",
       "                       0.0700,  0.0464,  0.0321,  0.0416,  0.0165, -0.0632,  0.0312, -0.0335,\n",
       "                      -0.0069, -0.0248, -0.0218,  0.0125,  0.0598, -0.0170, -0.0353, -0.0707,\n",
       "                      -0.0281,  0.0259, -0.0184,  0.0390,  0.0517,  0.0003, -0.0368,  0.0479,\n",
       "                      -0.0343, -0.0086,  0.0216, -0.0381, -0.0091, -0.0253,  0.0147, -0.0345,\n",
       "                      -0.0209,  0.0561,  0.0311, -0.0653, -0.0501, -0.0190,  0.0016, -0.0597,\n",
       "                      -0.0024, -0.0407, -0.0416, -0.0494, -0.0385, -0.0088,  0.0038, -0.0607,\n",
       "                      -0.0437,  0.0015, -0.0347,  0.0422, -0.0124,  0.0317, -0.0687, -0.0505,\n",
       "                      -0.0024, -0.0484, -0.0527, -0.0221, -0.0650,  0.0511,  0.0330,  0.0003,\n",
       "                      -0.0509, -0.0384, -0.0641, -0.0544, -0.0021,  0.0105,  0.0280,  0.0317,\n",
       "                       0.0433,  0.0111,  0.0647, -0.0446,  0.0132, -0.0145, -0.0500,  0.0435,\n",
       "                       0.0653,  0.0513, -0.0446, -0.0615, -0.0668, -0.0165,  0.0315,  0.0567,\n",
       "                       0.0626, -0.0484,  0.0106, -0.0261, -0.0305, -0.0543,  0.0502, -0.0344,\n",
       "                      -0.0233, -0.0031,  0.0515,  0.0615,  0.0493, -0.0321, -0.0081,  0.0344])),\n",
       "             ('hidden_layers.2.weight',\n",
       "              tensor([[ 0.0598,  0.0390, -0.0248,  ...,  0.0162, -0.0497,  0.0107],\n",
       "                      [ 0.0467,  0.0115,  0.0508,  ..., -0.0620, -0.0463,  0.0240],\n",
       "                      [ 0.0297,  0.0430,  0.0104,  ..., -0.0435,  0.0438,  0.0322],\n",
       "                      ...,\n",
       "                      [ 0.0096,  0.0505, -0.0698,  ...,  0.0118,  0.0427,  0.0262],\n",
       "                      [-0.0338, -0.0533,  0.0017,  ..., -0.0685, -0.0111, -0.0350],\n",
       "                      [-0.0389, -0.0206,  0.0599,  ...,  0.0117,  0.0531,  0.0230]])),\n",
       "             ('hidden_layers.2.bias',\n",
       "              tensor([-0.0196, -0.0686, -0.0234,  0.0531, -0.0594,  0.0202,  0.0238, -0.0164,\n",
       "                       0.0144,  0.0195, -0.0565, -0.0399,  0.0606, -0.0634,  0.0670,  0.0552,\n",
       "                      -0.0446,  0.0468,  0.0623, -0.0122, -0.0065, -0.0332,  0.0123,  0.0654,\n",
       "                       0.0288,  0.0288, -0.0122,  0.0414,  0.0101,  0.0626, -0.0033, -0.0079,\n",
       "                       0.0443,  0.0117,  0.0302,  0.0228, -0.0311,  0.0291,  0.0486, -0.0602,\n",
       "                      -0.0663, -0.0604,  0.0536, -0.0194,  0.0067,  0.0551,  0.0151,  0.0360,\n",
       "                      -0.0690, -0.0435,  0.0221,  0.0511, -0.0484, -0.0632,  0.0222,  0.0543,\n",
       "                       0.0015, -0.0082, -0.0370,  0.0173,  0.0374,  0.0623,  0.0223, -0.0305,\n",
       "                       0.0085, -0.0273, -0.0338,  0.0640, -0.0381,  0.0442, -0.0655, -0.0211,\n",
       "                       0.0072,  0.0005,  0.0323, -0.0279,  0.0506,  0.0612, -0.0456, -0.0678,\n",
       "                      -0.0174, -0.0323,  0.0201, -0.0227,  0.0507, -0.0324,  0.0014,  0.0250,\n",
       "                       0.0047,  0.0357,  0.0077,  0.0224, -0.0678,  0.0617,  0.0004, -0.0074,\n",
       "                      -0.0438, -0.0256,  0.0090,  0.0240, -0.0302, -0.0133, -0.0119,  0.0488,\n",
       "                       0.0134,  0.0478, -0.0339,  0.0379, -0.0133, -0.0374,  0.0086, -0.0210,\n",
       "                       0.0017,  0.0357, -0.0508, -0.0605,  0.0201, -0.0068,  0.0346, -0.0433,\n",
       "                      -0.0229,  0.0244,  0.0449, -0.0212,  0.0561,  0.0461, -0.0596,  0.0409,\n",
       "                       0.0217, -0.0234, -0.0703, -0.0451,  0.0344, -0.0355,  0.0497, -0.0186,\n",
       "                      -0.0067,  0.0530,  0.0308,  0.0417, -0.0090, -0.0556,  0.0300,  0.0639,\n",
       "                      -0.0529,  0.0136,  0.0019, -0.0079, -0.0273, -0.0616, -0.0322, -0.0578,\n",
       "                       0.0324,  0.0294,  0.0387, -0.0089,  0.0518,  0.0282, -0.0311, -0.0386,\n",
       "                      -0.0678,  0.0474, -0.0272, -0.0218, -0.0119,  0.0291,  0.0018,  0.0039,\n",
       "                       0.0341,  0.0041, -0.0142,  0.0511, -0.0540, -0.0647,  0.0452, -0.0056,\n",
       "                      -0.0208, -0.0036, -0.0180,  0.0627,  0.0018,  0.0138,  0.0278, -0.0164,\n",
       "                      -0.0425,  0.0536, -0.0245,  0.0002, -0.0260,  0.0400,  0.0676,  0.0513,\n",
       "                       0.0013, -0.0173,  0.0295, -0.0060, -0.0335,  0.0008,  0.0658, -0.0619])),\n",
       "             ('hidden_layers.3.weight',\n",
       "              tensor([[-0.0695, -0.0704, -0.0507,  ...,  0.0176,  0.0424, -0.0586],\n",
       "                      [ 0.0228, -0.0468, -0.0064,  ...,  0.0404, -0.0103,  0.0209],\n",
       "                      [ 0.0259, -0.0158,  0.0471,  ...,  0.0661,  0.0381, -0.0398],\n",
       "                      ...,\n",
       "                      [-0.0179,  0.0353,  0.0377,  ..., -0.0534, -0.0602, -0.0453],\n",
       "                      [-0.0365, -0.0307, -0.0560,  ..., -0.0528,  0.0183,  0.0335],\n",
       "                      [ 0.0020,  0.0356, -0.0426,  ..., -0.0475, -0.0065, -0.0057]])),\n",
       "             ('hidden_layers.3.bias',\n",
       "              tensor([-9.4308e-03,  6.8854e-02,  1.1240e-02,  4.1728e-03, -3.0191e-02,\n",
       "                       6.4124e-02, -2.2824e-02,  1.5327e-03, -3.2095e-02,  1.9315e-02,\n",
       "                      -1.2648e-02, -4.0452e-03,  1.5693e-02,  1.7275e-02, -4.1602e-02,\n",
       "                      -4.9013e-02, -3.7576e-02,  5.1132e-02, -6.2618e-02,  5.7422e-02,\n",
       "                      -5.1210e-02,  5.3188e-02,  3.4169e-02, -1.9343e-02, -6.0774e-02,\n",
       "                       7.5187e-03, -6.4740e-02, -3.6009e-02,  6.8782e-02, -2.6278e-02,\n",
       "                      -3.1014e-02, -3.3481e-02, -1.0383e-02, -3.1881e-02,  2.8550e-02,\n",
       "                       5.2455e-02, -5.6339e-02,  5.4465e-02,  1.5228e-02,  8.4492e-03,\n",
       "                      -3.3043e-02, -4.6688e-02,  3.8107e-02, -4.4000e-02, -2.3064e-02,\n",
       "                       5.0558e-03,  2.8964e-02, -2.8500e-02,  3.8712e-02,  5.9188e-02,\n",
       "                       2.6090e-02, -1.7228e-02,  5.8120e-02,  5.1438e-02, -6.5724e-05,\n",
       "                      -1.4534e-02,  5.0104e-02,  4.1235e-02,  3.8479e-02,  4.8369e-02,\n",
       "                      -7.8895e-03, -3.4516e-02, -4.3233e-04, -5.8988e-02, -6.8092e-02,\n",
       "                      -2.0905e-02, -4.6534e-03, -5.5392e-02,  6.0652e-02, -1.5282e-02,\n",
       "                      -6.3457e-02, -2.0325e-02,  6.0439e-02, -5.1676e-02,  4.1972e-03,\n",
       "                      -6.8574e-02,  6.6865e-02,  1.3178e-02,  2.1348e-02, -1.3814e-02,\n",
       "                       9.3879e-03, -4.8881e-02,  1.9264e-02,  6.4313e-02,  5.1865e-02,\n",
       "                      -2.8177e-02,  6.0343e-02, -1.3022e-02,  7.0479e-02,  7.0095e-02,\n",
       "                       3.8315e-02, -2.2732e-02,  9.2424e-04,  3.5948e-02, -2.6449e-02,\n",
       "                       6.6120e-02, -1.3960e-02, -1.3629e-02,  1.1778e-02, -6.1278e-02,\n",
       "                      -6.2884e-02,  4.9894e-02, -3.0673e-02,  2.6136e-02, -1.4076e-02,\n",
       "                       2.3444e-02, -4.1622e-02, -4.9202e-02, -5.1643e-02, -3.4001e-02,\n",
       "                      -4.1516e-02, -2.4636e-02, -1.0326e-02,  1.7479e-02,  2.7684e-02,\n",
       "                       1.6292e-02,  5.2289e-02, -5.4949e-02, -5.6630e-02,  3.1735e-02,\n",
       "                      -4.4406e-03,  2.4004e-02, -6.1095e-02, -9.3007e-03, -6.6968e-02,\n",
       "                       4.1519e-02, -2.8842e-03, -3.4644e-02, -6.9217e-02,  5.2096e-02,\n",
       "                      -3.7666e-02,  9.9041e-03,  3.8929e-02,  3.1799e-03,  1.5850e-02,\n",
       "                       3.6606e-02,  4.1569e-02, -6.2982e-02, -6.7157e-02, -2.1715e-02,\n",
       "                      -6.9389e-02,  5.5365e-02,  8.4312e-03,  5.9062e-02,  2.6134e-02,\n",
       "                      -6.9003e-02, -3.9323e-02, -5.3424e-02,  1.5517e-02, -5.5653e-02,\n",
       "                       1.0907e-02,  2.3858e-02,  5.4929e-02,  1.0614e-02,  9.5429e-03,\n",
       "                      -3.4082e-02,  4.7775e-02, -4.9424e-02,  6.2804e-02, -1.7293e-02,\n",
       "                       4.3546e-02,  2.0187e-02,  6.2576e-02, -1.8871e-02,  4.7002e-05,\n",
       "                       3.3018e-02, -2.8565e-02,  3.9153e-02,  9.2901e-03, -4.2764e-02,\n",
       "                      -3.8220e-02, -5.4226e-02,  4.2826e-02,  4.7452e-02, -2.1164e-02,\n",
       "                       6.7970e-02,  3.1000e-03,  5.6116e-02, -2.0727e-02, -3.3187e-02,\n",
       "                       1.6374e-02, -4.3162e-02, -6.8735e-02, -4.7926e-02, -2.0028e-02,\n",
       "                      -2.0347e-02,  2.9063e-02, -3.8200e-02, -6.4514e-02, -1.7710e-02,\n",
       "                       4.4507e-02, -2.7487e-02,  3.4948e-02,  9.8280e-03, -2.3741e-02,\n",
       "                      -1.9902e-02,  2.2827e-02,  1.9484e-02, -2.6168e-02, -1.9025e-02])),\n",
       "             ('output_layer.weight',\n",
       "              tensor([[ 0.0162, -0.0312, -0.0286,  0.0365,  0.0670,  0.0695, -0.0634,  0.0134,\n",
       "                       -0.0307,  0.0399, -0.0255, -0.0462, -0.0282, -0.0245,  0.0612,  0.0564,\n",
       "                       -0.0674,  0.0306,  0.0278, -0.0402, -0.0558,  0.0518, -0.0216,  0.0214,\n",
       "                        0.0626,  0.0392, -0.0630,  0.0119, -0.0554,  0.0169,  0.0233, -0.0695,\n",
       "                       -0.0290, -0.0382, -0.0431, -0.0592, -0.0200, -0.0259, -0.0105, -0.0235,\n",
       "                        0.0274, -0.0647,  0.0115, -0.0446,  0.0454, -0.0257, -0.0215, -0.0549,\n",
       "                        0.0258,  0.0251,  0.0592,  0.0097,  0.0525,  0.0285,  0.0082,  0.0333,\n",
       "                       -0.0273, -0.0506,  0.0682, -0.0441,  0.0019,  0.0615,  0.0389,  0.0487,\n",
       "                       -0.0025, -0.0208, -0.0204, -0.0336,  0.0015,  0.0437,  0.0158,  0.0530,\n",
       "                        0.0167, -0.0592, -0.0184,  0.0256,  0.0633, -0.0598,  0.0448, -0.0375,\n",
       "                        0.0485, -0.0221, -0.0145,  0.0553,  0.0550, -0.0192,  0.0456, -0.0286,\n",
       "                        0.0550,  0.0240,  0.0098, -0.0515,  0.0703, -0.0147, -0.0558,  0.0168,\n",
       "                       -0.0487,  0.0300, -0.0319,  0.0250,  0.0591, -0.0326,  0.0093, -0.0327,\n",
       "                        0.0114, -0.0610, -0.0272, -0.0308,  0.0544, -0.0049,  0.0048, -0.0267,\n",
       "                       -0.0262, -0.0065,  0.0101,  0.0232, -0.0170, -0.0627,  0.0352,  0.0052,\n",
       "                       -0.0378, -0.0502,  0.0555, -0.0682,  0.0525,  0.0282, -0.0593, -0.0164,\n",
       "                       -0.0469,  0.0372, -0.0215,  0.0162,  0.0460,  0.0276, -0.0611,  0.0508,\n",
       "                        0.0424,  0.0409,  0.0131,  0.0281,  0.0563, -0.0486,  0.0641, -0.0504,\n",
       "                        0.0528, -0.0411, -0.0272,  0.0078, -0.0414, -0.0617, -0.0419, -0.0125,\n",
       "                       -0.0042,  0.0318,  0.0028, -0.0542, -0.0084, -0.0238, -0.0118, -0.0629,\n",
       "                       -0.0078, -0.0609,  0.0201,  0.0004,  0.0343,  0.0137,  0.0645,  0.0645,\n",
       "                        0.0549, -0.0547,  0.0072, -0.0364, -0.0131,  0.0624,  0.0385, -0.0012,\n",
       "                        0.0531,  0.0219, -0.0243,  0.0023,  0.0405, -0.0351, -0.0356,  0.0285,\n",
       "                       -0.0400,  0.0541, -0.0457, -0.0279,  0.0473, -0.0507, -0.0323,  0.0707,\n",
       "                        0.0038,  0.0682, -0.0104, -0.0068,  0.0487, -0.0102,  0.0580,  0.0440]])),\n",
       "             ('output_layer.bias', tensor([-0.0531]))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_network.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f3c9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_network = NeuralNet(input_dimension=x.shape[1], output_dimension=y.shape[1], n_hidden_layers=4, neurons=200)\n",
    "\n",
    "# Load saved parameters for first two hidden layers\n",
    "params = torch.load(\"my_network_3_EB_layers.pt\")\n",
    "my_network.input_layer.weight.data = params[0].data\n",
    "my_network.input_layer.bias.data = params[1].data\n",
    "for i in range(3):\n",
    "    my_network.hidden_layers[i].weight.data = params[2*i+2].data\n",
    "    my_network.hidden_layers[i].bias.data = params[2*i+3].data\n",
    "\n",
    "# Freeze parameters of first two hidden layers\n",
    "for param in my_network.input_layer.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in my_network.hidden_layers.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3712353c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('input_layer.weight',\n",
       "              tensor([[-2.8867e-01,  2.1617e-01],\n",
       "                      [ 2.1635e-02, -1.7316e-01],\n",
       "                      [ 1.3589e-01, -4.0220e-01],\n",
       "                      [ 3.3085e-01, -3.0147e-01],\n",
       "                      [ 9.4485e-02,  6.7596e-01],\n",
       "                      [ 4.8037e-02,  1.5047e-01],\n",
       "                      [ 1.7420e-02, -1.2656e-02],\n",
       "                      [ 2.6476e-01, -2.8099e-01],\n",
       "                      [-1.6313e-01,  3.2822e-01],\n",
       "                      [ 1.5153e-01,  2.0016e-02],\n",
       "                      [ 3.1241e-01,  1.2560e-01],\n",
       "                      [-1.2882e-03,  1.3924e-02],\n",
       "                      [-2.9341e-02, -1.0869e-01],\n",
       "                      [ 4.2156e-01, -3.3039e-02],\n",
       "                      [-3.7150e-02,  1.6494e-01],\n",
       "                      [-2.8273e-01,  1.9943e-01],\n",
       "                      [-9.0187e-02, -1.0217e-01],\n",
       "                      [ 1.6243e-01, -2.4843e-01],\n",
       "                      [-2.6768e-02,  2.3992e-02],\n",
       "                      [-3.5832e-03, -3.6459e-01],\n",
       "                      [ 4.6498e-01,  1.9718e-01],\n",
       "                      [ 7.2692e-02,  2.9311e-01],\n",
       "                      [-3.1350e-01, -8.2516e-03],\n",
       "                      [-6.3186e-02, -2.4927e-01],\n",
       "                      [-2.8184e-01,  8.1370e-02],\n",
       "                      [ 3.0844e-02, -3.1635e-01],\n",
       "                      [ 3.0929e-02,  2.1060e-01],\n",
       "                      [ 7.5540e-02,  3.3052e-01],\n",
       "                      [-4.0240e-02, -1.2438e-01],\n",
       "                      [-2.0686e-01,  2.6395e-02],\n",
       "                      [ 1.4512e-01, -3.6718e-01],\n",
       "                      [ 2.8406e-01,  1.7301e-01],\n",
       "                      [-1.9123e-02,  3.0728e-01],\n",
       "                      [-2.2202e-02, -6.4514e-03],\n",
       "                      [-1.3281e-01, -7.4291e-02],\n",
       "                      [-2.6175e-01, -1.1558e-01],\n",
       "                      [-3.2532e-02, -2.1378e-01],\n",
       "                      [-5.6794e-02,  6.6944e-02],\n",
       "                      [-1.4606e-01, -2.7259e-01],\n",
       "                      [ 2.7718e-02, -2.4937e-01],\n",
       "                      [ 4.7628e-01,  4.2576e-03],\n",
       "                      [-1.6305e-02,  1.7546e-01],\n",
       "                      [ 2.6238e-02, -5.5113e-01],\n",
       "                      [-3.4372e-02,  5.9709e-02],\n",
       "                      [-9.8802e-02, -6.2963e-02],\n",
       "                      [-1.0513e-01,  1.5829e-01],\n",
       "                      [-3.0646e-01, -1.8775e-01],\n",
       "                      [-2.3739e-01,  1.2387e-01],\n",
       "                      [-1.1416e-01,  1.7131e-01],\n",
       "                      [-5.9335e-02,  4.7420e-03],\n",
       "                      [-3.6005e-02, -2.0754e-01],\n",
       "                      [-9.1216e-02,  1.2741e-01],\n",
       "                      [-1.0939e-01, -6.3282e-01],\n",
       "                      [-9.5100e-02, -3.0893e-01],\n",
       "                      [ 3.3333e-01, -3.4208e-02],\n",
       "                      [ 1.1731e-01,  1.7086e-01],\n",
       "                      [-6.8881e-02, -8.2503e-02],\n",
       "                      [-2.3585e-01, -1.5801e-01],\n",
       "                      [ 1.5790e-02, -3.9694e-01],\n",
       "                      [ 2.5868e-01,  1.1258e-01],\n",
       "                      [-2.8829e-02,  2.9623e-01],\n",
       "                      [ 4.4064e-01,  2.0890e-01],\n",
       "                      [-1.4628e-01, -1.8194e-01],\n",
       "                      [-1.6768e-02, -5.4282e-01],\n",
       "                      [-4.4631e-02, -1.3465e-01],\n",
       "                      [ 5.8991e-02, -4.5756e-01],\n",
       "                      [ 1.1417e-01,  4.9885e-01],\n",
       "                      [-4.1410e-01, -2.0217e-01],\n",
       "                      [-6.1487e-02, -2.3772e-01],\n",
       "                      [ 4.7831e-02,  6.6133e-02],\n",
       "                      [ 4.6623e-01,  2.2866e-01],\n",
       "                      [ 3.2589e-02, -1.7600e-01],\n",
       "                      [ 8.7284e-03,  2.4548e-01],\n",
       "                      [-3.9253e-02,  2.1957e-01],\n",
       "                      [ 2.3322e-01,  1.6981e-01],\n",
       "                      [ 1.0354e-01, -4.3028e-01],\n",
       "                      [ 4.3934e-02,  4.2741e-01],\n",
       "                      [-4.6327e-01, -2.5477e-01],\n",
       "                      [ 2.4621e-02,  1.4408e-01],\n",
       "                      [-1.3146e-01, -1.9079e-01],\n",
       "                      [-9.1755e-02,  6.2319e-01],\n",
       "                      [-4.5664e-03, -2.3589e-02],\n",
       "                      [-9.4396e-03,  3.2891e-01],\n",
       "                      [-7.4041e-03, -3.6519e-01],\n",
       "                      [ 1.4929e-01, -1.6883e-01],\n",
       "                      [ 3.1501e-01,  3.5829e-01],\n",
       "                      [-2.5014e-01, -3.0920e-01],\n",
       "                      [-1.3405e-01, -1.8314e-01],\n",
       "                      [-1.1270e-01,  3.3037e-01],\n",
       "                      [-9.4502e-02,  2.8110e-01],\n",
       "                      [ 7.6105e-02, -3.3334e-01],\n",
       "                      [ 3.2275e-01,  1.1120e-01],\n",
       "                      [-3.6575e-01, -2.2008e-01],\n",
       "                      [-9.6025e-02, -1.8639e-01],\n",
       "                      [-8.3378e-03, -2.9692e-01],\n",
       "                      [ 3.8765e-01,  2.9695e-01],\n",
       "                      [-1.9882e-01, -3.0480e-01],\n",
       "                      [-8.6165e-03,  1.5818e-01],\n",
       "                      [-1.2888e-01,  2.0233e-01],\n",
       "                      [-7.8402e-02,  6.3929e-02],\n",
       "                      [-1.1444e-01, -1.8224e-01],\n",
       "                      [ 1.5261e-01,  1.5353e-01],\n",
       "                      [ 4.0841e-02, -3.0157e-02],\n",
       "                      [-2.6280e-02, -1.8822e-01],\n",
       "                      [-9.4362e-02, -1.7506e-01],\n",
       "                      [ 2.3348e-02, -4.5616e-02],\n",
       "                      [ 2.4157e-01,  2.8048e-01],\n",
       "                      [ 2.4905e-01, -2.1689e-01],\n",
       "                      [-2.3419e-01,  1.2454e-01],\n",
       "                      [ 2.8305e-02, -2.9048e-01],\n",
       "                      [-4.1754e-02,  1.9034e-01],\n",
       "                      [-2.2627e-01, -2.3418e-01],\n",
       "                      [-3.7607e-02,  6.4252e-02],\n",
       "                      [ 4.4075e-02, -2.5236e-01],\n",
       "                      [-4.8613e-02,  3.0980e-01],\n",
       "                      [-3.5833e-02,  9.9740e-02],\n",
       "                      [-1.8814e-02, -1.4784e-01],\n",
       "                      [ 9.9843e-02, -3.1998e-01],\n",
       "                      [ 1.1875e-01, -2.5084e-01],\n",
       "                      [-3.7998e-01, -1.4078e-01],\n",
       "                      [ 2.2392e-02,  3.2690e-01],\n",
       "                      [ 6.4514e-02, -2.5892e-01],\n",
       "                      [-2.2460e-01,  5.0526e-02],\n",
       "                      [ 4.9397e-02, -2.9837e-03],\n",
       "                      [-6.7973e-02,  1.6549e-01],\n",
       "                      [ 4.8300e-01,  2.8938e-01],\n",
       "                      [ 4.5276e-01,  3.3805e-01],\n",
       "                      [-1.7848e-01,  2.4119e-01],\n",
       "                      [-3.1160e-01, -8.3120e-02],\n",
       "                      [ 4.1978e-01,  7.2025e-02],\n",
       "                      [ 2.4269e-02,  4.2806e-02],\n",
       "                      [ 1.6262e-02,  3.3993e-01],\n",
       "                      [-3.9217e-02, -2.1929e-02],\n",
       "                      [-7.2582e-02, -3.6168e-01],\n",
       "                      [ 2.8581e-02,  3.6118e-02],\n",
       "                      [ 2.8575e-01,  2.4000e-01],\n",
       "                      [ 3.2825e-02, -1.2068e-01],\n",
       "                      [ 1.7134e-01,  8.0932e-02],\n",
       "                      [-1.3659e-01, -3.7583e-02],\n",
       "                      [ 1.6367e-01,  1.6337e-01],\n",
       "                      [-2.4537e-01, -2.6936e-01],\n",
       "                      [-1.4744e-01,  6.3424e-03],\n",
       "                      [ 3.8359e-02,  9.5480e-02],\n",
       "                      [-3.7942e-02,  1.2213e-01],\n",
       "                      [-4.2206e-02, -2.2864e-01],\n",
       "                      [ 1.0954e-01, -2.4449e-01],\n",
       "                      [-5.8617e-02, -1.0413e-03],\n",
       "                      [-2.2031e-01, -2.2482e-02],\n",
       "                      [-1.9972e-02, -1.9440e-02],\n",
       "                      [ 8.4749e-02,  3.4601e-02],\n",
       "                      [ 2.8551e-02,  1.1509e-01],\n",
       "                      [ 1.1073e-01, -4.6713e-01],\n",
       "                      [ 1.9196e-02, -3.0658e-01],\n",
       "                      [-1.3548e-01, -2.4312e-01],\n",
       "                      [ 1.9419e-03,  1.5938e-01],\n",
       "                      [-1.5016e-01,  8.5101e-02],\n",
       "                      [-3.9010e-02,  1.3909e-01],\n",
       "                      [ 2.6556e-01,  2.2898e-01],\n",
       "                      [ 3.2736e-01, -5.6556e-02],\n",
       "                      [-2.4682e-02, -2.5014e-01],\n",
       "                      [-1.1585e-01,  1.7662e-01],\n",
       "                      [ 1.3892e-01, -6.5572e-01],\n",
       "                      [ 6.2874e-02, -4.6900e-02],\n",
       "                      [-5.1693e-02, -1.4331e-01],\n",
       "                      [-5.4871e-04,  3.1490e-01],\n",
       "                      [ 8.7286e-02, -7.0365e-02],\n",
       "                      [-2.2731e-01, -1.1048e-02],\n",
       "                      [-2.4272e-02, -4.9468e-01],\n",
       "                      [-1.3934e-01, -3.4035e-01],\n",
       "                      [-4.2900e-02, -3.6755e-01],\n",
       "                      [-3.7508e-02, -2.1726e-01],\n",
       "                      [-2.5034e-01,  2.5950e-01],\n",
       "                      [-3.4280e-01, -1.4179e-02],\n",
       "                      [-4.6941e-02,  2.0036e-02],\n",
       "                      [ 2.3305e-01,  7.5469e-04],\n",
       "                      [ 1.2994e-03, -1.4716e-01],\n",
       "                      [-2.2574e-01,  2.1557e-01],\n",
       "                      [-1.8665e-01,  3.0595e-01],\n",
       "                      [-3.4345e-02,  2.2665e-01],\n",
       "                      [ 8.6815e-02,  2.4025e-01],\n",
       "                      [-3.7667e-02, -3.7456e-01],\n",
       "                      [ 2.2601e-02,  9.9289e-02],\n",
       "                      [-1.2413e-02, -3.3107e-01],\n",
       "                      [-2.6962e-02,  1.7774e-01],\n",
       "                      [ 4.8206e-01,  1.7412e-01],\n",
       "                      [-1.2586e-01,  2.7618e-01],\n",
       "                      [-1.9242e-01,  7.8400e-03],\n",
       "                      [ 1.5393e-01,  2.1164e-01],\n",
       "                      [-1.2238e-01, -2.1279e-01],\n",
       "                      [ 3.0598e-01,  1.6220e-01],\n",
       "                      [-3.2858e-01, -2.3856e-01],\n",
       "                      [-9.6563e-02, -1.4897e-01],\n",
       "                      [-9.4273e-02,  3.9008e-01],\n",
       "                      [-5.4740e-01,  2.2137e-01],\n",
       "                      [-2.1388e-02, -2.0954e-01],\n",
       "                      [-1.4646e-01,  5.1353e-01],\n",
       "                      [ 2.1484e-01,  1.1449e-01],\n",
       "                      [-3.6185e-01, -2.5896e-01],\n",
       "                      [-3.9697e-01, -4.4289e-01],\n",
       "                      [-7.0913e-02, -3.3404e-01]])),\n",
       "             ('input_layer.bias',\n",
       "              tensor([ 1.7865e-01, -1.3582e-01,  1.0910e-01,  1.4094e-02,  1.6340e-01,\n",
       "                      -6.1595e-01, -2.4404e-01,  2.5568e-02,  2.2112e-01,  2.9918e-01,\n",
       "                      -5.7333e-02,  1.4877e-01,  1.8980e-01, -2.5915e-02,  7.0527e-02,\n",
       "                      -1.3364e-01,  1.2773e-01, -1.0795e-01,  8.7504e-02,  2.6548e-01,\n",
       "                       3.5656e-01, -1.4186e-01, -3.3384e-01,  3.0341e-01, -2.4277e-02,\n",
       "                      -1.9546e-01, -2.5239e-01,  9.4159e-02, -8.0113e-03, -2.6509e-01,\n",
       "                      -3.9228e-02,  7.7332e-02,  1.9758e-01,  2.8012e-01,  1.5099e-01,\n",
       "                       3.3415e-01,  3.1721e-01,  3.4243e-01, -1.1051e-02, -2.8050e-01,\n",
       "                       2.0468e-01,  2.5668e-02, -1.5741e-01,  4.5349e-01, -1.3699e-01,\n",
       "                      -4.2639e-02,  7.2242e-02, -1.6164e-01,  3.1212e-01, -9.0125e-05,\n",
       "                       1.9362e-01,  2.4849e-01,  1.0267e-01, -1.9240e-01,  3.5278e-01,\n",
       "                      -3.8800e-02,  3.9612e-01, -1.8817e-01,  4.7500e-02,  7.8041e-02,\n",
       "                       3.6491e-01,  2.7550e-01, -1.5948e-01,  1.1486e-02, -8.9751e-02,\n",
       "                      -3.1276e-02,  2.2311e-01, -9.2226e-02,  1.1458e-01,  5.2599e-03,\n",
       "                       1.7837e-01, -1.7141e-01, -3.7418e-01,  1.1103e-01,  1.5804e-01,\n",
       "                      -8.8020e-02, -3.2649e-01, -1.6711e-01, -3.1972e-01, -4.7763e-02,\n",
       "                      -5.3402e-03,  1.1397e-01,  1.3593e-01, -9.4066e-02,  6.2868e-02,\n",
       "                       2.4105e-01,  5.9373e-03, -2.1994e-01,  1.5597e-01,  2.1739e-02,\n",
       "                      -2.8740e-02,  3.1439e-01, -6.1330e-02,  2.3962e-01,  2.0131e-01,\n",
       "                       2.4778e-01, -1.4108e-01,  1.5528e-01,  5.3251e-02,  7.7225e-02,\n",
       "                      -2.4427e-01,  4.4594e-01, -4.3802e-01, -6.5870e-02,  2.3842e-01,\n",
       "                      -1.0189e-01, -1.4025e-01,  6.0503e-02, -2.2616e-01, -2.8282e-01,\n",
       "                       4.3077e-01, -1.3487e-01,  1.8337e-01, -7.6649e-02, -5.6110e-02,\n",
       "                       2.8684e-01,  1.8145e-01, -4.7870e-02, -7.8137e-02, -3.5739e-02,\n",
       "                       1.1891e-02, -1.6022e-01, -1.1705e-01, -2.1470e-01,  7.5430e-02,\n",
       "                       2.3036e-01,  2.4640e-01, -1.6375e-01, -2.4163e-01,  1.6846e-01,\n",
       "                      -1.7199e-01, -1.0501e-01,  1.9979e-01,  2.2916e-01, -3.5246e-01,\n",
       "                       1.9121e-02, -2.9237e-01, -4.2143e-04, -2.2514e-02,  7.7313e-02,\n",
       "                       5.7393e-02,  3.7209e-02, -4.5615e-02,  4.4217e-01,  3.1279e-01,\n",
       "                      -1.2092e-01, -8.7360e-02, -2.2571e-01,  1.4126e-01, -7.2321e-02,\n",
       "                      -1.0718e-02, -2.4311e-01,  2.1807e-01, -7.5316e-02, -1.6076e-01,\n",
       "                      -1.1726e-01,  3.8473e-01,  2.6947e-01,  4.5999e-02,  2.9634e-01,\n",
       "                       8.2713e-02, -1.1341e-01, -1.9112e-01,  4.1138e-01,  2.4260e-02,\n",
       "                       1.7922e-01, -7.7698e-02, -1.4781e-01, -1.9067e-01, -3.2039e-02,\n",
       "                       1.0850e-01, -1.5416e-01, -1.3515e-01,  1.5715e-01,  1.9230e-01,\n",
       "                       1.9343e-01, -9.8800e-02,  1.0731e-01,  1.3396e-01, -1.1533e-01,\n",
       "                      -2.1377e-02, -1.9465e-01,  2.9517e-01,  1.9366e-01,  2.1007e-01,\n",
       "                      -2.9370e-02,  1.0709e-01, -9.8048e-03, -2.4496e-01,  3.8732e-01,\n",
       "                       1.1967e-01,  1.9349e-01, -1.7347e-01, -1.9724e-01,  2.5820e-01,\n",
       "                       5.1397e-01,  1.2624e-01, -1.6281e-01, -9.8929e-02,  4.2574e-01])),\n",
       "             ('hidden_layers.0.weight',\n",
       "              tensor([[ 0.1265,  0.1227, -0.1929,  ...,  0.1885,  0.0444, -0.1090],\n",
       "                      [ 0.0221, -0.1102, -0.0909,  ...,  0.1240, -0.0052,  0.0170],\n",
       "                      [-0.0672, -0.0348, -0.2703,  ...,  0.1172,  0.0941,  0.0295],\n",
       "                      ...,\n",
       "                      [ 0.1308, -0.0410,  0.0716,  ..., -0.1626,  0.0851, -0.0541],\n",
       "                      [ 0.0997, -0.1588, -0.1841,  ..., -0.0861, -0.0950, -0.1568],\n",
       "                      [ 0.1129,  0.1186,  0.1057,  ..., -0.0216, -0.1126, -0.0824]])),\n",
       "             ('hidden_layers.0.bias',\n",
       "              tensor([-0.0389, -0.0100,  0.0947, -0.0612,  0.1698,  0.0749,  0.0163,  0.0399,\n",
       "                      -0.0089,  0.0881,  0.0534,  0.1441,  0.0526, -0.0513, -0.0234,  0.0628,\n",
       "                       0.1137,  0.1148,  0.0849, -0.0377,  0.0394,  0.1466,  0.0605,  0.1050,\n",
       "                       0.0859,  0.0899, -0.1088, -0.0221, -0.0968, -0.0232, -0.0091, -0.1621,\n",
       "                      -0.0662, -0.0730, -0.0592,  0.0295, -0.0587, -0.0435, -0.0417, -0.1510,\n",
       "                      -0.0234,  0.0055, -0.2798,  0.0245,  0.0736, -0.1010, -0.0052,  0.1130,\n",
       "                       0.2030, -0.0449,  0.0713,  0.0793,  0.1596,  0.0625, -0.0234, -0.1926,\n",
       "                       0.0238,  0.0734,  0.0819, -0.0219,  0.1944, -0.1027,  0.0106,  0.0205,\n",
       "                       0.0722,  0.0969, -0.0494,  0.0183, -0.1068, -0.0794, -0.0326,  0.0495,\n",
       "                      -0.2363, -0.0126,  0.0430,  0.1529,  0.1091,  0.0853, -0.1025, -0.0450,\n",
       "                      -0.1481,  0.1441,  0.2197, -0.0794,  0.1391,  0.1237, -0.0865, -0.1204,\n",
       "                       0.0413, -0.0929, -0.0702, -0.0457,  0.0263, -0.0058,  0.0549,  0.0199,\n",
       "                       0.1167,  0.0811, -0.0427, -0.1881,  0.0299, -0.1833, -0.0232, -0.1121,\n",
       "                      -0.0080, -0.1118,  0.0507,  0.1023,  0.1077,  0.0322, -0.0115, -0.0389,\n",
       "                       0.0500, -0.0470,  0.0847, -0.0704,  0.1194,  0.1049, -0.0252,  0.0799,\n",
       "                      -0.0115, -0.0345, -0.0190, -0.1124, -0.0761, -0.0989, -0.0338,  0.1182,\n",
       "                      -0.2132,  0.0330, -0.2862,  0.0178,  0.0990,  0.1622,  0.1396,  0.0764,\n",
       "                       0.0054,  0.1066, -0.1285,  0.0295, -0.0915, -0.0454,  0.2065, -0.0308,\n",
       "                       0.0567,  0.0235, -0.0309,  0.0830, -0.0837,  0.0034,  0.0159, -0.0154,\n",
       "                      -0.0296, -0.0880, -0.0680,  0.1278,  0.1694, -0.0390,  0.0412, -0.1267,\n",
       "                      -0.0738,  0.0335, -0.0972,  0.1376, -0.0586, -0.0083, -0.1895,  0.0520,\n",
       "                      -0.0740,  0.0251, -0.2985,  0.1414,  0.0213,  0.0290,  0.0456, -0.0845,\n",
       "                      -0.1708,  0.1847,  0.0048, -0.0313,  0.0008,  0.1420, -0.0638,  0.1201,\n",
       "                       0.0301, -0.1779, -0.0060,  0.0171, -0.1405, -0.0261, -0.0153, -0.0396,\n",
       "                      -0.0189, -0.0733, -0.0415, -0.0492, -0.0004, -0.0397, -0.0174,  0.0070])),\n",
       "             ('hidden_layers.1.weight',\n",
       "              tensor([[ 0.0319,  0.1165,  0.1666,  ...,  0.1257, -0.0897,  0.0685],\n",
       "                      [ 0.1030,  0.0224,  0.0698,  ..., -0.0433,  0.1303,  0.1616],\n",
       "                      [-0.1187, -0.1816, -0.0368,  ..., -0.1331,  0.0741,  0.0183],\n",
       "                      ...,\n",
       "                      [-0.0615,  0.1647, -0.0291,  ...,  0.1093, -0.0349,  0.1559],\n",
       "                      [ 0.1800, -0.0951,  0.1104,  ...,  0.1046, -0.1066, -0.0054],\n",
       "                      [ 0.1733, -0.1770,  0.1962,  ...,  0.1441, -0.2002,  0.1210]])),\n",
       "             ('hidden_layers.1.bias',\n",
       "              tensor([-9.2005e-02, -6.8525e-03, -1.3605e-02,  1.3041e-02, -1.1620e-02,\n",
       "                       3.4170e-02,  7.7764e-02, -2.4565e-02,  7.4586e-03,  1.8436e-03,\n",
       "                       1.0111e-01, -3.8147e-02, -1.5146e-02,  1.5505e-02, -4.5265e-02,\n",
       "                      -3.4314e-02, -1.9612e-02, -3.6232e-02,  6.0463e-02, -4.2225e-02,\n",
       "                       9.7863e-02, -5.8740e-02,  3.1256e-02, -2.9566e-02,  7.1166e-03,\n",
       "                       4.4672e-02, -8.4681e-04,  6.9028e-02,  2.2048e-02, -1.0849e-02,\n",
       "                       9.1921e-02,  3.3752e-02,  2.4321e-02,  2.3555e-03,  3.8460e-03,\n",
       "                      -6.9002e-02, -3.5883e-02, -5.8713e-02,  3.7651e-03, -6.8418e-02,\n",
       "                       7.6514e-02,  4.7924e-02, -4.0272e-02,  5.6952e-03,  1.2606e-02,\n",
       "                      -1.5191e-02,  3.9763e-03, -1.0004e-02, -1.1087e-01, -7.1620e-03,\n",
       "                       4.7191e-02,  5.5824e-02,  1.9906e-03, -3.9571e-03,  2.3883e-02,\n",
       "                      -1.1270e-01,  1.0560e-02, -7.6186e-04, -4.6156e-03,  2.6117e-02,\n",
       "                       2.3961e-02,  1.1881e-02, -9.3483e-02, -2.3355e-03, -9.3038e-02,\n",
       "                      -1.5222e-02, -4.3540e-02, -1.8476e-02,  3.8773e-02, -4.9075e-02,\n",
       "                       1.8162e-02, -4.3883e-02,  6.7645e-02, -8.3244e-04,  7.8041e-02,\n",
       "                      -1.4054e-02,  1.1240e-03, -1.1071e-02, -7.0124e-02, -2.4229e-02,\n",
       "                      -4.0805e-02,  1.6779e-04, -1.3788e-02, -4.1798e-02, -1.1476e-01,\n",
       "                      -5.1049e-02,  5.7818e-02, -3.6208e-02,  4.8388e-02,  9.5092e-02,\n",
       "                      -1.1649e-02, -7.8919e-02, -3.4285e-04,  4.9565e-03,  2.8996e-02,\n",
       "                       1.2340e-01, -1.6707e-02, -1.1830e-02,  1.9981e-02, -5.7411e-02,\n",
       "                       3.8017e-02, -7.6480e-02, -1.1388e-03, -3.9003e-02, -5.3492e-02,\n",
       "                       8.5682e-02,  6.2694e-02,  6.4785e-03,  4.8055e-02, -9.2648e-03,\n",
       "                       9.3060e-03, -1.1842e-02,  2.0468e-02,  5.2330e-02, -1.0923e-02,\n",
       "                       4.5515e-02, -1.6362e-02,  4.3036e-02,  4.8938e-02,  3.0985e-03,\n",
       "                       4.2580e-02, -2.0130e-02,  8.3514e-02,  5.3974e-02, -3.1842e-02,\n",
       "                       3.3168e-02, -1.5112e-02,  1.3681e-02, -1.0684e-01, -1.9587e-02,\n",
       "                      -5.5738e-02, -1.0026e-01, -5.7387e-02, -8.8272e-02, -1.6689e-02,\n",
       "                      -8.4638e-02,  1.2481e-01,  3.8259e-03,  1.2582e-02,  6.1121e-03,\n",
       "                      -5.3541e-02, -5.6621e-02, -2.0887e-02,  4.1111e-02,  2.0444e-02,\n",
       "                       1.7083e-01,  1.8408e-02,  1.8268e-02, -3.5137e-02,  2.2455e-02,\n",
       "                      -2.4932e-02,  5.8843e-02, -3.9862e-03,  2.6537e-02, -2.1278e-02,\n",
       "                       1.8729e-02, -2.0552e-02, -1.0746e-02, -6.5146e-02,  1.0228e-01,\n",
       "                       3.1373e-02,  9.9140e-02, -4.3340e-02,  3.3327e-02, -2.6557e-02,\n",
       "                       3.6975e-02, -6.7320e-02,  4.9787e-02,  6.7457e-02,  2.2444e-02,\n",
       "                       9.5716e-04, -1.5590e-02, -3.8374e-02, -1.9070e-02,  2.0167e-02,\n",
       "                      -2.3330e-02, -5.6446e-02, -1.2366e-02,  1.2389e-02, -4.2932e-02,\n",
       "                       2.9304e-02,  3.0664e-02, -8.9437e-02, -3.9482e-02, -5.3686e-02,\n",
       "                       3.0849e-02,  1.1812e-02, -3.3165e-02, -2.0127e-02,  1.9985e-03,\n",
       "                      -6.3346e-02, -2.8954e-02, -5.1022e-02, -6.9843e-02,  6.9846e-03,\n",
       "                       1.9541e-02,  4.1417e-02, -2.9363e-02, -4.5334e-02,  3.5026e-02])),\n",
       "             ('hidden_layers.2.weight',\n",
       "              tensor([[ 0.1648,  0.0056, -0.2074,  ...,  0.0556,  0.1134,  0.2036],\n",
       "                      [ 0.1931, -0.0179, -0.0217,  ...,  0.0770,  0.1302, -0.1741],\n",
       "                      [ 0.0267,  0.1492,  0.1055,  ..., -0.0870,  0.0670,  0.1784],\n",
       "                      ...,\n",
       "                      [-0.1676,  0.1195,  0.0958,  ..., -0.0332, -0.1921,  0.1934],\n",
       "                      [ 0.0472,  0.1726,  0.1525,  ..., -0.1397,  0.0080, -0.1714],\n",
       "                      [ 0.0948, -0.0720, -0.0990,  ...,  0.1799, -0.0042, -0.0530]])),\n",
       "             ('hidden_layers.2.bias',\n",
       "              tensor([-0.0020,  0.0323, -0.0133, -0.0236, -0.0267, -0.0174,  0.0832, -0.0855,\n",
       "                       0.0561, -0.0072, -0.0151,  0.0017, -0.0403,  0.0121, -0.0647,  0.0305,\n",
       "                       0.0014, -0.0184, -0.0440, -0.0276,  0.0034,  0.0295,  0.0045,  0.0321,\n",
       "                      -0.0198,  0.0363,  0.0343,  0.0111,  0.0285, -0.0039,  0.0086, -0.0561,\n",
       "                       0.0463, -0.0007, -0.0058,  0.0312, -0.0015, -0.0084, -0.0507,  0.0192,\n",
       "                       0.0176,  0.0221, -0.0176,  0.0790, -0.0007,  0.0316,  0.0314, -0.0136,\n",
       "                      -0.0116, -0.0115, -0.0454,  0.0009, -0.0137,  0.0388,  0.0042,  0.0054,\n",
       "                       0.0368,  0.0032, -0.0151, -0.0187, -0.0306, -0.0027,  0.0071, -0.0004,\n",
       "                      -0.0443,  0.0145,  0.0525, -0.0312,  0.0104,  0.0128,  0.0085, -0.0053,\n",
       "                       0.0837,  0.0446,  0.0462,  0.0014, -0.0151, -0.0074, -0.0080,  0.0326,\n",
       "                       0.0163, -0.0045,  0.0050,  0.0098, -0.0525,  0.0221, -0.0609, -0.0674,\n",
       "                      -0.0036, -0.0307, -0.0235, -0.0005, -0.0328, -0.0704, -0.0303, -0.0163,\n",
       "                      -0.0076, -0.0275,  0.0306, -0.0366, -0.0352, -0.0513,  0.0103,  0.0114,\n",
       "                      -0.0042, -0.0333,  0.0088,  0.0317,  0.0219, -0.0228,  0.0024, -0.0901,\n",
       "                       0.0147,  0.0070,  0.0113, -0.0590, -0.0504, -0.0181,  0.0083,  0.0367,\n",
       "                      -0.0069,  0.0020, -0.0775,  0.0166,  0.0207, -0.0017, -0.0351, -0.0080,\n",
       "                       0.0090, -0.0020,  0.0191,  0.0383, -0.0469,  0.0096,  0.0202, -0.0162,\n",
       "                      -0.0312,  0.0373, -0.0019, -0.0217, -0.0052,  0.0142, -0.0065, -0.0103,\n",
       "                      -0.0253,  0.0112,  0.0065,  0.0090,  0.0283, -0.0087,  0.0048,  0.0169,\n",
       "                      -0.0297, -0.0458, -0.0008, -0.0062,  0.0364, -0.0001,  0.0052,  0.0064,\n",
       "                       0.0234,  0.0277, -0.0409, -0.0101,  0.0625, -0.0148,  0.0085,  0.0318,\n",
       "                       0.0062,  0.0562,  0.0075, -0.0368, -0.0015,  0.0061, -0.0224,  0.0150,\n",
       "                       0.0067, -0.0260, -0.0129,  0.0152, -0.0182,  0.0231,  0.0426,  0.0458,\n",
       "                      -0.0077,  0.0496,  0.0427, -0.0230,  0.0241, -0.0312,  0.0283,  0.0038,\n",
       "                       0.0140, -0.0047, -0.0134, -0.0056, -0.0295,  0.0298,  0.0049, -0.0480])),\n",
       "             ('hidden_layers.3.weight',\n",
       "              tensor([[-0.0695, -0.0704, -0.0507,  ...,  0.0176,  0.0424, -0.0586],\n",
       "                      [ 0.0228, -0.0468, -0.0064,  ...,  0.0404, -0.0103,  0.0209],\n",
       "                      [ 0.0259, -0.0158,  0.0471,  ...,  0.0661,  0.0381, -0.0398],\n",
       "                      ...,\n",
       "                      [-0.0179,  0.0353,  0.0377,  ..., -0.0534, -0.0602, -0.0453],\n",
       "                      [-0.0365, -0.0307, -0.0560,  ..., -0.0528,  0.0183,  0.0335],\n",
       "                      [ 0.0020,  0.0356, -0.0426,  ..., -0.0475, -0.0065, -0.0057]])),\n",
       "             ('hidden_layers.3.bias',\n",
       "              tensor([-9.4308e-03,  6.8854e-02,  1.1240e-02,  4.1728e-03, -3.0191e-02,\n",
       "                       6.4124e-02, -2.2824e-02,  1.5327e-03, -3.2095e-02,  1.9315e-02,\n",
       "                      -1.2648e-02, -4.0452e-03,  1.5693e-02,  1.7275e-02, -4.1602e-02,\n",
       "                      -4.9013e-02, -3.7576e-02,  5.1132e-02, -6.2618e-02,  5.7422e-02,\n",
       "                      -5.1210e-02,  5.3188e-02,  3.4169e-02, -1.9343e-02, -6.0774e-02,\n",
       "                       7.5187e-03, -6.4740e-02, -3.6009e-02,  6.8782e-02, -2.6278e-02,\n",
       "                      -3.1014e-02, -3.3481e-02, -1.0383e-02, -3.1881e-02,  2.8550e-02,\n",
       "                       5.2455e-02, -5.6339e-02,  5.4465e-02,  1.5228e-02,  8.4492e-03,\n",
       "                      -3.3043e-02, -4.6688e-02,  3.8107e-02, -4.4000e-02, -2.3064e-02,\n",
       "                       5.0558e-03,  2.8964e-02, -2.8500e-02,  3.8712e-02,  5.9188e-02,\n",
       "                       2.6090e-02, -1.7228e-02,  5.8120e-02,  5.1438e-02, -6.5724e-05,\n",
       "                      -1.4534e-02,  5.0104e-02,  4.1235e-02,  3.8479e-02,  4.8369e-02,\n",
       "                      -7.8895e-03, -3.4516e-02, -4.3233e-04, -5.8988e-02, -6.8092e-02,\n",
       "                      -2.0905e-02, -4.6534e-03, -5.5392e-02,  6.0652e-02, -1.5282e-02,\n",
       "                      -6.3457e-02, -2.0325e-02,  6.0439e-02, -5.1676e-02,  4.1972e-03,\n",
       "                      -6.8574e-02,  6.6865e-02,  1.3178e-02,  2.1348e-02, -1.3814e-02,\n",
       "                       9.3879e-03, -4.8881e-02,  1.9264e-02,  6.4313e-02,  5.1865e-02,\n",
       "                      -2.8177e-02,  6.0343e-02, -1.3022e-02,  7.0479e-02,  7.0095e-02,\n",
       "                       3.8315e-02, -2.2732e-02,  9.2424e-04,  3.5948e-02, -2.6449e-02,\n",
       "                       6.6120e-02, -1.3960e-02, -1.3629e-02,  1.1778e-02, -6.1278e-02,\n",
       "                      -6.2884e-02,  4.9894e-02, -3.0673e-02,  2.6136e-02, -1.4076e-02,\n",
       "                       2.3444e-02, -4.1622e-02, -4.9202e-02, -5.1643e-02, -3.4001e-02,\n",
       "                      -4.1516e-02, -2.4636e-02, -1.0326e-02,  1.7479e-02,  2.7684e-02,\n",
       "                       1.6292e-02,  5.2289e-02, -5.4949e-02, -5.6630e-02,  3.1735e-02,\n",
       "                      -4.4406e-03,  2.4004e-02, -6.1095e-02, -9.3007e-03, -6.6968e-02,\n",
       "                       4.1519e-02, -2.8842e-03, -3.4644e-02, -6.9217e-02,  5.2096e-02,\n",
       "                      -3.7666e-02,  9.9041e-03,  3.8929e-02,  3.1799e-03,  1.5850e-02,\n",
       "                       3.6606e-02,  4.1569e-02, -6.2982e-02, -6.7157e-02, -2.1715e-02,\n",
       "                      -6.9389e-02,  5.5365e-02,  8.4312e-03,  5.9062e-02,  2.6134e-02,\n",
       "                      -6.9003e-02, -3.9323e-02, -5.3424e-02,  1.5517e-02, -5.5653e-02,\n",
       "                       1.0907e-02,  2.3858e-02,  5.4929e-02,  1.0614e-02,  9.5429e-03,\n",
       "                      -3.4082e-02,  4.7775e-02, -4.9424e-02,  6.2804e-02, -1.7293e-02,\n",
       "                       4.3546e-02,  2.0187e-02,  6.2576e-02, -1.8871e-02,  4.7002e-05,\n",
       "                       3.3018e-02, -2.8565e-02,  3.9153e-02,  9.2901e-03, -4.2764e-02,\n",
       "                      -3.8220e-02, -5.4226e-02,  4.2826e-02,  4.7452e-02, -2.1164e-02,\n",
       "                       6.7970e-02,  3.1000e-03,  5.6116e-02, -2.0727e-02, -3.3187e-02,\n",
       "                       1.6374e-02, -4.3162e-02, -6.8735e-02, -4.7926e-02, -2.0028e-02,\n",
       "                      -2.0347e-02,  2.9063e-02, -3.8200e-02, -6.4514e-02, -1.7710e-02,\n",
       "                       4.4507e-02, -2.7487e-02,  3.4948e-02,  9.8280e-03, -2.3741e-02,\n",
       "                      -1.9902e-02,  2.2827e-02,  1.9484e-02, -2.6168e-02, -1.9025e-02])),\n",
       "             ('output_layer.weight',\n",
       "              tensor([[ 0.0162, -0.0312, -0.0286,  0.0365,  0.0670,  0.0695, -0.0634,  0.0134,\n",
       "                       -0.0307,  0.0399, -0.0255, -0.0462, -0.0282, -0.0245,  0.0612,  0.0564,\n",
       "                       -0.0674,  0.0306,  0.0278, -0.0402, -0.0558,  0.0518, -0.0216,  0.0214,\n",
       "                        0.0626,  0.0392, -0.0630,  0.0119, -0.0554,  0.0169,  0.0233, -0.0695,\n",
       "                       -0.0290, -0.0382, -0.0431, -0.0592, -0.0200, -0.0259, -0.0105, -0.0235,\n",
       "                        0.0274, -0.0647,  0.0115, -0.0446,  0.0454, -0.0257, -0.0215, -0.0549,\n",
       "                        0.0258,  0.0251,  0.0592,  0.0097,  0.0525,  0.0285,  0.0082,  0.0333,\n",
       "                       -0.0273, -0.0506,  0.0682, -0.0441,  0.0019,  0.0615,  0.0389,  0.0487,\n",
       "                       -0.0025, -0.0208, -0.0204, -0.0336,  0.0015,  0.0437,  0.0158,  0.0530,\n",
       "                        0.0167, -0.0592, -0.0184,  0.0256,  0.0633, -0.0598,  0.0448, -0.0375,\n",
       "                        0.0485, -0.0221, -0.0145,  0.0553,  0.0550, -0.0192,  0.0456, -0.0286,\n",
       "                        0.0550,  0.0240,  0.0098, -0.0515,  0.0703, -0.0147, -0.0558,  0.0168,\n",
       "                       -0.0487,  0.0300, -0.0319,  0.0250,  0.0591, -0.0326,  0.0093, -0.0327,\n",
       "                        0.0114, -0.0610, -0.0272, -0.0308,  0.0544, -0.0049,  0.0048, -0.0267,\n",
       "                       -0.0262, -0.0065,  0.0101,  0.0232, -0.0170, -0.0627,  0.0352,  0.0052,\n",
       "                       -0.0378, -0.0502,  0.0555, -0.0682,  0.0525,  0.0282, -0.0593, -0.0164,\n",
       "                       -0.0469,  0.0372, -0.0215,  0.0162,  0.0460,  0.0276, -0.0611,  0.0508,\n",
       "                        0.0424,  0.0409,  0.0131,  0.0281,  0.0563, -0.0486,  0.0641, -0.0504,\n",
       "                        0.0528, -0.0411, -0.0272,  0.0078, -0.0414, -0.0617, -0.0419, -0.0125,\n",
       "                       -0.0042,  0.0318,  0.0028, -0.0542, -0.0084, -0.0238, -0.0118, -0.0629,\n",
       "                       -0.0078, -0.0609,  0.0201,  0.0004,  0.0343,  0.0137,  0.0645,  0.0645,\n",
       "                        0.0549, -0.0547,  0.0072, -0.0364, -0.0131,  0.0624,  0.0385, -0.0012,\n",
       "                        0.0531,  0.0219, -0.0243,  0.0023,  0.0405, -0.0351, -0.0356,  0.0285,\n",
       "                       -0.0400,  0.0541, -0.0457, -0.0279,  0.0473, -0.0507, -0.0323,  0.0707,\n",
       "                        0.0038,  0.0682, -0.0104, -0.0068,  0.0487, -0.0102,  0.0580,  0.0440]])),\n",
       "             ('output_layer.bias', tensor([-0.0531]))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_network.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f125a048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-2.8867e-01,  2.1617e-01],\n",
       "         [ 2.1635e-02, -1.7316e-01],\n",
       "         [ 1.3589e-01, -4.0220e-01],\n",
       "         [ 3.3085e-01, -3.0147e-01],\n",
       "         [ 9.4485e-02,  6.7596e-01],\n",
       "         [ 4.8037e-02,  1.5047e-01],\n",
       "         [ 1.7420e-02, -1.2656e-02],\n",
       "         [ 2.6476e-01, -2.8099e-01],\n",
       "         [-1.6313e-01,  3.2822e-01],\n",
       "         [ 1.5153e-01,  2.0016e-02],\n",
       "         [ 3.1241e-01,  1.2560e-01],\n",
       "         [-1.2882e-03,  1.3924e-02],\n",
       "         [-2.9341e-02, -1.0869e-01],\n",
       "         [ 4.2156e-01, -3.3039e-02],\n",
       "         [-3.7150e-02,  1.6494e-01],\n",
       "         [-2.8273e-01,  1.9943e-01],\n",
       "         [-9.0187e-02, -1.0217e-01],\n",
       "         [ 1.6243e-01, -2.4843e-01],\n",
       "         [-2.6768e-02,  2.3992e-02],\n",
       "         [-3.5832e-03, -3.6459e-01],\n",
       "         [ 4.6498e-01,  1.9718e-01],\n",
       "         [ 7.2692e-02,  2.9311e-01],\n",
       "         [-3.1350e-01, -8.2516e-03],\n",
       "         [-6.3186e-02, -2.4927e-01],\n",
       "         [-2.8184e-01,  8.1370e-02],\n",
       "         [ 3.0844e-02, -3.1635e-01],\n",
       "         [ 3.0929e-02,  2.1060e-01],\n",
       "         [ 7.5540e-02,  3.3052e-01],\n",
       "         [-4.0240e-02, -1.2438e-01],\n",
       "         [-2.0686e-01,  2.6395e-02],\n",
       "         [ 1.4512e-01, -3.6718e-01],\n",
       "         [ 2.8406e-01,  1.7301e-01],\n",
       "         [-1.9123e-02,  3.0728e-01],\n",
       "         [-2.2202e-02, -6.4514e-03],\n",
       "         [-1.3281e-01, -7.4291e-02],\n",
       "         [-2.6175e-01, -1.1558e-01],\n",
       "         [-3.2532e-02, -2.1378e-01],\n",
       "         [-5.6794e-02,  6.6944e-02],\n",
       "         [-1.4606e-01, -2.7259e-01],\n",
       "         [ 2.7718e-02, -2.4937e-01],\n",
       "         [ 4.7628e-01,  4.2576e-03],\n",
       "         [-1.6305e-02,  1.7546e-01],\n",
       "         [ 2.6238e-02, -5.5113e-01],\n",
       "         [-3.4372e-02,  5.9709e-02],\n",
       "         [-9.8802e-02, -6.2963e-02],\n",
       "         [-1.0513e-01,  1.5829e-01],\n",
       "         [-3.0646e-01, -1.8775e-01],\n",
       "         [-2.3739e-01,  1.2387e-01],\n",
       "         [-1.1416e-01,  1.7131e-01],\n",
       "         [-5.9335e-02,  4.7420e-03],\n",
       "         [-3.6005e-02, -2.0754e-01],\n",
       "         [-9.1216e-02,  1.2741e-01],\n",
       "         [-1.0939e-01, -6.3282e-01],\n",
       "         [-9.5100e-02, -3.0893e-01],\n",
       "         [ 3.3333e-01, -3.4208e-02],\n",
       "         [ 1.1731e-01,  1.7086e-01],\n",
       "         [-6.8881e-02, -8.2503e-02],\n",
       "         [-2.3585e-01, -1.5801e-01],\n",
       "         [ 1.5790e-02, -3.9694e-01],\n",
       "         [ 2.5868e-01,  1.1258e-01],\n",
       "         [-2.8829e-02,  2.9623e-01],\n",
       "         [ 4.4064e-01,  2.0890e-01],\n",
       "         [-1.4628e-01, -1.8194e-01],\n",
       "         [-1.6768e-02, -5.4282e-01],\n",
       "         [-4.4631e-02, -1.3465e-01],\n",
       "         [ 5.8991e-02, -4.5756e-01],\n",
       "         [ 1.1417e-01,  4.9885e-01],\n",
       "         [-4.1410e-01, -2.0217e-01],\n",
       "         [-6.1487e-02, -2.3772e-01],\n",
       "         [ 4.7831e-02,  6.6133e-02],\n",
       "         [ 4.6623e-01,  2.2866e-01],\n",
       "         [ 3.2589e-02, -1.7600e-01],\n",
       "         [ 8.7284e-03,  2.4548e-01],\n",
       "         [-3.9253e-02,  2.1957e-01],\n",
       "         [ 2.3322e-01,  1.6981e-01],\n",
       "         [ 1.0354e-01, -4.3028e-01],\n",
       "         [ 4.3934e-02,  4.2741e-01],\n",
       "         [-4.6327e-01, -2.5477e-01],\n",
       "         [ 2.4621e-02,  1.4408e-01],\n",
       "         [-1.3146e-01, -1.9079e-01],\n",
       "         [-9.1755e-02,  6.2319e-01],\n",
       "         [-4.5664e-03, -2.3589e-02],\n",
       "         [-9.4396e-03,  3.2891e-01],\n",
       "         [-7.4041e-03, -3.6519e-01],\n",
       "         [ 1.4929e-01, -1.6883e-01],\n",
       "         [ 3.1501e-01,  3.5829e-01],\n",
       "         [-2.5014e-01, -3.0920e-01],\n",
       "         [-1.3405e-01, -1.8314e-01],\n",
       "         [-1.1270e-01,  3.3037e-01],\n",
       "         [-9.4502e-02,  2.8110e-01],\n",
       "         [ 7.6105e-02, -3.3334e-01],\n",
       "         [ 3.2275e-01,  1.1120e-01],\n",
       "         [-3.6575e-01, -2.2008e-01],\n",
       "         [-9.6025e-02, -1.8639e-01],\n",
       "         [-8.3378e-03, -2.9692e-01],\n",
       "         [ 3.8765e-01,  2.9695e-01],\n",
       "         [-1.9882e-01, -3.0480e-01],\n",
       "         [-8.6165e-03,  1.5818e-01],\n",
       "         [-1.2888e-01,  2.0233e-01],\n",
       "         [-7.8402e-02,  6.3929e-02],\n",
       "         [-1.1444e-01, -1.8224e-01],\n",
       "         [ 1.5261e-01,  1.5353e-01],\n",
       "         [ 4.0841e-02, -3.0157e-02],\n",
       "         [-2.6280e-02, -1.8822e-01],\n",
       "         [-9.4362e-02, -1.7506e-01],\n",
       "         [ 2.3348e-02, -4.5616e-02],\n",
       "         [ 2.4157e-01,  2.8048e-01],\n",
       "         [ 2.4905e-01, -2.1689e-01],\n",
       "         [-2.3419e-01,  1.2454e-01],\n",
       "         [ 2.8305e-02, -2.9048e-01],\n",
       "         [-4.1754e-02,  1.9034e-01],\n",
       "         [-2.2627e-01, -2.3418e-01],\n",
       "         [-3.7607e-02,  6.4252e-02],\n",
       "         [ 4.4075e-02, -2.5236e-01],\n",
       "         [-4.8613e-02,  3.0980e-01],\n",
       "         [-3.5833e-02,  9.9740e-02],\n",
       "         [-1.8814e-02, -1.4784e-01],\n",
       "         [ 9.9843e-02, -3.1998e-01],\n",
       "         [ 1.1875e-01, -2.5084e-01],\n",
       "         [-3.7998e-01, -1.4078e-01],\n",
       "         [ 2.2392e-02,  3.2690e-01],\n",
       "         [ 6.4514e-02, -2.5892e-01],\n",
       "         [-2.2460e-01,  5.0526e-02],\n",
       "         [ 4.9397e-02, -2.9837e-03],\n",
       "         [-6.7973e-02,  1.6549e-01],\n",
       "         [ 4.8300e-01,  2.8938e-01],\n",
       "         [ 4.5276e-01,  3.3805e-01],\n",
       "         [-1.7848e-01,  2.4119e-01],\n",
       "         [-3.1160e-01, -8.3120e-02],\n",
       "         [ 4.1978e-01,  7.2025e-02],\n",
       "         [ 2.4269e-02,  4.2806e-02],\n",
       "         [ 1.6262e-02,  3.3993e-01],\n",
       "         [-3.9217e-02, -2.1929e-02],\n",
       "         [-7.2582e-02, -3.6168e-01],\n",
       "         [ 2.8581e-02,  3.6118e-02],\n",
       "         [ 2.8575e-01,  2.4000e-01],\n",
       "         [ 3.2825e-02, -1.2068e-01],\n",
       "         [ 1.7134e-01,  8.0932e-02],\n",
       "         [-1.3659e-01, -3.7583e-02],\n",
       "         [ 1.6367e-01,  1.6337e-01],\n",
       "         [-2.4537e-01, -2.6936e-01],\n",
       "         [-1.4744e-01,  6.3424e-03],\n",
       "         [ 3.8359e-02,  9.5480e-02],\n",
       "         [-3.7942e-02,  1.2213e-01],\n",
       "         [-4.2206e-02, -2.2864e-01],\n",
       "         [ 1.0954e-01, -2.4449e-01],\n",
       "         [-5.8617e-02, -1.0413e-03],\n",
       "         [-2.2031e-01, -2.2482e-02],\n",
       "         [-1.9972e-02, -1.9440e-02],\n",
       "         [ 8.4749e-02,  3.4601e-02],\n",
       "         [ 2.8551e-02,  1.1509e-01],\n",
       "         [ 1.1073e-01, -4.6713e-01],\n",
       "         [ 1.9196e-02, -3.0658e-01],\n",
       "         [-1.3548e-01, -2.4312e-01],\n",
       "         [ 1.9419e-03,  1.5938e-01],\n",
       "         [-1.5016e-01,  8.5101e-02],\n",
       "         [-3.9010e-02,  1.3909e-01],\n",
       "         [ 2.6556e-01,  2.2898e-01],\n",
       "         [ 3.2736e-01, -5.6556e-02],\n",
       "         [-2.4682e-02, -2.5014e-01],\n",
       "         [-1.1585e-01,  1.7662e-01],\n",
       "         [ 1.3892e-01, -6.5572e-01],\n",
       "         [ 6.2874e-02, -4.6900e-02],\n",
       "         [-5.1693e-02, -1.4331e-01],\n",
       "         [-5.4871e-04,  3.1490e-01],\n",
       "         [ 8.7286e-02, -7.0365e-02],\n",
       "         [-2.2731e-01, -1.1048e-02],\n",
       "         [-2.4272e-02, -4.9468e-01],\n",
       "         [-1.3934e-01, -3.4035e-01],\n",
       "         [-4.2900e-02, -3.6755e-01],\n",
       "         [-3.7508e-02, -2.1726e-01],\n",
       "         [-2.5034e-01,  2.5950e-01],\n",
       "         [-3.4280e-01, -1.4179e-02],\n",
       "         [-4.6941e-02,  2.0036e-02],\n",
       "         [ 2.3305e-01,  7.5469e-04],\n",
       "         [ 1.2994e-03, -1.4716e-01],\n",
       "         [-2.2574e-01,  2.1557e-01],\n",
       "         [-1.8665e-01,  3.0595e-01],\n",
       "         [-3.4345e-02,  2.2665e-01],\n",
       "         [ 8.6815e-02,  2.4025e-01],\n",
       "         [-3.7667e-02, -3.7456e-01],\n",
       "         [ 2.2601e-02,  9.9289e-02],\n",
       "         [-1.2413e-02, -3.3107e-01],\n",
       "         [-2.6962e-02,  1.7774e-01],\n",
       "         [ 4.8206e-01,  1.7412e-01],\n",
       "         [-1.2586e-01,  2.7618e-01],\n",
       "         [-1.9242e-01,  7.8400e-03],\n",
       "         [ 1.5393e-01,  2.1164e-01],\n",
       "         [-1.2238e-01, -2.1279e-01],\n",
       "         [ 3.0598e-01,  1.6220e-01],\n",
       "         [-3.2858e-01, -2.3856e-01],\n",
       "         [-9.6563e-02, -1.4897e-01],\n",
       "         [-9.4273e-02,  3.9008e-01],\n",
       "         [-5.4740e-01,  2.2137e-01],\n",
       "         [-2.1388e-02, -2.0954e-01],\n",
       "         [-1.4646e-01,  5.1353e-01],\n",
       "         [ 2.1484e-01,  1.1449e-01],\n",
       "         [-3.6185e-01, -2.5896e-01],\n",
       "         [-3.9697e-01, -4.4289e-01],\n",
       "         [-7.0913e-02, -3.3404e-01]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 1.7865e-01, -1.3582e-01,  1.0910e-01,  1.4094e-02,  1.6340e-01,\n",
       "         -6.1595e-01, -2.4404e-01,  2.5568e-02,  2.2112e-01,  2.9918e-01,\n",
       "         -5.7333e-02,  1.4877e-01,  1.8980e-01, -2.5915e-02,  7.0527e-02,\n",
       "         -1.3364e-01,  1.2773e-01, -1.0795e-01,  8.7504e-02,  2.6548e-01,\n",
       "          3.5656e-01, -1.4186e-01, -3.3384e-01,  3.0341e-01, -2.4277e-02,\n",
       "         -1.9546e-01, -2.5239e-01,  9.4159e-02, -8.0113e-03, -2.6509e-01,\n",
       "         -3.9228e-02,  7.7332e-02,  1.9758e-01,  2.8012e-01,  1.5099e-01,\n",
       "          3.3415e-01,  3.1721e-01,  3.4243e-01, -1.1051e-02, -2.8050e-01,\n",
       "          2.0468e-01,  2.5668e-02, -1.5741e-01,  4.5349e-01, -1.3699e-01,\n",
       "         -4.2639e-02,  7.2242e-02, -1.6164e-01,  3.1212e-01, -9.0125e-05,\n",
       "          1.9362e-01,  2.4849e-01,  1.0267e-01, -1.9240e-01,  3.5278e-01,\n",
       "         -3.8800e-02,  3.9612e-01, -1.8817e-01,  4.7500e-02,  7.8041e-02,\n",
       "          3.6491e-01,  2.7550e-01, -1.5948e-01,  1.1486e-02, -8.9751e-02,\n",
       "         -3.1276e-02,  2.2311e-01, -9.2226e-02,  1.1458e-01,  5.2599e-03,\n",
       "          1.7837e-01, -1.7141e-01, -3.7418e-01,  1.1103e-01,  1.5804e-01,\n",
       "         -8.8020e-02, -3.2649e-01, -1.6711e-01, -3.1972e-01, -4.7763e-02,\n",
       "         -5.3402e-03,  1.1397e-01,  1.3593e-01, -9.4066e-02,  6.2868e-02,\n",
       "          2.4105e-01,  5.9373e-03, -2.1994e-01,  1.5597e-01,  2.1739e-02,\n",
       "         -2.8740e-02,  3.1439e-01, -6.1330e-02,  2.3962e-01,  2.0131e-01,\n",
       "          2.4778e-01, -1.4108e-01,  1.5528e-01,  5.3251e-02,  7.7225e-02,\n",
       "         -2.4427e-01,  4.4594e-01, -4.3802e-01, -6.5870e-02,  2.3842e-01,\n",
       "         -1.0189e-01, -1.4025e-01,  6.0503e-02, -2.2616e-01, -2.8282e-01,\n",
       "          4.3077e-01, -1.3487e-01,  1.8337e-01, -7.6649e-02, -5.6110e-02,\n",
       "          2.8684e-01,  1.8145e-01, -4.7870e-02, -7.8137e-02, -3.5739e-02,\n",
       "          1.1891e-02, -1.6022e-01, -1.1705e-01, -2.1470e-01,  7.5430e-02,\n",
       "          2.3036e-01,  2.4640e-01, -1.6375e-01, -2.4163e-01,  1.6846e-01,\n",
       "         -1.7199e-01, -1.0501e-01,  1.9979e-01,  2.2916e-01, -3.5246e-01,\n",
       "          1.9121e-02, -2.9237e-01, -4.2143e-04, -2.2514e-02,  7.7313e-02,\n",
       "          5.7393e-02,  3.7209e-02, -4.5615e-02,  4.4217e-01,  3.1279e-01,\n",
       "         -1.2092e-01, -8.7360e-02, -2.2571e-01,  1.4126e-01, -7.2321e-02,\n",
       "         -1.0718e-02, -2.4311e-01,  2.1807e-01, -7.5316e-02, -1.6076e-01,\n",
       "         -1.1726e-01,  3.8473e-01,  2.6947e-01,  4.5999e-02,  2.9634e-01,\n",
       "          8.2713e-02, -1.1341e-01, -1.9112e-01,  4.1138e-01,  2.4260e-02,\n",
       "          1.7922e-01, -7.7698e-02, -1.4781e-01, -1.9067e-01, -3.2039e-02,\n",
       "          1.0850e-01, -1.5416e-01, -1.3515e-01,  1.5715e-01,  1.9230e-01,\n",
       "          1.9343e-01, -9.8800e-02,  1.0731e-01,  1.3396e-01, -1.1533e-01,\n",
       "         -2.1377e-02, -1.9465e-01,  2.9517e-01,  1.9366e-01,  2.1007e-01,\n",
       "         -2.9370e-02,  1.0709e-01, -9.8048e-03, -2.4496e-01,  3.8732e-01,\n",
       "          1.1967e-01,  1.9349e-01, -1.7347e-01, -1.9724e-01,  2.5820e-01,\n",
       "          5.1397e-01,  1.2624e-01, -1.6281e-01, -9.8929e-02,  4.2574e-01],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1265,  0.1227, -0.1929,  ...,  0.1885,  0.0444, -0.1090],\n",
       "         [ 0.0221, -0.1102, -0.0909,  ...,  0.1240, -0.0052,  0.0170],\n",
       "         [-0.0672, -0.0348, -0.2703,  ...,  0.1172,  0.0941,  0.0295],\n",
       "         ...,\n",
       "         [ 0.1308, -0.0410,  0.0716,  ..., -0.1626,  0.0851, -0.0541],\n",
       "         [ 0.0997, -0.1588, -0.1841,  ..., -0.0861, -0.0950, -0.1568],\n",
       "         [ 0.1129,  0.1186,  0.1057,  ..., -0.0216, -0.1126, -0.0824]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0389, -0.0100,  0.0947, -0.0612,  0.1698,  0.0749,  0.0163,  0.0399,\n",
       "         -0.0089,  0.0881,  0.0534,  0.1441,  0.0526, -0.0513, -0.0234,  0.0628,\n",
       "          0.1137,  0.1148,  0.0849, -0.0377,  0.0394,  0.1466,  0.0605,  0.1050,\n",
       "          0.0859,  0.0899, -0.1088, -0.0221, -0.0968, -0.0232, -0.0091, -0.1621,\n",
       "         -0.0662, -0.0730, -0.0592,  0.0295, -0.0587, -0.0435, -0.0417, -0.1510,\n",
       "         -0.0234,  0.0055, -0.2798,  0.0245,  0.0736, -0.1010, -0.0052,  0.1130,\n",
       "          0.2030, -0.0449,  0.0713,  0.0793,  0.1596,  0.0625, -0.0234, -0.1926,\n",
       "          0.0238,  0.0734,  0.0819, -0.0219,  0.1944, -0.1027,  0.0106,  0.0205,\n",
       "          0.0722,  0.0969, -0.0494,  0.0183, -0.1068, -0.0794, -0.0326,  0.0495,\n",
       "         -0.2363, -0.0126,  0.0430,  0.1529,  0.1091,  0.0853, -0.1025, -0.0450,\n",
       "         -0.1481,  0.1441,  0.2197, -0.0794,  0.1391,  0.1237, -0.0865, -0.1204,\n",
       "          0.0413, -0.0929, -0.0702, -0.0457,  0.0263, -0.0058,  0.0549,  0.0199,\n",
       "          0.1167,  0.0811, -0.0427, -0.1881,  0.0299, -0.1833, -0.0232, -0.1121,\n",
       "         -0.0080, -0.1118,  0.0507,  0.1023,  0.1077,  0.0322, -0.0115, -0.0389,\n",
       "          0.0500, -0.0470,  0.0847, -0.0704,  0.1194,  0.1049, -0.0252,  0.0799,\n",
       "         -0.0115, -0.0345, -0.0190, -0.1124, -0.0761, -0.0989, -0.0338,  0.1182,\n",
       "         -0.2132,  0.0330, -0.2862,  0.0178,  0.0990,  0.1622,  0.1396,  0.0764,\n",
       "          0.0054,  0.1066, -0.1285,  0.0295, -0.0915, -0.0454,  0.2065, -0.0308,\n",
       "          0.0567,  0.0235, -0.0309,  0.0830, -0.0837,  0.0034,  0.0159, -0.0154,\n",
       "         -0.0296, -0.0880, -0.0680,  0.1278,  0.1694, -0.0390,  0.0412, -0.1267,\n",
       "         -0.0738,  0.0335, -0.0972,  0.1376, -0.0586, -0.0083, -0.1895,  0.0520,\n",
       "         -0.0740,  0.0251, -0.2985,  0.1414,  0.0213,  0.0290,  0.0456, -0.0845,\n",
       "         -0.1708,  0.1847,  0.0048, -0.0313,  0.0008,  0.1420, -0.0638,  0.1201,\n",
       "          0.0301, -0.1779, -0.0060,  0.0171, -0.1405, -0.0261, -0.0153, -0.0396,\n",
       "         -0.0189, -0.0733, -0.0415, -0.0492, -0.0004, -0.0397, -0.0174,  0.0070],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0319,  0.1165,  0.1666,  ...,  0.1257, -0.0897,  0.0685],\n",
       "         [ 0.1030,  0.0224,  0.0698,  ..., -0.0433,  0.1303,  0.1616],\n",
       "         [-0.1187, -0.1816, -0.0368,  ..., -0.1331,  0.0741,  0.0183],\n",
       "         ...,\n",
       "         [-0.0615,  0.1647, -0.0291,  ...,  0.1093, -0.0349,  0.1559],\n",
       "         [ 0.1800, -0.0951,  0.1104,  ...,  0.1046, -0.1066, -0.0054],\n",
       "         [ 0.1733, -0.1770,  0.1962,  ...,  0.1441, -0.2002,  0.1210]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-9.2005e-02, -6.8525e-03, -1.3605e-02,  1.3041e-02, -1.1620e-02,\n",
       "          3.4170e-02,  7.7764e-02, -2.4565e-02,  7.4586e-03,  1.8436e-03,\n",
       "          1.0111e-01, -3.8147e-02, -1.5146e-02,  1.5505e-02, -4.5265e-02,\n",
       "         -3.4314e-02, -1.9612e-02, -3.6232e-02,  6.0463e-02, -4.2225e-02,\n",
       "          9.7863e-02, -5.8740e-02,  3.1256e-02, -2.9566e-02,  7.1166e-03,\n",
       "          4.4672e-02, -8.4681e-04,  6.9028e-02,  2.2048e-02, -1.0849e-02,\n",
       "          9.1921e-02,  3.3752e-02,  2.4321e-02,  2.3555e-03,  3.8460e-03,\n",
       "         -6.9002e-02, -3.5883e-02, -5.8713e-02,  3.7651e-03, -6.8418e-02,\n",
       "          7.6514e-02,  4.7924e-02, -4.0272e-02,  5.6952e-03,  1.2606e-02,\n",
       "         -1.5191e-02,  3.9763e-03, -1.0004e-02, -1.1087e-01, -7.1620e-03,\n",
       "          4.7191e-02,  5.5824e-02,  1.9906e-03, -3.9571e-03,  2.3883e-02,\n",
       "         -1.1270e-01,  1.0560e-02, -7.6186e-04, -4.6156e-03,  2.6117e-02,\n",
       "          2.3961e-02,  1.1881e-02, -9.3483e-02, -2.3355e-03, -9.3038e-02,\n",
       "         -1.5222e-02, -4.3540e-02, -1.8476e-02,  3.8773e-02, -4.9075e-02,\n",
       "          1.8162e-02, -4.3883e-02,  6.7645e-02, -8.3244e-04,  7.8041e-02,\n",
       "         -1.4054e-02,  1.1240e-03, -1.1071e-02, -7.0124e-02, -2.4229e-02,\n",
       "         -4.0805e-02,  1.6779e-04, -1.3788e-02, -4.1798e-02, -1.1476e-01,\n",
       "         -5.1049e-02,  5.7818e-02, -3.6208e-02,  4.8388e-02,  9.5092e-02,\n",
       "         -1.1649e-02, -7.8919e-02, -3.4285e-04,  4.9565e-03,  2.8996e-02,\n",
       "          1.2340e-01, -1.6707e-02, -1.1830e-02,  1.9981e-02, -5.7411e-02,\n",
       "          3.8017e-02, -7.6480e-02, -1.1388e-03, -3.9003e-02, -5.3492e-02,\n",
       "          8.5682e-02,  6.2694e-02,  6.4785e-03,  4.8055e-02, -9.2648e-03,\n",
       "          9.3060e-03, -1.1842e-02,  2.0468e-02,  5.2330e-02, -1.0923e-02,\n",
       "          4.5515e-02, -1.6362e-02,  4.3036e-02,  4.8938e-02,  3.0985e-03,\n",
       "          4.2580e-02, -2.0130e-02,  8.3514e-02,  5.3974e-02, -3.1842e-02,\n",
       "          3.3168e-02, -1.5112e-02,  1.3681e-02, -1.0684e-01, -1.9587e-02,\n",
       "         -5.5738e-02, -1.0026e-01, -5.7387e-02, -8.8272e-02, -1.6689e-02,\n",
       "         -8.4638e-02,  1.2481e-01,  3.8259e-03,  1.2582e-02,  6.1121e-03,\n",
       "         -5.3541e-02, -5.6621e-02, -2.0887e-02,  4.1111e-02,  2.0444e-02,\n",
       "          1.7083e-01,  1.8408e-02,  1.8268e-02, -3.5137e-02,  2.2455e-02,\n",
       "         -2.4932e-02,  5.8843e-02, -3.9862e-03,  2.6537e-02, -2.1278e-02,\n",
       "          1.8729e-02, -2.0552e-02, -1.0746e-02, -6.5146e-02,  1.0228e-01,\n",
       "          3.1373e-02,  9.9140e-02, -4.3340e-02,  3.3327e-02, -2.6557e-02,\n",
       "          3.6975e-02, -6.7320e-02,  4.9787e-02,  6.7457e-02,  2.2444e-02,\n",
       "          9.5716e-04, -1.5590e-02, -3.8374e-02, -1.9070e-02,  2.0167e-02,\n",
       "         -2.3330e-02, -5.6446e-02, -1.2366e-02,  1.2389e-02, -4.2932e-02,\n",
       "          2.9304e-02,  3.0664e-02, -8.9437e-02, -3.9482e-02, -5.3686e-02,\n",
       "          3.0849e-02,  1.1812e-02, -3.3165e-02, -2.0127e-02,  1.9985e-03,\n",
       "         -6.3346e-02, -2.8954e-02, -5.1022e-02, -6.9843e-02,  6.9846e-03,\n",
       "          1.9541e-02,  4.1417e-02, -2.9363e-02, -4.5334e-02,  3.5026e-02],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1648,  0.0056, -0.2074,  ...,  0.0556,  0.1134,  0.2036],\n",
       "         [ 0.1931, -0.0179, -0.0217,  ...,  0.0770,  0.1302, -0.1741],\n",
       "         [ 0.0267,  0.1492,  0.1055,  ..., -0.0870,  0.0670,  0.1784],\n",
       "         ...,\n",
       "         [-0.1676,  0.1195,  0.0958,  ..., -0.0332, -0.1921,  0.1934],\n",
       "         [ 0.0472,  0.1726,  0.1525,  ..., -0.1397,  0.0080, -0.1714],\n",
       "         [ 0.0948, -0.0720, -0.0990,  ...,  0.1799, -0.0042, -0.0530]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0020,  0.0323, -0.0133, -0.0236, -0.0267, -0.0174,  0.0832, -0.0855,\n",
       "          0.0561, -0.0072, -0.0151,  0.0017, -0.0403,  0.0121, -0.0647,  0.0305,\n",
       "          0.0014, -0.0184, -0.0440, -0.0276,  0.0034,  0.0295,  0.0045,  0.0321,\n",
       "         -0.0198,  0.0363,  0.0343,  0.0111,  0.0285, -0.0039,  0.0086, -0.0561,\n",
       "          0.0463, -0.0007, -0.0058,  0.0312, -0.0015, -0.0084, -0.0507,  0.0192,\n",
       "          0.0176,  0.0221, -0.0176,  0.0790, -0.0007,  0.0316,  0.0314, -0.0136,\n",
       "         -0.0116, -0.0115, -0.0454,  0.0009, -0.0137,  0.0388,  0.0042,  0.0054,\n",
       "          0.0368,  0.0032, -0.0151, -0.0187, -0.0306, -0.0027,  0.0071, -0.0004,\n",
       "         -0.0443,  0.0145,  0.0525, -0.0312,  0.0104,  0.0128,  0.0085, -0.0053,\n",
       "          0.0837,  0.0446,  0.0462,  0.0014, -0.0151, -0.0074, -0.0080,  0.0326,\n",
       "          0.0163, -0.0045,  0.0050,  0.0098, -0.0525,  0.0221, -0.0609, -0.0674,\n",
       "         -0.0036, -0.0307, -0.0235, -0.0005, -0.0328, -0.0704, -0.0303, -0.0163,\n",
       "         -0.0076, -0.0275,  0.0306, -0.0366, -0.0352, -0.0513,  0.0103,  0.0114,\n",
       "         -0.0042, -0.0333,  0.0088,  0.0317,  0.0219, -0.0228,  0.0024, -0.0901,\n",
       "          0.0147,  0.0070,  0.0113, -0.0590, -0.0504, -0.0181,  0.0083,  0.0367,\n",
       "         -0.0069,  0.0020, -0.0775,  0.0166,  0.0207, -0.0017, -0.0351, -0.0080,\n",
       "          0.0090, -0.0020,  0.0191,  0.0383, -0.0469,  0.0096,  0.0202, -0.0162,\n",
       "         -0.0312,  0.0373, -0.0019, -0.0217, -0.0052,  0.0142, -0.0065, -0.0103,\n",
       "         -0.0253,  0.0112,  0.0065,  0.0090,  0.0283, -0.0087,  0.0048,  0.0169,\n",
       "         -0.0297, -0.0458, -0.0008, -0.0062,  0.0364, -0.0001,  0.0052,  0.0064,\n",
       "          0.0234,  0.0277, -0.0409, -0.0101,  0.0625, -0.0148,  0.0085,  0.0318,\n",
       "          0.0062,  0.0562,  0.0075, -0.0368, -0.0015,  0.0061, -0.0224,  0.0150,\n",
       "          0.0067, -0.0260, -0.0129,  0.0152, -0.0182,  0.0231,  0.0426,  0.0458,\n",
       "         -0.0077,  0.0496,  0.0427, -0.0230,  0.0241, -0.0312,  0.0283,  0.0038,\n",
       "          0.0140, -0.0047, -0.0134, -0.0056, -0.0295,  0.0298,  0.0049, -0.0480],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86efc2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "my_network = NeuralNet(input_dimension = init.shape[1], output_dimension = u_init.shape[1], n_hidden_layers=4, neurons=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a2fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if opt_type == \"ADAM\":\n",
    "    optimizer_ = optim.Adam(my_network.parameters(), lr=0.001)\n",
    "elif opt_type == \"LBFGS\":\n",
    "    optimizer_ = optim.LBFGS(my_network.parameters(), lr=0.1, max_iter=1, max_eval=50000, tolerance_change=1.0 * np.finfo(float).eps)\n",
    "else:\n",
    "    raise ValueError(\"Optimizer not recognized\")\n",
    "\n",
    "\n",
    "def fit(model, training_set, interior, num_epochs, optimizer, p, verbose=True):\n",
    "    history = list()\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "\n",
    "        running_loss = list([0])\n",
    "\n",
    "        # Loop over batches\n",
    "        for j, (initial, u_initial, u_initial_t, bd_left, bd_right, ubl, ubr) in enumerate(training_set):\n",
    "            def closure():\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # for initial\n",
    "                initial.requires_grad = True\n",
    "                u_initial_pred_ = model(initial)\n",
    "                inputs = torch.ones(initial_pts, 1).to(device)\n",
    "                grad_u_init = torch.autograd.grad(u_initial_pred_, initial, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_init_t = grad_u_init[:, 1].reshape(-1, )\n",
    "\n",
    "                # for left boundary\n",
    "                bd_left.requires_grad = True\n",
    "                bd_left_pred_ = model(bd_left)\n",
    "                inputs = torch.ones(left_boundary_pts, 1).to(device)\n",
    "                grad_bd_left = torch.autograd.grad(bd_left_pred_, bd_left, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_x_left = grad_bd_left[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(left_boundary_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_bd_x_left = torch.autograd.grad(u_bd_x_left, bd_left, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_xx_left = grad_u_bd_x_left[:, 0].reshape(-1, )\n",
    "                #inputs = torch.ones(left_boundary_pts, 1).reshape(-1, )\n",
    "                #grad_u_bd_xx_left = torch.autograd.grad(u_bd_xx_left, bd_left, grad_outputs=inputs, create_graph=True)[0]\n",
    "                #u_bd_xxx_left = grad_u_bd_xx_left[:, 0].reshape(-1, )\n",
    "\n",
    "                # for right boundary\n",
    "                bd_right.requires_grad = True\n",
    "                bd_right_pred_ = model(bd_right)\n",
    "                inputs = torch.ones(right_boundary_pts, 1).to(device)\n",
    "                grad_bd_right = torch.autograd.grad(bd_right_pred_, bd_right, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_x_right = grad_bd_right[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(right_boundary_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_bd_x_right = torch.autograd.grad(u_bd_x_right, bd_right, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_bd_xx_right = grad_u_bd_x_right[:, 0].reshape(-1, )\n",
    "\n",
    "                # residual calculation\n",
    "                interior.requires_grad = True\n",
    "                u_hat = model(interior)\n",
    "                inputs = torch.ones(residual_pts, 1).to(device)\n",
    "                grad_u_hat = torch.autograd.grad(u_hat, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "\n",
    "                u_x = grad_u_hat[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_x = torch.autograd.grad(u_x, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_xx = grad_u_x[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_xx = torch.autograd.grad(u_xx, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_xxx = grad_u_xx[:, 0].reshape(-1, )\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_xxx = torch.autograd.grad(u_xxx, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_xxxx = grad_u_xxx[:, 0].reshape(-1, )\n",
    "\n",
    "                u_t = grad_u_hat[:, 1]\n",
    "                inputs = torch.ones(residual_pts, 1).reshape(-1, ).to(device)\n",
    "                grad_u_t = torch.autograd.grad(u_t, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                u_tt = grad_u_t[:, 1].reshape(-1, )\n",
    "\n",
    "                pde_single_column = (u_tt.reshape(-1, ) + u_xxxx.reshape(-1, ) + \\\n",
    "                                     u_hat.reshape(-1, ) - 3*torch.sin(interior[:,0])*torch.exp(interior[:,1])) ** 2\n",
    "                pde_single_column = pde_single_column.reshape(-1, 1)\n",
    "\n",
    "                pde_matrix = pde_single_column.reshape(100, 100)\n",
    "\n",
    "                loss_at_time_steps = torch.mean(pde_matrix, 1)\n",
    "                loss_at_time_steps = loss_at_time_steps.reshape(-1, 1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    weighted_loss = torch.matmul(W, loss_at_time_steps)\n",
    "                weighted_loss = torch.exp(-eps * weighted_loss)\n",
    "\n",
    "                loss_pde = torch.mean(weighted_loss * loss_at_time_steps)\n",
    "\n",
    "                # Item 1. below\n",
    "\n",
    "                loss_ic = torch.mean((u_initial_pred_.reshape(-1, ) - u_initial.reshape(-1, )) ** p) + \\\n",
    "                          torch.mean((u_init_t.reshape(-1, ) - u_initial_t.reshape(-1, )) ** p)\n",
    "                #loss_pde = torch.mean((u_tt.reshape(-1, ) + u_xxxx.reshape(-1, )) ** p)\n",
    "                loss_left_b = torch.mean((bd_left_pred_.reshape(-1, )) ** p) + \\\n",
    "                              torch.mean((u_bd_xx_left.reshape(-1, )) ** p)\n",
    "                loss_right_b = torch.mean((bd_right_pred_.reshape(-1, )) ** p) + \\\n",
    "                               torch.mean((u_bd_xx_right.reshape(-1, )) ** p)\n",
    "\n",
    "                loss = loss_ic + loss_pde + loss_left_b + loss_right_b\n",
    "\n",
    "                # Item 2. below\n",
    "                loss.backward()\n",
    "                # Compute average training loss over batches for the current epoch\n",
    "                running_loss[0] += loss.item()\n",
    "                return loss\n",
    "\n",
    "            # Item 3. below\n",
    "            optimizer.step(closure=closure)\n",
    "\n",
    "        print('Loss: ', (running_loss[0] / len(training_set)))\n",
    "        history.append(running_loss[0])\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27e0cca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################  0  ################################\n",
      "Loss:  1.260790228843689\n",
      "################################  1  ################################\n",
      "Loss:  1.210425615310669\n",
      "################################  2  ################################\n",
      "Loss:  1.187355637550354\n",
      "################################  3  ################################\n",
      "Loss:  1.1673636436462402\n",
      "################################  4  ################################\n",
      "Loss:  1.1500794887542725\n",
      "################################  5  ################################\n",
      "Loss:  1.1351267099380493\n",
      "################################  6  ################################\n",
      "Loss:  1.1221719980239868\n",
      "################################  7  ################################\n",
      "Loss:  1.110925316810608\n",
      "################################  8  ################################\n",
      "Loss:  1.1011372804641724\n",
      "################################  9  ################################\n",
      "Loss:  1.0925920009613037\n",
      "################################  10  ################################\n",
      "Loss:  1.0851030349731445\n",
      "################################  11  ################################\n",
      "Loss:  1.078508734703064\n",
      "################################  12  ################################\n",
      "Loss:  1.0726686716079712\n",
      "################################  13  ################################\n",
      "Loss:  1.0674594640731812\n",
      "################################  14  ################################\n",
      "Loss:  1.0627731084823608\n",
      "################################  15  ################################\n",
      "Loss:  1.0585137605667114\n",
      "################################  16  ################################\n",
      "Loss:  1.0545953512191772\n",
      "################################  17  ################################\n",
      "Loss:  1.0509394407272339\n",
      "################################  18  ################################\n",
      "Loss:  1.0474740266799927\n",
      "################################  19  ################################\n",
      "Loss:  1.0441316366195679\n",
      "################################  20  ################################\n",
      "Loss:  1.040847897529602\n",
      "################################  21  ################################\n",
      "Loss:  1.0375597476959229\n",
      "################################  22  ################################\n",
      "Loss:  1.0342059135437012\n",
      "################################  23  ################################\n",
      "Loss:  1.030724287033081\n",
      "################################  24  ################################\n",
      "Loss:  1.027052640914917\n",
      "################################  25  ################################\n",
      "Loss:  1.0231266021728516\n",
      "################################  26  ################################\n",
      "Loss:  1.0188815593719482\n",
      "################################  27  ################################\n",
      "Loss:  1.0142518281936646\n",
      "################################  28  ################################\n",
      "Loss:  1.009171962738037\n",
      "################################  29  ################################\n",
      "Loss:  1.003577470779419\n",
      "################################  30  ################################\n",
      "Loss:  0.9974035024642944\n",
      "################################  31  ################################\n",
      "Loss:  0.9905739426612854\n",
      "################################  32  ################################\n",
      "Loss:  0.9829654693603516\n",
      "################################  33  ################################\n",
      "Loss:  0.9743220210075378\n",
      "################################  34  ################################\n",
      "Loss:  0.9637433886528015\n",
      "################################  35  ################################\n",
      "Loss:  0.9556986093521118\n",
      "################################  36  ################################\n",
      "Loss:  0.94715416431427\n",
      "################################  37  ################################\n",
      "Loss:  0.9391326308250427\n",
      "################################  38  ################################\n",
      "Loss:  0.930331826210022\n",
      "################################  39  ################################\n",
      "Loss:  0.9206559062004089\n",
      "################################  40  ################################\n",
      "Loss:  0.9106297492980957\n",
      "################################  41  ################################\n",
      "Loss:  0.899968147277832\n",
      "################################  42  ################################\n",
      "Loss:  0.8890265822410583\n",
      "################################  43  ################################\n",
      "Loss:  0.8778877258300781\n",
      "################################  44  ################################\n",
      "Loss:  0.8667935132980347\n",
      "################################  45  ################################\n",
      "Loss:  0.8558474779129028\n",
      "################################  46  ################################\n",
      "Loss:  0.8452250361442566\n",
      "################################  47  ################################\n",
      "Loss:  0.8350164890289307\n",
      "################################  48  ################################\n",
      "Loss:  0.8251640796661377\n",
      "################################  49  ################################\n",
      "Loss:  0.8156980276107788\n",
      "################################  50  ################################\n",
      "Loss:  0.8066087365150452\n",
      "################################  51  ################################\n",
      "Loss:  0.797878623008728\n",
      "################################  52  ################################\n",
      "Loss:  0.7894856929779053\n",
      "################################  53  ################################\n",
      "Loss:  0.781342625617981\n",
      "################################  54  ################################\n",
      "Loss:  0.7734524607658386\n",
      "################################  55  ################################\n",
      "Loss:  0.7654788494110107\n",
      "################################  56  ################################\n",
      "Loss:  0.758297860622406\n",
      "################################  57  ################################\n",
      "Loss:  0.7511519193649292\n",
      "################################  58  ################################\n",
      "Loss:  0.7428984045982361\n",
      "################################  59  ################################\n",
      "Loss:  0.7367526292800903\n",
      "################################  60  ################################\n",
      "Loss:  0.7296131253242493\n",
      "################################  61  ################################\n",
      "Loss:  0.721993625164032\n",
      "################################  62  ################################\n",
      "Loss:  0.7116391062736511\n",
      "################################  63  ################################\n",
      "Loss:  0.7034940719604492\n",
      "################################  64  ################################\n",
      "Loss:  0.691860556602478\n",
      "################################  65  ################################\n",
      "Loss:  0.6835033297538757\n",
      "################################  66  ################################\n",
      "Loss:  0.6731810569763184\n",
      "################################  67  ################################\n",
      "Loss:  0.6580632328987122\n",
      "################################  68  ################################\n",
      "Loss:  0.6411700248718262\n",
      "################################  69  ################################\n",
      "Loss:  0.6217692494392395\n",
      "################################  70  ################################\n",
      "Loss:  0.6054959893226624\n",
      "################################  71  ################################\n",
      "Loss:  0.5885604619979858\n",
      "################################  72  ################################\n",
      "Loss:  0.5715323686599731\n",
      "################################  73  ################################\n",
      "Loss:  0.556028425693512\n",
      "################################  74  ################################\n",
      "Loss:  0.5410080552101135\n",
      "################################  75  ################################\n",
      "Loss:  0.5272400379180908\n",
      "################################  76  ################################\n",
      "Loss:  0.5129909515380859\n",
      "################################  77  ################################\n",
      "Loss:  0.49883925914764404\n",
      "################################  78  ################################\n",
      "Loss:  0.48438572883605957\n",
      "################################  79  ################################\n",
      "Loss:  0.47043949365615845\n",
      "################################  80  ################################\n",
      "Loss:  0.4571053981781006\n",
      "################################  81  ################################\n",
      "Loss:  0.4440760016441345\n",
      "################################  82  ################################\n",
      "Loss:  0.4316153824329376\n",
      "################################  83  ################################\n",
      "Loss:  0.42013978958129883\n",
      "################################  84  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.4090205430984497\n",
      "################################  85  ################################\n",
      "Loss:  0.3983456492424011\n",
      "################################  86  ################################\n",
      "Loss:  0.38716089725494385\n",
      "################################  87  ################################\n",
      "Loss:  0.3794934153556824\n",
      "################################  88  ################################\n",
      "Loss:  0.37173280119895935\n",
      "################################  89  ################################\n",
      "Loss:  0.3638409972190857\n",
      "################################  90  ################################\n",
      "Loss:  0.3569152057170868\n",
      "################################  91  ################################\n",
      "Loss:  0.34973111748695374\n",
      "################################  92  ################################\n",
      "Loss:  0.3430020213127136\n",
      "################################  93  ################################\n",
      "Loss:  0.3351229131221771\n",
      "################################  94  ################################\n",
      "Loss:  0.3277020752429962\n",
      "################################  95  ################################\n",
      "Loss:  0.3198434114456177\n",
      "################################  96  ################################\n",
      "Loss:  0.3124951720237732\n",
      "################################  97  ################################\n",
      "Loss:  0.30508366227149963\n",
      "################################  98  ################################\n",
      "Loss:  0.2979672849178314\n",
      "################################  99  ################################\n",
      "Loss:  0.29176434874534607\n",
      "################################  100  ################################\n",
      "Loss:  0.28558313846588135\n",
      "################################  101  ################################\n",
      "Loss:  0.28046974539756775\n",
      "################################  102  ################################\n",
      "Loss:  0.275481641292572\n",
      "################################  103  ################################\n",
      "Loss:  0.27122461795806885\n",
      "################################  104  ################################\n",
      "Loss:  0.26695600152015686\n",
      "################################  105  ################################\n",
      "Loss:  0.2635018527507782\n",
      "################################  106  ################################\n",
      "Loss:  0.2602216303348541\n",
      "################################  107  ################################\n",
      "Loss:  0.257129967212677\n",
      "################################  108  ################################\n",
      "Loss:  0.2542262077331543\n",
      "################################  109  ################################\n",
      "Loss:  0.2515832185745239\n",
      "################################  110  ################################\n",
      "Loss:  0.24913570284843445\n",
      "################################  111  ################################\n",
      "Loss:  0.2469375729560852\n",
      "################################  112  ################################\n",
      "Loss:  0.2449072003364563\n",
      "################################  113  ################################\n",
      "Loss:  0.24306665360927582\n",
      "################################  114  ################################\n",
      "Loss:  0.24125957489013672\n",
      "################################  115  ################################\n",
      "Loss:  0.23954768478870392\n",
      "################################  116  ################################\n",
      "Loss:  0.23759040236473083\n",
      "################################  117  ################################\n",
      "Loss:  0.2361840307712555\n",
      "################################  118  ################################\n",
      "Loss:  0.23457671701908112\n",
      "################################  119  ################################\n",
      "Loss:  0.23286737501621246\n",
      "################################  120  ################################\n",
      "Loss:  0.23094439506530762\n",
      "################################  121  ################################\n",
      "Loss:  0.2288285195827484\n",
      "################################  122  ################################\n",
      "Loss:  0.22625887393951416\n",
      "################################  123  ################################\n",
      "Loss:  0.2237195521593094\n",
      "################################  124  ################################\n",
      "Loss:  0.22085294127464294\n",
      "################################  125  ################################\n",
      "Loss:  0.21785172820091248\n",
      "################################  126  ################################\n",
      "Loss:  0.21481549739837646\n",
      "################################  127  ################################\n",
      "Loss:  0.21171239018440247\n",
      "################################  128  ################################\n",
      "Loss:  0.20845380425453186\n",
      "################################  129  ################################\n",
      "Loss:  0.20541726052761078\n",
      "################################  130  ################################\n",
      "Loss:  0.2025696039199829\n",
      "################################  131  ################################\n",
      "Loss:  0.1998988687992096\n",
      "################################  132  ################################\n",
      "Loss:  0.19735684990882874\n",
      "################################  133  ################################\n",
      "Loss:  0.19494150578975677\n",
      "################################  134  ################################\n",
      "Loss:  0.1926819533109665\n",
      "################################  135  ################################\n",
      "Loss:  0.19044841825962067\n",
      "################################  136  ################################\n",
      "Loss:  0.1884233057498932\n",
      "################################  137  ################################\n",
      "Loss:  0.18660835921764374\n",
      "################################  138  ################################\n",
      "Loss:  0.18494869768619537\n",
      "################################  139  ################################\n",
      "Loss:  0.18350981175899506\n",
      "################################  140  ################################\n",
      "Loss:  0.18216608464717865\n",
      "################################  141  ################################\n",
      "Loss:  0.1810026317834854\n",
      "################################  142  ################################\n",
      "Loss:  0.1798727810382843\n",
      "################################  143  ################################\n",
      "Loss:  0.17875811457633972\n",
      "################################  144  ################################\n",
      "Loss:  0.17764249444007874\n",
      "################################  145  ################################\n",
      "Loss:  0.17647244036197662\n",
      "################################  146  ################################\n",
      "Loss:  0.17528218030929565\n",
      "################################  147  ################################\n",
      "Loss:  0.1740654557943344\n",
      "################################  148  ################################\n",
      "Loss:  0.17281033098697662\n",
      "################################  149  ################################\n",
      "Loss:  0.17148743569850922\n",
      "################################  150  ################################\n",
      "Loss:  0.170096755027771\n",
      "################################  151  ################################\n",
      "Loss:  0.16862189769744873\n",
      "################################  152  ################################\n",
      "Loss:  0.1670512706041336\n",
      "################################  153  ################################\n",
      "Loss:  0.16536302864551544\n",
      "################################  154  ################################\n",
      "Loss:  0.1634054332971573\n",
      "################################  155  ################################\n",
      "Loss:  0.16186019778251648\n",
      "################################  156  ################################\n",
      "Loss:  0.16014684736728668\n",
      "################################  157  ################################\n",
      "Loss:  0.15820203721523285\n",
      "################################  158  ################################\n",
      "Loss:  0.1561092585325241\n",
      "################################  159  ################################\n",
      "Loss:  0.15387582778930664\n",
      "################################  160  ################################\n",
      "Loss:  0.15150271356105804\n",
      "################################  161  ################################\n",
      "Loss:  0.1490514725446701\n",
      "################################  162  ################################\n",
      "Loss:  0.146538645029068\n",
      "################################  163  ################################\n",
      "Loss:  0.14398407936096191\n",
      "################################  164  ################################\n",
      "Loss:  0.14108000695705414\n",
      "################################  165  ################################\n",
      "Loss:  0.1386539489030838\n",
      "################################  166  ################################\n",
      "Loss:  0.13650478422641754\n",
      "################################  167  ################################\n",
      "Loss:  0.1346099078655243\n",
      "################################  168  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.13273479044437408\n",
      "################################  169  ################################\n",
      "Loss:  0.1310897022485733\n",
      "################################  170  ################################\n",
      "Loss:  0.12954553961753845\n",
      "################################  171  ################################\n",
      "Loss:  0.1281435638666153\n",
      "################################  172  ################################\n",
      "Loss:  0.12687256932258606\n",
      "################################  173  ################################\n",
      "Loss:  0.1256309300661087\n",
      "################################  174  ################################\n",
      "Loss:  0.12455593049526215\n",
      "################################  175  ################################\n",
      "Loss:  0.12343097478151321\n",
      "################################  176  ################################\n",
      "Loss:  0.12229851633310318\n",
      "################################  177  ################################\n",
      "Loss:  0.12114103138446808\n",
      "################################  178  ################################\n",
      "Loss:  0.12011722475290298\n",
      "################################  179  ################################\n",
      "Loss:  0.11910857260227203\n",
      "################################  180  ################################\n",
      "Loss:  0.11814087629318237\n",
      "################################  181  ################################\n",
      "Loss:  0.11723509430885315\n",
      "################################  182  ################################\n",
      "Loss:  0.11632393300533295\n",
      "################################  183  ################################\n",
      "Loss:  0.11545827239751816\n",
      "################################  184  ################################\n",
      "Loss:  0.11460158973932266\n",
      "################################  185  ################################\n",
      "Loss:  0.11377265304327011\n",
      "################################  186  ################################\n",
      "Loss:  0.11302473396062851\n",
      "################################  187  ################################\n",
      "Loss:  0.11224733293056488\n",
      "################################  188  ################################\n",
      "Loss:  0.11157888919115067\n",
      "################################  189  ################################\n",
      "Loss:  0.11103182286024094\n",
      "################################  190  ################################\n",
      "Loss:  0.11048144847154617\n",
      "################################  191  ################################\n",
      "Loss:  0.10995175689458847\n",
      "################################  192  ################################\n",
      "Loss:  0.10944928973913193\n",
      "################################  193  ################################\n",
      "Loss:  0.10896918922662735\n",
      "################################  194  ################################\n",
      "Loss:  0.10850998759269714\n",
      "################################  195  ################################\n",
      "Loss:  0.10806956887245178\n",
      "################################  196  ################################\n",
      "Loss:  0.10764510929584503\n",
      "################################  197  ################################\n",
      "Loss:  0.10723203420639038\n",
      "################################  198  ################################\n",
      "Loss:  0.10682660341262817\n",
      "################################  199  ################################\n",
      "Loss:  0.1064252033829689\n",
      "################################  200  ################################\n",
      "Loss:  0.10604546964168549\n",
      "################################  201  ################################\n",
      "Loss:  0.10568443685770035\n",
      "################################  202  ################################\n",
      "Loss:  0.10532413423061371\n",
      "################################  203  ################################\n",
      "Loss:  0.10498365014791489\n",
      "################################  204  ################################\n",
      "Loss:  0.1046527847647667\n",
      "################################  205  ################################\n",
      "Loss:  0.10432125627994537\n",
      "################################  206  ################################\n",
      "Loss:  0.10398684442043304\n",
      "################################  207  ################################\n",
      "Loss:  0.10364911705255508\n",
      "################################  208  ################################\n",
      "Loss:  0.1032957211136818\n",
      "################################  209  ################################\n",
      "Loss:  0.10291944444179535\n",
      "################################  210  ################################\n",
      "Loss:  0.10251966118812561\n",
      "################################  211  ################################\n",
      "Loss:  0.10209118574857712\n",
      "################################  212  ################################\n",
      "Loss:  0.1016634926199913\n",
      "################################  213  ################################\n",
      "Loss:  0.1012091115117073\n",
      "################################  214  ################################\n",
      "Loss:  0.100735604763031\n",
      "################################  215  ################################\n",
      "Loss:  0.10022993385791779\n",
      "################################  216  ################################\n",
      "Loss:  0.09969059377908707\n",
      "################################  217  ################################\n",
      "Loss:  0.09912124276161194\n",
      "################################  218  ################################\n",
      "Loss:  0.09863249212503433\n",
      "################################  219  ################################\n",
      "Loss:  0.09805570542812347\n",
      "################################  220  ################################\n",
      "Loss:  0.09761566668748856\n",
      "################################  221  ################################\n",
      "Loss:  0.09713573753833771\n",
      "################################  222  ################################\n",
      "Loss:  0.09661798924207687\n",
      "################################  223  ################################\n",
      "Loss:  0.09606114774942398\n",
      "################################  224  ################################\n",
      "Loss:  0.09547887742519379\n",
      "################################  225  ################################\n",
      "Loss:  0.09486127644777298\n",
      "################################  226  ################################\n",
      "Loss:  0.09426435083150864\n",
      "################################  227  ################################\n",
      "Loss:  0.09368184953927994\n",
      "################################  228  ################################\n",
      "Loss:  0.09307757765054703\n",
      "################################  229  ################################\n",
      "Loss:  0.09237898886203766\n",
      "################################  230  ################################\n",
      "Loss:  0.0918961837887764\n",
      "################################  231  ################################\n",
      "Loss:  0.09138121455907822\n",
      "################################  232  ################################\n",
      "Loss:  0.09087824821472168\n",
      "################################  233  ################################\n",
      "Loss:  0.09036778658628464\n",
      "################################  234  ################################\n",
      "Loss:  0.08986971527338028\n",
      "################################  235  ################################\n",
      "Loss:  0.08940719813108444\n",
      "################################  236  ################################\n",
      "Loss:  0.08899132162332535\n",
      "################################  237  ################################\n",
      "Loss:  0.08861304074525833\n",
      "################################  238  ################################\n",
      "Loss:  0.08822700381278992\n",
      "################################  239  ################################\n",
      "Loss:  0.08786788582801819\n",
      "################################  240  ################################\n",
      "Loss:  0.08751745522022247\n",
      "################################  241  ################################\n",
      "Loss:  0.08716826140880585\n",
      "################################  242  ################################\n",
      "Loss:  0.08683598041534424\n",
      "################################  243  ################################\n",
      "Loss:  0.08651191741228104\n",
      "################################  244  ################################\n",
      "Loss:  0.0862041562795639\n",
      "################################  245  ################################\n",
      "Loss:  0.08590108901262283\n",
      "################################  246  ################################\n",
      "Loss:  0.08560725301504135\n",
      "################################  247  ################################\n",
      "Loss:  0.08531028777360916\n",
      "################################  248  ################################\n",
      "Loss:  0.08501418679952621\n",
      "################################  249  ################################\n",
      "Loss:  0.08470632135868073\n",
      "################################  250  ################################\n",
      "Loss:  0.0843852087855339\n",
      "################################  251  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.08405347913503647\n",
      "################################  252  ################################\n",
      "Loss:  0.08370637893676758\n",
      "################################  253  ################################\n",
      "Loss:  0.08335667103528976\n",
      "################################  254  ################################\n",
      "Loss:  0.08300038427114487\n",
      "################################  255  ################################\n",
      "Loss:  0.08264665305614471\n",
      "################################  256  ################################\n",
      "Loss:  0.08227735012769699\n",
      "################################  257  ################################\n",
      "Loss:  0.08193205296993256\n",
      "################################  258  ################################\n",
      "Loss:  0.08157762140035629\n",
      "################################  259  ################################\n",
      "Loss:  0.08126866817474365\n",
      "################################  260  ################################\n",
      "Loss:  0.08089181035757065\n",
      "################################  261  ################################\n",
      "Loss:  0.08050452917814255\n",
      "################################  262  ################################\n",
      "Loss:  0.07996867597103119\n",
      "################################  263  ################################\n",
      "Loss:  0.07959138602018356\n",
      "################################  264  ################################\n",
      "Loss:  0.07917258143424988\n",
      "################################  265  ################################\n",
      "Loss:  0.0787031427025795\n",
      "################################  266  ################################\n",
      "Loss:  0.0781613439321518\n",
      "################################  267  ################################\n",
      "Loss:  0.07760177552700043\n",
      "################################  268  ################################\n",
      "Loss:  0.07712837308645248\n",
      "################################  269  ################################\n",
      "Loss:  0.07665883749723434\n",
      "################################  270  ################################\n",
      "Loss:  0.07624665647745132\n",
      "################################  271  ################################\n",
      "Loss:  0.07585856318473816\n",
      "################################  272  ################################\n",
      "Loss:  0.07546083629131317\n",
      "################################  273  ################################\n",
      "Loss:  0.07511303573846817\n",
      "################################  274  ################################\n",
      "Loss:  0.07478900253772736\n",
      "################################  275  ################################\n",
      "Loss:  0.07448078691959381\n",
      "################################  276  ################################\n",
      "Loss:  0.07420115172863007\n",
      "################################  277  ################################\n",
      "Loss:  0.07394060492515564\n",
      "################################  278  ################################\n",
      "Loss:  0.073709636926651\n",
      "################################  279  ################################\n",
      "Loss:  0.07348300516605377\n",
      "################################  280  ################################\n",
      "Loss:  0.07328812777996063\n",
      "################################  281  ################################\n",
      "Loss:  0.07308654487133026\n",
      "################################  282  ################################\n",
      "Loss:  0.07288209348917007\n",
      "################################  283  ################################\n",
      "Loss:  0.07265980541706085\n",
      "################################  284  ################################\n",
      "Loss:  0.07237239181995392\n",
      "################################  285  ################################\n",
      "Loss:  0.07212235778570175\n",
      "################################  286  ################################\n",
      "Loss:  0.07182327657938004\n",
      "################################  287  ################################\n",
      "Loss:  0.07152526080608368\n",
      "################################  288  ################################\n",
      "Loss:  0.07115905731916428\n",
      "################################  289  ################################\n",
      "Loss:  0.07085742801427841\n",
      "################################  290  ################################\n",
      "Loss:  0.07052101194858551\n",
      "################################  291  ################################\n",
      "Loss:  0.07012451440095901\n",
      "################################  292  ################################\n",
      "Loss:  0.0698496401309967\n",
      "################################  293  ################################\n",
      "Loss:  0.06956392526626587\n",
      "################################  294  ################################\n",
      "Loss:  0.0692753791809082\n",
      "################################  295  ################################\n",
      "Loss:  0.06897103041410446\n",
      "################################  296  ################################\n",
      "Loss:  0.06866651773452759\n",
      "################################  297  ################################\n",
      "Loss:  0.0683484897017479\n",
      "################################  298  ################################\n",
      "Loss:  0.06801946461200714\n",
      "################################  299  ################################\n",
      "Loss:  0.06768593937158585\n",
      "################################  300  ################################\n",
      "Loss:  0.06733971834182739\n",
      "################################  301  ################################\n",
      "Loss:  0.06698530167341232\n",
      "################################  302  ################################\n",
      "Loss:  0.0666344165802002\n",
      "################################  303  ################################\n",
      "Loss:  0.06628099083900452\n",
      "################################  304  ################################\n",
      "Loss:  0.06591115146875381\n",
      "################################  305  ################################\n",
      "Loss:  0.06554077565670013\n",
      "################################  306  ################################\n",
      "Loss:  0.06517364829778671\n",
      "################################  307  ################################\n",
      "Loss:  0.06481301039457321\n",
      "################################  308  ################################\n",
      "Loss:  0.06445380300283432\n",
      "################################  309  ################################\n",
      "Loss:  0.06410381942987442\n",
      "################################  310  ################################\n",
      "Loss:  0.06375403702259064\n",
      "################################  311  ################################\n",
      "Loss:  0.06341476738452911\n",
      "################################  312  ################################\n",
      "Loss:  0.06307036429643631\n",
      "################################  313  ################################\n",
      "Loss:  0.06270869076251984\n",
      "################################  314  ################################\n",
      "Loss:  0.06233728677034378\n",
      "################################  315  ################################\n",
      "Loss:  0.06200850382447243\n",
      "################################  316  ################################\n",
      "Loss:  0.06167387217283249\n",
      "################################  317  ################################\n",
      "Loss:  0.06130925193428993\n",
      "################################  318  ################################\n",
      "Loss:  0.060936689376831055\n",
      "################################  319  ################################\n",
      "Loss:  0.0606059730052948\n",
      "################################  320  ################################\n",
      "Loss:  0.06031349301338196\n",
      "################################  321  ################################\n",
      "Loss:  0.0599604994058609\n",
      "################################  322  ################################\n",
      "Loss:  0.059707652777433395\n",
      "################################  323  ################################\n",
      "Loss:  0.059434544295072556\n",
      "################################  324  ################################\n",
      "Loss:  0.05922603979706764\n",
      "################################  325  ################################\n",
      "Loss:  0.05895556882023811\n",
      "################################  326  ################################\n",
      "Loss:  0.05866510793566704\n",
      "################################  327  ################################\n",
      "Loss:  0.05831192433834076\n",
      "################################  328  ################################\n",
      "Loss:  0.05792928859591484\n",
      "################################  329  ################################\n",
      "Loss:  0.05752839893102646\n",
      "################################  330  ################################\n",
      "Loss:  0.057094622403383255\n",
      "################################  331  ################################\n",
      "Loss:  0.056672267615795135\n",
      "################################  332  ################################\n",
      "Loss:  0.056272704154253006\n",
      "################################  333  ################################\n",
      "Loss:  0.05589146539568901\n",
      "################################  334  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.055529166013002396\n",
      "################################  335  ################################\n",
      "Loss:  0.055184755474328995\n",
      "################################  336  ################################\n",
      "Loss:  0.05485045909881592\n",
      "################################  337  ################################\n",
      "Loss:  0.05453779920935631\n",
      "################################  338  ################################\n",
      "Loss:  0.05423395335674286\n",
      "################################  339  ################################\n",
      "Loss:  0.053964726626873016\n",
      "################################  340  ################################\n",
      "Loss:  0.0537090003490448\n",
      "################################  341  ################################\n",
      "Loss:  0.05344947800040245\n",
      "################################  342  ################################\n",
      "Loss:  0.053192708641290665\n",
      "################################  343  ################################\n",
      "Loss:  0.052952274680137634\n",
      "################################  344  ################################\n",
      "Loss:  0.0527055449783802\n",
      "################################  345  ################################\n",
      "Loss:  0.052455633878707886\n",
      "################################  346  ################################\n",
      "Loss:  0.052165187895298004\n",
      "################################  347  ################################\n",
      "Loss:  0.05188998952507973\n",
      "################################  348  ################################\n",
      "Loss:  0.051550377160310745\n",
      "################################  349  ################################\n",
      "Loss:  0.05131009966135025\n",
      "################################  350  ################################\n",
      "Loss:  0.05105312541127205\n",
      "################################  351  ################################\n",
      "Loss:  0.05076485872268677\n",
      "################################  352  ################################\n",
      "Loss:  0.05046526715159416\n",
      "################################  353  ################################\n",
      "Loss:  0.050134073942899704\n",
      "################################  354  ################################\n",
      "Loss:  0.04976990818977356\n",
      "################################  355  ################################\n",
      "Loss:  0.049417052417993546\n",
      "################################  356  ################################\n",
      "Loss:  0.049017250537872314\n",
      "################################  357  ################################\n",
      "Loss:  0.04863181710243225\n",
      "################################  358  ################################\n",
      "Loss:  0.04815599322319031\n",
      "################################  359  ################################\n",
      "Loss:  0.04770130664110184\n",
      "################################  360  ################################\n",
      "Loss:  0.04722300171852112\n",
      "################################  361  ################################\n",
      "Loss:  0.046669915318489075\n",
      "################################  362  ################################\n",
      "Loss:  0.046213168650865555\n",
      "################################  363  ################################\n",
      "Loss:  0.04573117196559906\n",
      "################################  364  ################################\n",
      "Loss:  0.045039840042591095\n",
      "################################  365  ################################\n",
      "Loss:  0.04458768665790558\n",
      "################################  366  ################################\n",
      "Loss:  0.04411894083023071\n",
      "################################  367  ################################\n",
      "Loss:  0.04360334947705269\n",
      "################################  368  ################################\n",
      "Loss:  0.04309994727373123\n",
      "################################  369  ################################\n",
      "Loss:  0.04258815571665764\n",
      "################################  370  ################################\n",
      "Loss:  0.04209486022591591\n",
      "################################  371  ################################\n",
      "Loss:  0.04160357266664505\n",
      "################################  372  ################################\n",
      "Loss:  0.041133102029561996\n",
      "################################  373  ################################\n",
      "Loss:  0.04068775102496147\n",
      "################################  374  ################################\n",
      "Loss:  0.04025253280997276\n",
      "################################  375  ################################\n",
      "Loss:  0.039861083030700684\n",
      "################################  376  ################################\n",
      "Loss:  0.0394926480948925\n",
      "################################  377  ################################\n",
      "Loss:  0.039171308279037476\n",
      "################################  378  ################################\n",
      "Loss:  0.038880039006471634\n",
      "################################  379  ################################\n",
      "Loss:  0.038608450442552567\n",
      "################################  380  ################################\n",
      "Loss:  0.038368698209524155\n",
      "################################  381  ################################\n",
      "Loss:  0.03814861550927162\n",
      "################################  382  ################################\n",
      "Loss:  0.03796377405524254\n",
      "################################  383  ################################\n",
      "Loss:  0.037792906165122986\n",
      "################################  384  ################################\n",
      "Loss:  0.037647634744644165\n",
      "################################  385  ################################\n",
      "Loss:  0.03750729560852051\n",
      "################################  386  ################################\n",
      "Loss:  0.03735538199543953\n",
      "################################  387  ################################\n",
      "Loss:  0.037221863865852356\n",
      "################################  388  ################################\n",
      "Loss:  0.03709287941455841\n",
      "################################  389  ################################\n",
      "Loss:  0.036989886313676834\n",
      "################################  390  ################################\n",
      "Loss:  0.03686824440956116\n",
      "################################  391  ################################\n",
      "Loss:  0.03677331283688545\n",
      "################################  392  ################################\n",
      "Loss:  0.03666031360626221\n",
      "################################  393  ################################\n",
      "Loss:  0.036575812846422195\n",
      "################################  394  ################################\n",
      "Loss:  0.03647434711456299\n",
      "################################  395  ################################\n",
      "Loss:  0.03636403754353523\n",
      "################################  396  ################################\n",
      "Loss:  0.0362415574491024\n",
      "################################  397  ################################\n",
      "Loss:  0.036108843982219696\n",
      "################################  398  ################################\n",
      "Loss:  0.035969328135252\n",
      "################################  399  ################################\n",
      "Loss:  0.03582430258393288\n",
      "################################  400  ################################\n",
      "Loss:  0.03567594289779663\n",
      "################################  401  ################################\n",
      "Loss:  0.03552579507231712\n",
      "################################  402  ################################\n",
      "Loss:  0.03537384048104286\n",
      "################################  403  ################################\n",
      "Loss:  0.035220980644226074\n",
      "################################  404  ################################\n",
      "Loss:  0.035067569464445114\n",
      "################################  405  ################################\n",
      "Loss:  0.0349297970533371\n",
      "################################  406  ################################\n",
      "Loss:  0.03478996455669403\n",
      "################################  407  ################################\n",
      "Loss:  0.034646883606910706\n",
      "################################  408  ################################\n",
      "Loss:  0.03451022133231163\n",
      "################################  409  ################################\n",
      "Loss:  0.03438159450888634\n",
      "################################  410  ################################\n",
      "Loss:  0.034251775592565536\n",
      "################################  411  ################################\n",
      "Loss:  0.03413504362106323\n",
      "################################  412  ################################\n",
      "Loss:  0.03397870808839798\n",
      "################################  413  ################################\n",
      "Loss:  0.03382465988397598\n",
      "################################  414  ################################\n",
      "Loss:  0.03364870324730873\n",
      "################################  415  ################################\n",
      "Loss:  0.03344665840268135\n",
      "################################  416  ################################\n",
      "Loss:  0.033273711800575256\n",
      "################################  417  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.03306913748383522\n",
      "################################  418  ################################\n",
      "Loss:  0.03284090384840965\n",
      "################################  419  ################################\n",
      "Loss:  0.03259113430976868\n",
      "################################  420  ################################\n",
      "Loss:  0.032364655286073685\n",
      "################################  421  ################################\n",
      "Loss:  0.03214740753173828\n",
      "################################  422  ################################\n",
      "Loss:  0.031909845769405365\n",
      "################################  423  ################################\n",
      "Loss:  0.03170689567923546\n",
      "################################  424  ################################\n",
      "Loss:  0.03150411695241928\n",
      "################################  425  ################################\n",
      "Loss:  0.03130432590842247\n",
      "################################  426  ################################\n",
      "Loss:  0.0311150960624218\n",
      "################################  427  ################################\n",
      "Loss:  0.030923809856176376\n",
      "################################  428  ################################\n",
      "Loss:  0.030744997784495354\n",
      "################################  429  ################################\n",
      "Loss:  0.0305777620524168\n",
      "################################  430  ################################\n",
      "Loss:  0.030427193269133568\n",
      "################################  431  ################################\n",
      "Loss:  0.030282653868198395\n",
      "################################  432  ################################\n",
      "Loss:  0.030149534344673157\n",
      "################################  433  ################################\n",
      "Loss:  0.03002563677728176\n",
      "################################  434  ################################\n",
      "Loss:  0.029911251738667488\n",
      "################################  435  ################################\n",
      "Loss:  0.02980644255876541\n",
      "################################  436  ################################\n",
      "Loss:  0.02970930002629757\n",
      "################################  437  ################################\n",
      "Loss:  0.029620112851262093\n",
      "################################  438  ################################\n",
      "Loss:  0.029538486152887344\n",
      "################################  439  ################################\n",
      "Loss:  0.02945607900619507\n",
      "################################  440  ################################\n",
      "Loss:  0.029381684958934784\n",
      "################################  441  ################################\n",
      "Loss:  0.029305610805749893\n",
      "################################  442  ################################\n",
      "Loss:  0.029241465032100677\n",
      "################################  443  ################################\n",
      "Loss:  0.029170779511332512\n",
      "################################  444  ################################\n",
      "Loss:  0.029113806784152985\n",
      "################################  445  ################################\n",
      "Loss:  0.029025593772530556\n",
      "################################  446  ################################\n",
      "Loss:  0.02896154671907425\n",
      "################################  447  ################################\n",
      "Loss:  0.028859807178378105\n",
      "################################  448  ################################\n",
      "Loss:  0.028777286410331726\n",
      "################################  449  ################################\n",
      "Loss:  0.029012542217969894\n",
      "################################  450  ################################\n",
      "Loss:  0.028864078223705292\n",
      "################################  451  ################################\n",
      "Loss:  0.028716351836919785\n",
      "################################  452  ################################\n",
      "Loss:  0.02857278473675251\n",
      "################################  453  ################################\n",
      "Loss:  0.028435571119189262\n",
      "################################  454  ################################\n",
      "Loss:  0.02830592729151249\n",
      "################################  455  ################################\n",
      "Loss:  0.028185004368424416\n",
      "################################  456  ################################\n",
      "Loss:  0.028073173016309738\n",
      "################################  457  ################################\n",
      "Loss:  0.027970118448138237\n",
      "################################  458  ################################\n",
      "Loss:  0.027875550091266632\n",
      "################################  459  ################################\n",
      "Loss:  0.027788572013378143\n",
      "################################  460  ################################\n",
      "Loss:  0.02770865522325039\n",
      "################################  461  ################################\n",
      "Loss:  0.02763473615050316\n",
      "################################  462  ################################\n",
      "Loss:  0.027565982192754745\n",
      "################################  463  ################################\n",
      "Loss:  0.02750137634575367\n",
      "################################  464  ################################\n",
      "Loss:  0.02743985876441002\n",
      "################################  465  ################################\n",
      "Loss:  0.0273808091878891\n",
      "################################  466  ################################\n",
      "Loss:  0.027323661372065544\n",
      "################################  467  ################################\n",
      "Loss:  0.027264565229415894\n",
      "################################  468  ################################\n",
      "Loss:  0.027206655591726303\n",
      "################################  469  ################################\n",
      "Loss:  0.027148280292749405\n",
      "################################  470  ################################\n",
      "Loss:  0.02709999680519104\n",
      "################################  471  ################################\n",
      "Loss:  0.027051422744989395\n",
      "################################  472  ################################\n",
      "Loss:  0.026993952691555023\n",
      "################################  473  ################################\n",
      "Loss:  0.02691454440355301\n",
      "################################  474  ################################\n",
      "Loss:  0.026861349120736122\n",
      "################################  475  ################################\n",
      "Loss:  0.026796231046319008\n",
      "################################  476  ################################\n",
      "Loss:  0.02671314775943756\n",
      "################################  477  ################################\n",
      "Loss:  0.02662903070449829\n",
      "################################  478  ################################\n",
      "Loss:  0.026543548330664635\n",
      "################################  479  ################################\n",
      "Loss:  0.02645440213382244\n",
      "################################  480  ################################\n",
      "Loss:  0.026369387283921242\n",
      "################################  481  ################################\n",
      "Loss:  0.026282712817192078\n",
      "################################  482  ################################\n",
      "Loss:  0.026201099157333374\n",
      "################################  483  ################################\n",
      "Loss:  0.02612079493701458\n",
      "################################  484  ################################\n",
      "Loss:  0.026046056300401688\n",
      "################################  485  ################################\n",
      "Loss:  0.025975221768021584\n",
      "################################  486  ################################\n",
      "Loss:  0.02591252140700817\n",
      "################################  487  ################################\n",
      "Loss:  0.025851408019661903\n",
      "################################  488  ################################\n",
      "Loss:  0.025800321251153946\n",
      "################################  489  ################################\n",
      "Loss:  0.025751430541276932\n",
      "################################  490  ################################\n",
      "Loss:  0.025703810155391693\n",
      "################################  491  ################################\n",
      "Loss:  0.02566554769873619\n",
      "################################  492  ################################\n",
      "Loss:  0.025627240538597107\n",
      "################################  493  ################################\n",
      "Loss:  0.02559369057416916\n",
      "################################  494  ################################\n",
      "Loss:  0.02556590363383293\n",
      "################################  495  ################################\n",
      "Loss:  0.025539249181747437\n",
      "################################  496  ################################\n",
      "Loss:  0.025510232895612717\n",
      "################################  497  ################################\n",
      "Loss:  0.025480026379227638\n",
      "################################  498  ################################\n",
      "Loss:  0.02545289508998394\n",
      "################################  499  ################################\n",
      "Loss:  0.02541801705956459\n",
      "################################  500  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.02537636272609234\n",
      "################################  501  ################################\n",
      "Loss:  0.025319427251815796\n",
      "################################  502  ################################\n",
      "Loss:  0.02527501806616783\n",
      "################################  503  ################################\n",
      "Loss:  0.0252198725938797\n",
      "################################  504  ################################\n",
      "Loss:  0.025148598477244377\n",
      "################################  505  ################################\n",
      "Loss:  0.025083115324378014\n",
      "################################  506  ################################\n",
      "Loss:  0.024998445063829422\n",
      "################################  507  ################################\n",
      "Loss:  0.02489936538040638\n",
      "################################  508  ################################\n",
      "Loss:  0.024799894541502\n",
      "################################  509  ################################\n",
      "Loss:  0.024688079953193665\n",
      "################################  510  ################################\n",
      "Loss:  0.024549802765250206\n",
      "################################  511  ################################\n",
      "Loss:  0.024427728727459908\n",
      "################################  512  ################################\n",
      "Loss:  0.024273773655295372\n",
      "################################  513  ################################\n",
      "Loss:  0.02415093220770359\n",
      "################################  514  ################################\n",
      "Loss:  0.024020064622163773\n",
      "################################  515  ################################\n",
      "Loss:  0.023883823305368423\n",
      "################################  516  ################################\n",
      "Loss:  0.02375221624970436\n",
      "################################  517  ################################\n",
      "Loss:  0.023622766137123108\n",
      "################################  518  ################################\n",
      "Loss:  0.02350868284702301\n",
      "################################  519  ################################\n",
      "Loss:  0.02340494841337204\n",
      "################################  520  ################################\n",
      "Loss:  0.023306841030716896\n",
      "################################  521  ################################\n",
      "Loss:  0.023214710876345634\n",
      "################################  522  ################################\n",
      "Loss:  0.023128870874643326\n",
      "################################  523  ################################\n",
      "Loss:  0.02304929308593273\n",
      "################################  524  ################################\n",
      "Loss:  0.02297644503414631\n",
      "################################  525  ################################\n",
      "Loss:  0.022903846576809883\n",
      "################################  526  ################################\n",
      "Loss:  0.022827081382274628\n",
      "################################  527  ################################\n",
      "Loss:  0.022774748504161835\n",
      "################################  528  ################################\n",
      "Loss:  0.022728119045495987\n",
      "################################  529  ################################\n",
      "Loss:  0.02268308959901333\n",
      "################################  530  ################################\n",
      "Loss:  0.022640028968453407\n",
      "################################  531  ################################\n",
      "Loss:  0.022599952295422554\n",
      "################################  532  ################################\n",
      "Loss:  0.02256236970424652\n",
      "################################  533  ################################\n",
      "Loss:  0.022525297477841377\n",
      "################################  534  ################################\n",
      "Loss:  0.022488493472337723\n",
      "################################  535  ################################\n",
      "Loss:  0.022452859207987785\n",
      "################################  536  ################################\n",
      "Loss:  0.02241809107363224\n",
      "################################  537  ################################\n",
      "Loss:  0.022382695227861404\n",
      "################################  538  ################################\n",
      "Loss:  0.022345710545778275\n",
      "################################  539  ################################\n",
      "Loss:  0.022310499101877213\n",
      "################################  540  ################################\n",
      "Loss:  0.022274496033787727\n",
      "################################  541  ################################\n",
      "Loss:  0.022235842421650887\n",
      "################################  542  ################################\n",
      "Loss:  0.022192494943737984\n",
      "################################  543  ################################\n",
      "Loss:  0.022152448073029518\n",
      "################################  544  ################################\n",
      "Loss:  0.022110071033239365\n",
      "################################  545  ################################\n",
      "Loss:  0.02206380106508732\n",
      "################################  546  ################################\n",
      "Loss:  0.022012338042259216\n",
      "################################  547  ################################\n",
      "Loss:  0.021934153512120247\n",
      "################################  548  ################################\n",
      "Loss:  0.021897053346037865\n",
      "################################  549  ################################\n",
      "Loss:  0.02185305580496788\n",
      "################################  550  ################################\n",
      "Loss:  0.02181326597929001\n",
      "################################  551  ################################\n",
      "Loss:  0.02176094613969326\n",
      "################################  552  ################################\n",
      "Loss:  0.021723294630646706\n",
      "################################  553  ################################\n",
      "Loss:  0.02168073132634163\n",
      "################################  554  ################################\n",
      "Loss:  0.021631989628076553\n",
      "################################  555  ################################\n",
      "Loss:  0.021579235792160034\n",
      "################################  556  ################################\n",
      "Loss:  0.021522896364331245\n",
      "################################  557  ################################\n",
      "Loss:  0.021464919671416283\n",
      "################################  558  ################################\n",
      "Loss:  0.021405620500445366\n",
      "################################  559  ################################\n",
      "Loss:  0.02134653367102146\n",
      "################################  560  ################################\n",
      "Loss:  0.021287336945533752\n",
      "################################  561  ################################\n",
      "Loss:  0.021229997277259827\n",
      "################################  562  ################################\n",
      "Loss:  0.021171486005187035\n",
      "################################  563  ################################\n",
      "Loss:  0.02111894264817238\n",
      "################################  564  ################################\n",
      "Loss:  0.02106548100709915\n",
      "################################  565  ################################\n",
      "Loss:  0.02101120911538601\n",
      "################################  566  ################################\n",
      "Loss:  0.020958421751856804\n",
      "################################  567  ################################\n",
      "Loss:  0.02090335823595524\n",
      "################################  568  ################################\n",
      "Loss:  0.020851733162999153\n",
      "################################  569  ################################\n",
      "Loss:  0.020794933661818504\n",
      "################################  570  ################################\n",
      "Loss:  0.020760878920555115\n",
      "################################  571  ################################\n",
      "Loss:  0.020715875551104546\n",
      "################################  572  ################################\n",
      "Loss:  0.020662153139710426\n",
      "################################  573  ################################\n",
      "Loss:  0.020604360848665237\n",
      "################################  574  ################################\n",
      "Loss:  0.02053731307387352\n",
      "################################  575  ################################\n",
      "Loss:  0.02047678455710411\n",
      "################################  576  ################################\n",
      "Loss:  0.02041517198085785\n",
      "################################  577  ################################\n",
      "Loss:  0.020358506590127945\n",
      "################################  578  ################################\n",
      "Loss:  0.020301183685660362\n",
      "################################  579  ################################\n",
      "Loss:  0.02025153674185276\n",
      "################################  580  ################################\n",
      "Loss:  0.020199637860059738\n",
      "################################  581  ################################\n",
      "Loss:  0.020154476165771484\n",
      "################################  582  ################################\n",
      "Loss:  0.020108701661229134\n",
      "################################  583  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.02006811648607254\n",
      "################################  584  ################################\n",
      "Loss:  0.020028525963425636\n",
      "################################  585  ################################\n",
      "Loss:  0.019992778077721596\n",
      "################################  586  ################################\n",
      "Loss:  0.01995929516851902\n",
      "################################  587  ################################\n",
      "Loss:  0.019928427413105965\n",
      "################################  588  ################################\n",
      "Loss:  0.019900048151612282\n",
      "################################  589  ################################\n",
      "Loss:  0.01987343840301037\n",
      "################################  590  ################################\n",
      "Loss:  0.019849415868520737\n",
      "################################  591  ################################\n",
      "Loss:  0.019825972616672516\n",
      "################################  592  ################################\n",
      "Loss:  0.01980505883693695\n",
      "################################  593  ################################\n",
      "Loss:  0.019785072654485703\n",
      "################################  594  ################################\n",
      "Loss:  0.019768260419368744\n",
      "################################  595  ################################\n",
      "Loss:  0.019750285893678665\n",
      "################################  596  ################################\n",
      "Loss:  0.019733982160687447\n",
      "################################  597  ################################\n",
      "Loss:  0.019714543595910072\n",
      "################################  598  ################################\n",
      "Loss:  0.019695498049259186\n",
      "################################  599  ################################\n",
      "Loss:  0.019674882292747498\n",
      "################################  600  ################################\n",
      "Loss:  0.019654875621199608\n",
      "################################  601  ################################\n",
      "Loss:  0.019632581621408463\n",
      "################################  602  ################################\n",
      "Loss:  0.019604535773396492\n",
      "################################  603  ################################\n",
      "Loss:  0.01956980489194393\n",
      "################################  604  ################################\n",
      "Loss:  0.019542880356311798\n",
      "################################  605  ################################\n",
      "Loss:  0.01950700953602791\n",
      "################################  606  ################################\n",
      "Loss:  0.019473517313599586\n",
      "################################  607  ################################\n",
      "Loss:  0.019438380375504494\n",
      "################################  608  ################################\n",
      "Loss:  0.019397009164094925\n",
      "################################  609  ################################\n",
      "Loss:  0.01935392990708351\n",
      "################################  610  ################################\n",
      "Loss:  0.019306255504488945\n",
      "################################  611  ################################\n",
      "Loss:  0.019244248047471046\n",
      "################################  612  ################################\n",
      "Loss:  0.019189754500985146\n",
      "################################  613  ################################\n",
      "Loss:  0.019126195460557938\n",
      "################################  614  ################################\n",
      "Loss:  0.019053636118769646\n",
      "################################  615  ################################\n",
      "Loss:  0.018981002271175385\n",
      "################################  616  ################################\n",
      "Loss:  0.018901441246271133\n",
      "################################  617  ################################\n",
      "Loss:  0.018826602026820183\n",
      "################################  618  ################################\n",
      "Loss:  0.018734993413090706\n",
      "################################  619  ################################\n",
      "Loss:  0.01867186836898327\n",
      "################################  620  ################################\n",
      "Loss:  0.018600372597575188\n",
      "################################  621  ################################\n",
      "Loss:  0.018506692722439766\n",
      "################################  622  ################################\n",
      "Loss:  0.018438532948493958\n",
      "################################  623  ################################\n",
      "Loss:  0.01834326796233654\n",
      "################################  624  ################################\n",
      "Loss:  0.018271377310156822\n",
      "################################  625  ################################\n",
      "Loss:  0.01817333698272705\n",
      "################################  626  ################################\n",
      "Loss:  0.01807737909257412\n",
      "################################  627  ################################\n",
      "Loss:  0.01796409860253334\n",
      "################################  628  ################################\n",
      "Loss:  0.017834307625889778\n",
      "################################  629  ################################\n",
      "Loss:  0.017705490812659264\n",
      "################################  630  ################################\n",
      "Loss:  0.01757395640015602\n",
      "################################  631  ################################\n",
      "Loss:  0.017444690689444542\n",
      "################################  632  ################################\n",
      "Loss:  0.017318740487098694\n",
      "################################  633  ################################\n",
      "Loss:  0.01719893328845501\n",
      "################################  634  ################################\n",
      "Loss:  0.01708219386637211\n",
      "################################  635  ################################\n",
      "Loss:  0.0169724989682436\n",
      "################################  636  ################################\n",
      "Loss:  0.01686750538647175\n",
      "################################  637  ################################\n",
      "Loss:  0.016769621521234512\n",
      "################################  638  ################################\n",
      "Loss:  0.01667492650449276\n",
      "################################  639  ################################\n",
      "Loss:  0.016584331169724464\n",
      "################################  640  ################################\n",
      "Loss:  0.016493787989020348\n",
      "################################  641  ################################\n",
      "Loss:  0.01640276610851288\n",
      "################################  642  ################################\n",
      "Loss:  0.016319481655955315\n",
      "################################  643  ################################\n",
      "Loss:  0.016235869377851486\n",
      "################################  644  ################################\n",
      "Loss:  0.016162872314453125\n",
      "################################  645  ################################\n",
      "Loss:  0.016091883182525635\n",
      "################################  646  ################################\n",
      "Loss:  0.016030916944146156\n",
      "################################  647  ################################\n",
      "Loss:  0.01597112976014614\n",
      "################################  648  ################################\n",
      "Loss:  0.01591557450592518\n",
      "################################  649  ################################\n",
      "Loss:  0.015847234055399895\n",
      "################################  650  ################################\n",
      "Loss:  0.015808746218681335\n",
      "################################  651  ################################\n",
      "Loss:  0.01577330194413662\n",
      "################################  652  ################################\n",
      "Loss:  0.015728531405329704\n",
      "################################  653  ################################\n",
      "Loss:  0.015696298331022263\n",
      "################################  654  ################################\n",
      "Loss:  0.015666784718632698\n",
      "################################  655  ################################\n",
      "Loss:  0.015639375895261765\n",
      "################################  656  ################################\n",
      "Loss:  0.015613285824656487\n",
      "################################  657  ################################\n",
      "Loss:  0.015588496811687946\n",
      "################################  658  ################################\n",
      "Loss:  0.015563659369945526\n",
      "################################  659  ################################\n",
      "Loss:  0.015536514110863209\n",
      "################################  660  ################################\n",
      "Loss:  0.01550673320889473\n",
      "################################  661  ################################\n",
      "Loss:  0.015477207489311695\n",
      "################################  662  ################################\n",
      "Loss:  0.015442488715052605\n",
      "################################  663  ################################\n",
      "Loss:  0.015401023440063\n",
      "################################  664  ################################\n",
      "Loss:  0.01535155437886715\n",
      "################################  665  ################################\n",
      "Loss:  0.015305043198168278\n",
      "################################  666  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.015250012278556824\n",
      "################################  667  ################################\n",
      "Loss:  0.015179392881691456\n",
      "################################  668  ################################\n",
      "Loss:  0.015114176087081432\n",
      "################################  669  ################################\n",
      "Loss:  0.015042806975543499\n",
      "################################  670  ################################\n",
      "Loss:  0.014962131157517433\n",
      "################################  671  ################################\n",
      "Loss:  0.014887399040162563\n",
      "################################  672  ################################\n",
      "Loss:  0.014792434871196747\n",
      "################################  673  ################################\n",
      "Loss:  0.014718073420226574\n",
      "################################  674  ################################\n",
      "Loss:  0.014645924791693687\n",
      "################################  675  ################################\n",
      "Loss:  0.014577161520719528\n",
      "################################  676  ################################\n",
      "Loss:  0.014510668814182281\n",
      "################################  677  ################################\n",
      "Loss:  0.014444632455706596\n",
      "################################  678  ################################\n",
      "Loss:  0.014380766078829765\n",
      "################################  679  ################################\n",
      "Loss:  0.014320366084575653\n",
      "################################  680  ################################\n",
      "Loss:  0.014268282800912857\n",
      "################################  681  ################################\n",
      "Loss:  0.014221462421119213\n",
      "################################  682  ################################\n",
      "Loss:  0.014171183109283447\n",
      "################################  683  ################################\n",
      "Loss:  0.01412965077906847\n",
      "################################  684  ################################\n",
      "Loss:  0.014088091440498829\n",
      "################################  685  ################################\n",
      "Loss:  0.014053110964596272\n",
      "################################  686  ################################\n",
      "Loss:  0.014017784036695957\n",
      "################################  687  ################################\n",
      "Loss:  0.013988283462822437\n",
      "################################  688  ################################\n",
      "Loss:  0.01395886205136776\n",
      "################################  689  ################################\n",
      "Loss:  0.013933966867625713\n",
      "################################  690  ################################\n",
      "Loss:  0.013909202069044113\n",
      "################################  691  ################################\n",
      "Loss:  0.013888046145439148\n",
      "################################  692  ################################\n",
      "Loss:  0.013866357505321503\n",
      "################################  693  ################################\n",
      "Loss:  0.013846869580447674\n",
      "################################  694  ################################\n",
      "Loss:  0.013827384449541569\n",
      "################################  695  ################################\n",
      "Loss:  0.01380692794919014\n",
      "################################  696  ################################\n",
      "Loss:  0.013781322166323662\n",
      "################################  697  ################################\n",
      "Loss:  0.01375571172684431\n",
      "################################  698  ################################\n",
      "Loss:  0.013729519210755825\n",
      "################################  699  ################################\n",
      "Loss:  0.013702388852834702\n",
      "################################  700  ################################\n",
      "Loss:  0.013673470355570316\n",
      "################################  701  ################################\n",
      "Loss:  0.013644205406308174\n",
      "################################  702  ################################\n",
      "Loss:  0.013611245900392532\n",
      "################################  703  ################################\n",
      "Loss:  0.013578205369412899\n",
      "################################  704  ################################\n",
      "Loss:  0.013539498671889305\n",
      "################################  705  ################################\n",
      "Loss:  0.013505121693015099\n",
      "################################  706  ################################\n",
      "Loss:  0.013469061814248562\n",
      "################################  707  ################################\n",
      "Loss:  0.013424864038825035\n",
      "################################  708  ################################\n",
      "Loss:  0.013384025543928146\n",
      "################################  709  ################################\n",
      "Loss:  0.013339472003281116\n",
      "################################  710  ################################\n",
      "Loss:  0.013296603225171566\n",
      "################################  711  ################################\n",
      "Loss:  0.013256136327981949\n",
      "################################  712  ################################\n",
      "Loss:  0.01321452483534813\n",
      "################################  713  ################################\n",
      "Loss:  0.013179576024413109\n",
      "################################  714  ################################\n",
      "Loss:  0.013144949451088905\n",
      "################################  715  ################################\n",
      "Loss:  0.013109828345477581\n",
      "################################  716  ################################\n",
      "Loss:  0.013062763027846813\n",
      "################################  717  ################################\n",
      "Loss:  0.013023115694522858\n",
      "################################  718  ################################\n",
      "Loss:  0.01298869214951992\n",
      "################################  719  ################################\n",
      "Loss:  0.012954672798514366\n",
      "################################  720  ################################\n",
      "Loss:  0.012907955795526505\n",
      "################################  721  ################################\n",
      "Loss:  0.012858922593295574\n",
      "################################  722  ################################\n",
      "Loss:  0.012818333692848682\n",
      "################################  723  ################################\n",
      "Loss:  0.01278003491461277\n",
      "################################  724  ################################\n",
      "Loss:  0.0127534419298172\n",
      "################################  725  ################################\n",
      "Loss:  0.012723226100206375\n",
      "################################  726  ################################\n",
      "Loss:  0.01269233413040638\n",
      "################################  727  ################################\n",
      "Loss:  0.012665239162743092\n",
      "################################  728  ################################\n",
      "Loss:  0.012631094083189964\n",
      "################################  729  ################################\n",
      "Loss:  0.012605714611709118\n",
      "################################  730  ################################\n",
      "Loss:  0.012579693458974361\n",
      "################################  731  ################################\n",
      "Loss:  0.012553525157272816\n",
      "################################  732  ################################\n",
      "Loss:  0.012528974562883377\n",
      "################################  733  ################################\n",
      "Loss:  0.012502194382250309\n",
      "################################  734  ################################\n",
      "Loss:  0.012478113174438477\n",
      "################################  735  ################################\n",
      "Loss:  0.012452893890440464\n",
      "################################  736  ################################\n",
      "Loss:  0.01242821291089058\n",
      "################################  737  ################################\n",
      "Loss:  0.012402515858411789\n",
      "################################  738  ################################\n",
      "Loss:  0.012373657897114754\n",
      "################################  739  ################################\n",
      "Loss:  0.012345255352556705\n",
      "################################  740  ################################\n",
      "Loss:  0.012316710315644741\n",
      "################################  741  ################################\n",
      "Loss:  0.012288943864405155\n",
      "################################  742  ################################\n",
      "Loss:  0.01226179301738739\n",
      "################################  743  ################################\n",
      "Loss:  0.012235024943947792\n",
      "################################  744  ################################\n",
      "Loss:  0.012209451757371426\n",
      "################################  745  ################################\n",
      "Loss:  0.012183355167508125\n",
      "################################  746  ################################\n",
      "Loss:  0.012159045785665512\n",
      "################################  747  ################################\n",
      "Loss:  0.012134331278502941\n",
      "################################  748  ################################\n",
      "Loss:  0.012112602591514587\n",
      "################################  749  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.012090216390788555\n",
      "################################  750  ################################\n",
      "Loss:  0.012071376666426659\n",
      "################################  751  ################################\n",
      "Loss:  0.012052606791257858\n",
      "################################  752  ################################\n",
      "Loss:  0.012038005515933037\n",
      "################################  753  ################################\n",
      "Loss:  0.012023254297673702\n",
      "################################  754  ################################\n",
      "Loss:  0.012007756158709526\n",
      "################################  755  ################################\n",
      "Loss:  0.011987718753516674\n",
      "################################  756  ################################\n",
      "Loss:  0.011973912827670574\n",
      "################################  757  ################################\n",
      "Loss:  0.011959869414567947\n",
      "################################  758  ################################\n",
      "Loss:  0.011944940313696861\n",
      "################################  759  ################################\n",
      "Loss:  0.011919578537344933\n",
      "################################  760  ################################\n",
      "Loss:  0.011907229200005531\n",
      "################################  761  ################################\n",
      "Loss:  0.011893199756741524\n",
      "################################  762  ################################\n",
      "Loss:  0.011875954456627369\n",
      "################################  763  ################################\n",
      "Loss:  0.01185721904039383\n",
      "################################  764  ################################\n",
      "Loss:  0.011835336685180664\n",
      "################################  765  ################################\n",
      "Loss:  0.011812496930360794\n",
      "################################  766  ################################\n",
      "Loss:  0.01178626250475645\n",
      "################################  767  ################################\n",
      "Loss:  0.011761137284338474\n",
      "################################  768  ################################\n",
      "Loss:  0.011734853498637676\n",
      "################################  769  ################################\n",
      "Loss:  0.011705441400408745\n",
      "################################  770  ################################\n",
      "Loss:  0.011670558713376522\n",
      "################################  771  ################################\n",
      "Loss:  0.011643525213003159\n",
      "################################  772  ################################\n",
      "Loss:  0.011609047651290894\n",
      "################################  773  ################################\n",
      "Loss:  0.011577506549656391\n",
      "################################  774  ################################\n",
      "Loss:  0.011538760736584663\n",
      "################################  775  ################################\n",
      "Loss:  0.011495577171444893\n",
      "################################  776  ################################\n",
      "Loss:  0.011442560702562332\n",
      "################################  777  ################################\n",
      "Loss:  0.011400140821933746\n",
      "################################  778  ################################\n",
      "Loss:  0.011349151842296124\n",
      "################################  779  ################################\n",
      "Loss:  0.01129709929227829\n",
      "################################  780  ################################\n",
      "Loss:  0.011231324635446072\n",
      "################################  781  ################################\n",
      "Loss:  0.011185754090547562\n",
      "################################  782  ################################\n",
      "Loss:  0.01113665197044611\n",
      "################################  783  ################################\n",
      "Loss:  0.0110923508182168\n",
      "################################  784  ################################\n",
      "Loss:  0.011046979576349258\n",
      "################################  785  ################################\n",
      "Loss:  0.010998000390827656\n",
      "################################  786  ################################\n",
      "Loss:  0.010951376520097256\n",
      "################################  787  ################################\n",
      "Loss:  0.010905465111136436\n",
      "################################  788  ################################\n",
      "Loss:  0.010864759795367718\n",
      "################################  789  ################################\n",
      "Loss:  0.010824191384017467\n",
      "################################  790  ################################\n",
      "Loss:  0.010789535008370876\n",
      "################################  791  ################################\n",
      "Loss:  0.010754653252661228\n",
      "################################  792  ################################\n",
      "Loss:  0.010725727304816246\n",
      "################################  793  ################################\n",
      "Loss:  0.01069730520248413\n",
      "################################  794  ################################\n",
      "Loss:  0.010673284530639648\n",
      "################################  795  ################################\n",
      "Loss:  0.010650510899722576\n",
      "################################  796  ################################\n",
      "Loss:  0.010630951263010502\n",
      "################################  797  ################################\n",
      "Loss:  0.01061167661100626\n",
      "################################  798  ################################\n",
      "Loss:  0.010594652034342289\n",
      "################################  799  ################################\n",
      "Loss:  0.010577843524515629\n",
      "################################  800  ################################\n",
      "Loss:  0.010562357492744923\n",
      "################################  801  ################################\n",
      "Loss:  0.010547112673521042\n",
      "################################  802  ################################\n",
      "Loss:  0.010532381944358349\n",
      "################################  803  ################################\n",
      "Loss:  0.010517745278775692\n",
      "################################  804  ################################\n",
      "Loss:  0.010502652265131474\n",
      "################################  805  ################################\n",
      "Loss:  0.010487141087651253\n",
      "################################  806  ################################\n",
      "Loss:  0.010470638982951641\n",
      "################################  807  ################################\n",
      "Loss:  0.010454408824443817\n",
      "################################  808  ################################\n",
      "Loss:  0.01043633557856083\n",
      "################################  809  ################################\n",
      "Loss:  0.010419249534606934\n",
      "################################  810  ################################\n",
      "Loss:  0.010400526225566864\n",
      "################################  811  ################################\n",
      "Loss:  0.010380415245890617\n",
      "################################  812  ################################\n",
      "Loss:  0.010358617641031742\n",
      "################################  813  ################################\n",
      "Loss:  0.010338209569454193\n",
      "################################  814  ################################\n",
      "Loss:  0.010318910703063011\n",
      "################################  815  ################################\n",
      "Loss:  0.01029759831726551\n",
      "################################  816  ################################\n",
      "Loss:  0.010276327840983868\n",
      "################################  817  ################################\n",
      "Loss:  0.010250721126794815\n",
      "################################  818  ################################\n",
      "Loss:  0.010226023383438587\n",
      "################################  819  ################################\n",
      "Loss:  0.01019684225320816\n",
      "################################  820  ################################\n",
      "Loss:  0.010170924477279186\n",
      "################################  821  ################################\n",
      "Loss:  0.01014347281306982\n",
      "################################  822  ################################\n",
      "Loss:  0.010115327313542366\n",
      "################################  823  ################################\n",
      "Loss:  0.010087687522172928\n",
      "################################  824  ################################\n",
      "Loss:  0.010059235617518425\n",
      "################################  825  ################################\n",
      "Loss:  0.010033318772912025\n",
      "################################  826  ################################\n",
      "Loss:  0.01000538282096386\n",
      "################################  827  ################################\n",
      "Loss:  0.00998026691377163\n",
      "################################  828  ################################\n",
      "Loss:  0.009953822940587997\n",
      "################################  829  ################################\n",
      "Loss:  0.009929611347615719\n",
      "################################  830  ################################\n",
      "Loss:  0.009904664009809494\n",
      "################################  831  ################################\n",
      "Loss:  0.009880876168608665\n",
      "################################  832  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.009855076670646667\n",
      "################################  833  ################################\n",
      "Loss:  0.009830720722675323\n",
      "################################  834  ################################\n",
      "Loss:  0.009807256050407887\n",
      "################################  835  ################################\n",
      "Loss:  0.009783230721950531\n",
      "################################  836  ################################\n",
      "Loss:  0.009761340916156769\n",
      "################################  837  ################################\n",
      "Loss:  0.009736424311995506\n",
      "################################  838  ################################\n",
      "Loss:  0.009714982472360134\n",
      "################################  839  ################################\n",
      "Loss:  0.009689541533589363\n",
      "################################  840  ################################\n",
      "Loss:  0.00966841634362936\n",
      "################################  841  ################################\n",
      "Loss:  0.009646070189774036\n",
      "################################  842  ################################\n",
      "Loss:  0.00962119735777378\n",
      "################################  843  ################################\n",
      "Loss:  0.009596572257578373\n",
      "################################  844  ################################\n",
      "Loss:  0.009572561830282211\n",
      "################################  845  ################################\n",
      "Loss:  0.009546350687742233\n",
      "################################  846  ################################\n",
      "Loss:  0.009522056207060814\n",
      "################################  847  ################################\n",
      "Loss:  0.009497320279479027\n",
      "################################  848  ################################\n",
      "Loss:  0.009475689381361008\n",
      "################################  849  ################################\n",
      "Loss:  0.009451163001358509\n",
      "################################  850  ################################\n",
      "Loss:  0.009431139566004276\n",
      "################################  851  ################################\n",
      "Loss:  0.009408054873347282\n",
      "################################  852  ################################\n",
      "Loss:  0.009389393962919712\n",
      "################################  853  ################################\n",
      "Loss:  0.009370837360620499\n",
      "################################  854  ################################\n",
      "Loss:  0.009357359260320663\n",
      "################################  855  ################################\n",
      "Loss:  0.009343619458377361\n",
      "################################  856  ################################\n",
      "Loss:  0.009329520165920258\n",
      "################################  857  ################################\n",
      "Loss:  0.009315060451626778\n",
      "################################  858  ################################\n",
      "Loss:  0.009299817495048046\n",
      "################################  859  ################################\n",
      "Loss:  0.009285462088882923\n",
      "################################  860  ################################\n",
      "Loss:  0.009274540469050407\n",
      "################################  861  ################################\n",
      "Loss:  0.009462093934416771\n",
      "################################  862  ################################\n",
      "Loss:  0.009413369931280613\n",
      "################################  863  ################################\n",
      "Loss:  0.009373364970088005\n",
      "################################  864  ################################\n",
      "Loss:  0.009340312331914902\n",
      "################################  865  ################################\n",
      "Loss:  0.009313026443123817\n",
      "################################  866  ################################\n",
      "Loss:  0.009290491230785847\n",
      "################################  867  ################################\n",
      "Loss:  0.009271806105971336\n",
      "################################  868  ################################\n",
      "Loss:  0.009256374090909958\n",
      "################################  869  ################################\n",
      "Loss:  0.009243538603186607\n",
      "################################  870  ################################\n",
      "Loss:  0.00923291314393282\n",
      "################################  871  ################################\n",
      "Loss:  0.00922397244721651\n",
      "################################  872  ################################\n",
      "Loss:  0.009216438978910446\n",
      "################################  873  ################################\n",
      "Loss:  0.009210069663822651\n",
      "################################  874  ################################\n",
      "Loss:  0.00920459907501936\n",
      "################################  875  ################################\n",
      "Loss:  0.009199962951242924\n",
      "################################  876  ################################\n",
      "Loss:  0.009195911698043346\n",
      "################################  877  ################################\n",
      "Loss:  0.009192309342324734\n",
      "################################  878  ################################\n",
      "Loss:  0.00918921921402216\n",
      "################################  879  ################################\n",
      "Loss:  0.009186327457427979\n",
      "################################  880  ################################\n",
      "Loss:  0.009183654561638832\n",
      "################################  881  ################################\n",
      "Loss:  0.009181162342429161\n",
      "################################  882  ################################\n",
      "Loss:  0.009178772568702698\n",
      "################################  883  ################################\n",
      "Loss:  0.009176371619105339\n",
      "################################  884  ################################\n",
      "Loss:  0.0091740433126688\n",
      "################################  885  ################################\n",
      "Loss:  0.00917160976678133\n",
      "################################  886  ################################\n",
      "Loss:  0.0091692004352808\n",
      "################################  887  ################################\n",
      "Loss:  0.009166311472654343\n",
      "################################  888  ################################\n",
      "Loss:  0.009163526818156242\n",
      "################################  889  ################################\n",
      "Loss:  0.009160656481981277\n",
      "################################  890  ################################\n",
      "Loss:  0.00915758591145277\n",
      "################################  891  ################################\n",
      "Loss:  0.009154114872217178\n",
      "################################  892  ################################\n",
      "Loss:  0.009150414727628231\n",
      "################################  893  ################################\n",
      "Loss:  0.009147010743618011\n",
      "################################  894  ################################\n",
      "Loss:  0.009142638184130192\n",
      "################################  895  ################################\n",
      "Loss:  0.00913846492767334\n",
      "################################  896  ################################\n",
      "Loss:  0.009132996201515198\n",
      "################################  897  ################################\n",
      "Loss:  0.009123599156737328\n",
      "################################  898  ################################\n",
      "Loss:  0.009115751832723618\n",
      "################################  899  ################################\n",
      "Loss:  0.009107249788939953\n",
      "################################  900  ################################\n",
      "Loss:  0.009094679728150368\n",
      "################################  901  ################################\n",
      "Loss:  0.009083446115255356\n",
      "################################  902  ################################\n",
      "Loss:  0.009069060906767845\n",
      "################################  903  ################################\n",
      "Loss:  0.00905604287981987\n",
      "################################  904  ################################\n",
      "Loss:  0.00904187560081482\n",
      "################################  905  ################################\n",
      "Loss:  0.009025960229337215\n",
      "################################  906  ################################\n",
      "Loss:  0.009007226675748825\n",
      "################################  907  ################################\n",
      "Loss:  0.008990045636892319\n",
      "################################  908  ################################\n",
      "Loss:  0.008973128162324429\n",
      "################################  909  ################################\n",
      "Loss:  0.008956478908658028\n",
      "################################  910  ################################\n",
      "Loss:  0.008939101360738277\n",
      "################################  911  ################################\n",
      "Loss:  0.008923441171646118\n",
      "################################  912  ################################\n",
      "Loss:  0.00890790019184351\n",
      "################################  913  ################################\n",
      "Loss:  0.008893745020031929\n",
      "################################  914  ################################\n",
      "Loss:  0.008880121633410454\n",
      "################################  915  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.00886534433811903\n",
      "################################  916  ################################\n",
      "Loss:  0.008853999897837639\n",
      "################################  917  ################################\n",
      "Loss:  0.008843542076647282\n",
      "################################  918  ################################\n",
      "Loss:  0.008832924999296665\n",
      "################################  919  ################################\n",
      "Loss:  0.008822576142847538\n",
      "################################  920  ################################\n",
      "Loss:  0.008809906430542469\n",
      "################################  921  ################################\n",
      "Loss:  0.008799739181995392\n",
      "################################  922  ################################\n",
      "Loss:  0.008790048770606518\n",
      "################################  923  ################################\n",
      "Loss:  0.00877503864467144\n",
      "################################  924  ################################\n",
      "Loss:  0.008765283972024918\n",
      "################################  925  ################################\n",
      "Loss:  0.008754166774451733\n",
      "################################  926  ################################\n",
      "Loss:  0.008739140816032887\n",
      "################################  927  ################################\n",
      "Loss:  0.008725224994122982\n",
      "################################  928  ################################\n",
      "Loss:  0.008709318935871124\n",
      "################################  929  ################################\n",
      "Loss:  0.008694165386259556\n",
      "################################  930  ################################\n",
      "Loss:  0.008674674667418003\n",
      "################################  931  ################################\n",
      "Loss:  0.008656090125441551\n",
      "################################  932  ################################\n",
      "Loss:  0.008634868077933788\n",
      "################################  933  ################################\n",
      "Loss:  0.008611094206571579\n",
      "################################  934  ################################\n",
      "Loss:  0.00858973152935505\n",
      "################################  935  ################################\n",
      "Loss:  0.008567189797759056\n",
      "################################  936  ################################\n",
      "Loss:  0.008540619164705276\n",
      "################################  937  ################################\n",
      "Loss:  0.008515547029674053\n",
      "################################  938  ################################\n",
      "Loss:  0.008489159867167473\n",
      "################################  939  ################################\n",
      "Loss:  0.008464599959552288\n",
      "################################  940  ################################\n",
      "Loss:  0.008440522477030754\n",
      "################################  941  ################################\n",
      "Loss:  0.008418197743594646\n",
      "################################  942  ################################\n",
      "Loss:  0.008398027159273624\n",
      "################################  943  ################################\n",
      "Loss:  0.008378078229725361\n",
      "################################  944  ################################\n",
      "Loss:  0.00836104340851307\n",
      "################################  945  ################################\n",
      "Loss:  0.008345984853804111\n",
      "################################  946  ################################\n",
      "Loss:  0.008331784047186375\n",
      "################################  947  ################################\n",
      "Loss:  0.008319120854139328\n",
      "################################  948  ################################\n",
      "Loss:  0.008307697251439095\n",
      "################################  949  ################################\n",
      "Loss:  0.008297272957861423\n",
      "################################  950  ################################\n",
      "Loss:  0.008288511075079441\n",
      "################################  951  ################################\n",
      "Loss:  0.008279968053102493\n",
      "################################  952  ################################\n",
      "Loss:  0.008273798041045666\n",
      "################################  953  ################################\n",
      "Loss:  0.008266955614089966\n",
      "################################  954  ################################\n",
      "Loss:  0.008262421004474163\n",
      "################################  955  ################################\n",
      "Loss:  0.008253245614469051\n",
      "################################  956  ################################\n",
      "Loss:  0.008250206708908081\n",
      "################################  957  ################################\n",
      "Loss:  0.008244974538683891\n",
      "################################  958  ################################\n",
      "Loss:  0.008239771239459515\n",
      "################################  959  ################################\n",
      "Loss:  0.008234228938817978\n",
      "################################  960  ################################\n",
      "Loss:  0.008228451944887638\n",
      "################################  961  ################################\n",
      "Loss:  0.008221825584769249\n",
      "################################  962  ################################\n",
      "Loss:  0.008215521462261677\n",
      "################################  963  ################################\n",
      "Loss:  0.00820762850344181\n",
      "################################  964  ################################\n",
      "Loss:  0.008200529962778091\n",
      "################################  965  ################################\n",
      "Loss:  0.008190960623323917\n",
      "################################  966  ################################\n",
      "Loss:  0.008183827623724937\n",
      "################################  967  ################################\n",
      "Loss:  0.008175801485776901\n",
      "################################  968  ################################\n",
      "Loss:  0.008165372535586357\n",
      "################################  969  ################################\n",
      "Loss:  0.008153514936566353\n",
      "################################  970  ################################\n",
      "Loss:  0.008142400532960892\n",
      "################################  971  ################################\n",
      "Loss:  0.008128657005727291\n",
      "################################  972  ################################\n",
      "Loss:  0.008111048489809036\n",
      "################################  973  ################################\n",
      "Loss:  0.00809471309185028\n",
      "################################  974  ################################\n",
      "Loss:  0.008076418191194534\n",
      "################################  975  ################################\n",
      "Loss:  0.008053588680922985\n",
      "################################  976  ################################\n",
      "Loss:  0.008031969889998436\n",
      "################################  977  ################################\n",
      "Loss:  0.008003349415957928\n",
      "################################  978  ################################\n",
      "Loss:  0.007981759496033192\n",
      "################################  979  ################################\n",
      "Loss:  0.007955825887620449\n",
      "################################  980  ################################\n",
      "Loss:  0.007926623336970806\n",
      "################################  981  ################################\n",
      "Loss:  0.007891003042459488\n",
      "################################  982  ################################\n",
      "Loss:  0.007852483540773392\n",
      "################################  983  ################################\n",
      "Loss:  0.007812774740159512\n",
      "################################  984  ################################\n",
      "Loss:  0.007765724323689938\n",
      "################################  985  ################################\n",
      "Loss:  0.007722494658082724\n",
      "################################  986  ################################\n",
      "Loss:  0.007654836867004633\n",
      "################################  987  ################################\n",
      "Loss:  0.007619501557201147\n",
      "################################  988  ################################\n",
      "Loss:  0.007581614889204502\n",
      "################################  989  ################################\n",
      "Loss:  0.00754272285848856\n",
      "################################  990  ################################\n",
      "Loss:  0.007503995206207037\n",
      "################################  991  ################################\n",
      "Loss:  0.0074655478820204735\n",
      "################################  992  ################################\n",
      "Loss:  0.007425716612488031\n",
      "################################  993  ################################\n",
      "Loss:  0.007396433036774397\n",
      "################################  994  ################################\n",
      "Loss:  0.0073672691360116005\n",
      "################################  995  ################################\n",
      "Loss:  0.007339270319789648\n",
      "################################  996  ################################\n",
      "Loss:  0.00732546066865325\n",
      "################################  997  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.007301412057131529\n",
      "################################  998  ################################\n",
      "Loss:  0.007279457990080118\n",
      "################################  999  ################################\n",
      "Loss:  0.007258656434714794\n",
      "################################  1000  ################################\n",
      "Loss:  0.007240388076752424\n",
      "################################  1001  ################################\n",
      "Loss:  0.007221173960715532\n",
      "################################  1002  ################################\n",
      "Loss:  0.0072054616175591946\n",
      "################################  1003  ################################\n",
      "Loss:  0.007190784905105829\n",
      "################################  1004  ################################\n",
      "Loss:  0.007178702391684055\n",
      "################################  1005  ################################\n",
      "Loss:  0.0071675218641757965\n",
      "################################  1006  ################################\n",
      "Loss:  0.007158023305237293\n",
      "################################  1007  ################################\n",
      "Loss:  0.007148394826799631\n",
      "################################  1008  ################################\n",
      "Loss:  0.007139778696000576\n",
      "################################  1009  ################################\n",
      "Loss:  0.00713170412927866\n",
      "################################  1010  ################################\n",
      "Loss:  0.007124574389308691\n",
      "################################  1011  ################################\n",
      "Loss:  0.0071175592020154\n",
      "################################  1012  ################################\n",
      "Loss:  0.0071107312105596066\n",
      "################################  1013  ################################\n",
      "Loss:  0.007104620337486267\n",
      "################################  1014  ################################\n",
      "Loss:  0.007098783738911152\n",
      "################################  1015  ################################\n",
      "Loss:  0.007093449123203754\n",
      "################################  1016  ################################\n",
      "Loss:  0.007087509613484144\n",
      "################################  1017  ################################\n",
      "Loss:  0.007082781754434109\n",
      "################################  1018  ################################\n",
      "Loss:  0.00707798358052969\n",
      "################################  1019  ################################\n",
      "Loss:  0.007073772139847279\n",
      "################################  1020  ################################\n",
      "Loss:  0.007068932987749577\n",
      "################################  1021  ################################\n",
      "Loss:  0.007064789067953825\n",
      "################################  1022  ################################\n",
      "Loss:  0.007060128264129162\n",
      "################################  1023  ################################\n",
      "Loss:  0.007055938243865967\n",
      "################################  1024  ################################\n",
      "Loss:  0.0070516448467969894\n",
      "################################  1025  ################################\n",
      "Loss:  0.007047345396131277\n",
      "################################  1026  ################################\n",
      "Loss:  0.007044394500553608\n",
      "################################  1027  ################################\n",
      "Loss:  0.007040711585432291\n",
      "################################  1028  ################################\n",
      "Loss:  0.007033603265881538\n",
      "################################  1029  ################################\n",
      "Loss:  0.007026516832411289\n",
      "################################  1030  ################################\n",
      "Loss:  0.007023181766271591\n",
      "################################  1031  ################################\n",
      "Loss:  0.007015124429017305\n",
      "################################  1032  ################################\n",
      "Loss:  0.007005564868450165\n",
      "################################  1033  ################################\n",
      "Loss:  0.006994768511503935\n",
      "################################  1034  ################################\n",
      "Loss:  0.006982977967709303\n",
      "################################  1035  ################################\n",
      "Loss:  0.006971023511141539\n",
      "################################  1036  ################################\n",
      "Loss:  0.006959136109799147\n",
      "################################  1037  ################################\n",
      "Loss:  0.006948262453079224\n",
      "################################  1038  ################################\n",
      "Loss:  0.006938448175787926\n",
      "################################  1039  ################################\n",
      "Loss:  0.0069285505451262\n",
      "################################  1040  ################################\n",
      "Loss:  0.006919930689036846\n",
      "################################  1041  ################################\n",
      "Loss:  0.006912355776876211\n",
      "################################  1042  ################################\n",
      "Loss:  0.00690618809312582\n",
      "################################  1043  ################################\n",
      "Loss:  0.006900763139128685\n",
      "################################  1044  ################################\n",
      "Loss:  0.006896289065480232\n",
      "################################  1045  ################################\n",
      "Loss:  0.006892561912536621\n",
      "################################  1046  ################################\n",
      "Loss:  0.0068894666619598866\n",
      "################################  1047  ################################\n",
      "Loss:  0.006886852439492941\n",
      "################################  1048  ################################\n",
      "Loss:  0.006884631235152483\n",
      "################################  1049  ################################\n",
      "Loss:  0.006882782094180584\n",
      "################################  1050  ################################\n",
      "Loss:  0.006881262641400099\n",
      "################################  1051  ################################\n",
      "Loss:  0.0068799471482634544\n",
      "################################  1052  ################################\n",
      "Loss:  0.00687897764146328\n",
      "################################  1053  ################################\n",
      "Loss:  0.006877906620502472\n",
      "################################  1054  ################################\n",
      "Loss:  0.006877066567540169\n",
      "################################  1055  ################################\n",
      "Loss:  0.006875862367451191\n",
      "################################  1056  ################################\n",
      "Loss:  0.006874816957861185\n",
      "################################  1057  ################################\n",
      "Loss:  0.006874301005154848\n",
      "################################  1058  ################################\n",
      "Loss:  0.006873550359159708\n",
      "################################  1059  ################################\n",
      "Loss:  0.006872927770018578\n",
      "################################  1060  ################################\n",
      "Loss:  0.006871273275464773\n",
      "################################  1061  ################################\n",
      "Loss:  0.0068707335740327835\n",
      "################################  1062  ################################\n",
      "Loss:  0.006868726573884487\n",
      "################################  1063  ################################\n",
      "Loss:  0.0068674953654408455\n",
      "################################  1064  ################################\n",
      "Loss:  0.0068672881461679935\n",
      "################################  1065  ################################\n",
      "Loss:  0.006866018753498793\n",
      "################################  1066  ################################\n",
      "Loss:  0.006863908376544714\n",
      "################################  1067  ################################\n",
      "Loss:  0.0068615274503827095\n",
      "################################  1068  ################################\n",
      "Loss:  0.006858689244836569\n",
      "################################  1069  ################################\n",
      "Loss:  0.006855727173388004\n",
      "################################  1070  ################################\n",
      "Loss:  0.006851883139461279\n",
      "################################  1071  ################################\n",
      "Loss:  0.006848721764981747\n",
      "################################  1072  ################################\n",
      "Loss:  0.006845656782388687\n",
      "################################  1073  ################################\n",
      "Loss:  0.006843451876193285\n",
      "################################  1074  ################################\n",
      "Loss:  0.006841822527348995\n",
      "################################  1075  ################################\n",
      "Loss:  0.006840541027486324\n",
      "################################  1076  ################################\n",
      "Loss:  0.006839216221123934\n",
      "################################  1077  ################################\n",
      "Loss:  0.006838073022663593\n",
      "################################  1078  ################################\n",
      "Loss:  0.0068380809389054775\n",
      "################################  1079  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0068380278535187244\n",
      "################################  1080  ################################\n",
      "Loss:  0.006837903987616301\n",
      "################################  1081  ################################\n",
      "Loss:  0.006840136367827654\n",
      "################################  1082  ################################\n",
      "Loss:  0.006841831840574741\n",
      "################################  1083  ################################\n",
      "Loss:  0.006843983195722103\n",
      "################################  1084  ################################\n",
      "Loss:  0.006848854012787342\n",
      "################################  1085  ################################\n",
      "Loss:  0.00685142632573843\n",
      "################################  1086  ################################\n",
      "Loss:  0.006860679946839809\n",
      "################################  1087  ################################\n",
      "Loss:  0.006863727234303951\n",
      "################################  1088  ################################\n",
      "Loss:  0.0068650138564407825\n",
      "################################  1089  ################################\n",
      "Loss:  0.006872104946523905\n",
      "################################  1090  ################################\n",
      "Loss:  0.006874415557831526\n",
      "################################  1091  ################################\n",
      "Loss:  0.006875327322632074\n",
      "################################  1092  ################################\n",
      "Loss:  0.006875691004097462\n",
      "################################  1093  ################################\n",
      "Loss:  0.006874094717204571\n",
      "################################  1094  ################################\n",
      "Loss:  0.006876824423670769\n",
      "################################  1095  ################################\n",
      "Loss:  0.006876273546367884\n",
      "################################  1096  ################################\n",
      "Loss:  0.006876650266349316\n",
      "################################  1097  ################################\n",
      "Loss:  0.006876508705317974\n",
      "################################  1098  ################################\n",
      "Loss:  0.0068750642240047455\n",
      "################################  1099  ################################\n",
      "Loss:  0.006875337101519108\n",
      "################################  1100  ################################\n",
      "Loss:  0.0068737175315618515\n",
      "################################  1101  ################################\n",
      "Loss:  0.006873102393001318\n",
      "################################  1102  ################################\n",
      "Loss:  0.006871265824884176\n",
      "################################  1103  ################################\n",
      "Loss:  0.006869790144264698\n",
      "################################  1104  ################################\n",
      "Loss:  0.00686922250315547\n",
      "################################  1105  ################################\n",
      "Loss:  0.006867543328553438\n",
      "################################  1106  ################################\n",
      "Loss:  0.006863313727080822\n",
      "################################  1107  ################################\n",
      "Loss:  0.006860974244773388\n",
      "################################  1108  ################################\n",
      "Loss:  0.006857899483293295\n",
      "################################  1109  ################################\n",
      "Loss:  0.006857610307633877\n",
      "################################  1110  ################################\n",
      "Loss:  0.006854495033621788\n",
      "################################  1111  ################################\n",
      "Loss:  0.006845773197710514\n",
      "################################  1112  ################################\n",
      "Loss:  0.0068422784097492695\n",
      "################################  1113  ################################\n",
      "Loss:  0.006836456712335348\n",
      "################################  1114  ################################\n",
      "Loss:  0.0068298159167170525\n",
      "################################  1115  ################################\n",
      "Loss:  0.006821724586188793\n",
      "################################  1116  ################################\n",
      "Loss:  0.006813731975853443\n",
      "################################  1117  ################################\n",
      "Loss:  0.0068033188581466675\n",
      "################################  1118  ################################\n",
      "Loss:  0.00679472740739584\n",
      "################################  1119  ################################\n",
      "Loss:  0.006791191641241312\n",
      "################################  1120  ################################\n",
      "Loss:  0.00678314408287406\n",
      "################################  1121  ################################\n",
      "Loss:  0.006773288361728191\n",
      "################################  1122  ################################\n",
      "Loss:  0.006761563941836357\n",
      "################################  1123  ################################\n",
      "Loss:  0.006748680956661701\n",
      "################################  1124  ################################\n",
      "Loss:  0.0067373597994446754\n",
      "################################  1125  ################################\n",
      "Loss:  0.006726183462888002\n",
      "################################  1126  ################################\n",
      "Loss:  0.006715670228004456\n",
      "################################  1127  ################################\n",
      "Loss:  0.006703848950564861\n",
      "################################  1128  ################################\n",
      "Loss:  0.006693006027489901\n",
      "################################  1129  ################################\n",
      "Loss:  0.006679496727883816\n",
      "################################  1130  ################################\n",
      "Loss:  0.006669268943369389\n",
      "################################  1131  ################################\n",
      "Loss:  0.006656849756836891\n",
      "################################  1132  ################################\n",
      "Loss:  0.006648357026278973\n",
      "################################  1133  ################################\n",
      "Loss:  0.00663752481341362\n",
      "################################  1134  ################################\n",
      "Loss:  0.006631878204643726\n",
      "################################  1135  ################################\n",
      "Loss:  0.006626067217439413\n",
      "################################  1136  ################################\n",
      "Loss:  0.006620589178055525\n",
      "################################  1137  ################################\n",
      "Loss:  0.006627980154007673\n",
      "################################  1138  ################################\n",
      "Loss:  0.0066232536919415\n",
      "################################  1139  ################################\n",
      "Loss:  0.006618858780711889\n",
      "################################  1140  ################################\n",
      "Loss:  0.006614864803850651\n",
      "################################  1141  ################################\n",
      "Loss:  0.006611109711229801\n",
      "################################  1142  ################################\n",
      "Loss:  0.006607706658542156\n",
      "################################  1143  ################################\n",
      "Loss:  0.006604364607483149\n",
      "################################  1144  ################################\n",
      "Loss:  0.00660132197663188\n",
      "################################  1145  ################################\n",
      "Loss:  0.006598443724215031\n",
      "################################  1146  ################################\n",
      "Loss:  0.006595885846763849\n",
      "################################  1147  ################################\n",
      "Loss:  0.006593374069780111\n",
      "################################  1148  ################################\n",
      "Loss:  0.00659112399443984\n",
      "################################  1149  ################################\n",
      "Loss:  0.0065887258388102055\n",
      "################################  1150  ################################\n",
      "Loss:  0.006586370058357716\n",
      "################################  1151  ################################\n",
      "Loss:  0.006583875510841608\n",
      "################################  1152  ################################\n",
      "Loss:  0.006581136956810951\n",
      "################################  1153  ################################\n",
      "Loss:  0.0065783001482486725\n",
      "################################  1154  ################################\n",
      "Loss:  0.006574714556336403\n",
      "################################  1155  ################################\n",
      "Loss:  0.006571134086698294\n",
      "################################  1156  ################################\n",
      "Loss:  0.006566350813955069\n",
      "################################  1157  ################################\n",
      "Loss:  0.006561462767422199\n",
      "################################  1158  ################################\n",
      "Loss:  0.006555532105267048\n",
      "################################  1159  ################################\n",
      "Loss:  0.006548956967890263\n",
      "################################  1160  ################################\n",
      "Loss:  0.006541464012116194\n",
      "################################  1161  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0065316567197442055\n",
      "################################  1162  ################################\n",
      "Loss:  0.0065269265323877335\n",
      "################################  1163  ################################\n",
      "Loss:  0.0065190233290195465\n",
      "################################  1164  ################################\n",
      "Loss:  0.006509503349661827\n",
      "################################  1165  ################################\n",
      "Loss:  0.00649783480912447\n",
      "################################  1166  ################################\n",
      "Loss:  0.006484237499535084\n",
      "################################  1167  ################################\n",
      "Loss:  0.006468853913247585\n",
      "################################  1168  ################################\n",
      "Loss:  0.006451359484344721\n",
      "################################  1169  ################################\n",
      "Loss:  0.006431985180824995\n",
      "################################  1170  ################################\n",
      "Loss:  0.0064146434888243675\n",
      "################################  1171  ################################\n",
      "Loss:  0.006397845223546028\n",
      "################################  1172  ################################\n",
      "Loss:  0.0063801109790802\n",
      "################################  1173  ################################\n",
      "Loss:  0.00636171642690897\n",
      "################################  1174  ################################\n",
      "Loss:  0.0063449484296143055\n",
      "################################  1175  ################################\n",
      "Loss:  0.006329256575554609\n",
      "################################  1176  ################################\n",
      "Loss:  0.0063137453980743885\n",
      "################################  1177  ################################\n",
      "Loss:  0.006299493368715048\n",
      "################################  1178  ################################\n",
      "Loss:  0.006283136084675789\n",
      "################################  1179  ################################\n",
      "Loss:  0.0062765637412667274\n",
      "################################  1180  ################################\n",
      "Loss:  0.0062629710882902145\n",
      "################################  1181  ################################\n",
      "Loss:  0.006245854776352644\n",
      "################################  1182  ################################\n",
      "Loss:  0.00622947420924902\n",
      "################################  1183  ################################\n",
      "Loss:  0.006211824249476194\n",
      "################################  1184  ################################\n",
      "Loss:  0.006193614564836025\n",
      "################################  1185  ################################\n",
      "Loss:  0.0061766295693814754\n",
      "################################  1186  ################################\n",
      "Loss:  0.0061630005948245525\n",
      "################################  1187  ################################\n",
      "Loss:  0.006147919222712517\n",
      "################################  1188  ################################\n",
      "Loss:  0.006135099567472935\n",
      "################################  1189  ################################\n",
      "Loss:  0.006124286446720362\n",
      "################################  1190  ################################\n",
      "Loss:  0.006115416996181011\n",
      "################################  1191  ################################\n",
      "Loss:  0.006107913330197334\n",
      "################################  1192  ################################\n",
      "Loss:  0.00611101882532239\n",
      "################################  1193  ################################\n",
      "Loss:  0.006105802487581968\n",
      "################################  1194  ################################\n",
      "Loss:  0.00610200734809041\n",
      "################################  1195  ################################\n",
      "Loss:  0.006098151206970215\n",
      "################################  1196  ################################\n",
      "Loss:  0.006096159107983112\n",
      "################################  1197  ################################\n",
      "Loss:  0.006094279699027538\n",
      "################################  1198  ################################\n",
      "Loss:  0.0060930014587938786\n",
      "################################  1199  ################################\n",
      "Loss:  0.006092146970331669\n",
      "################################  1200  ################################\n",
      "Loss:  0.006092372350394726\n",
      "################################  1201  ################################\n",
      "Loss:  0.00609319843351841\n",
      "################################  1202  ################################\n",
      "Loss:  0.006094098091125488\n",
      "################################  1203  ################################\n",
      "Loss:  0.0060963560827076435\n",
      "################################  1204  ################################\n",
      "Loss:  0.006098208483308554\n",
      "################################  1205  ################################\n",
      "Loss:  0.006101120728999376\n",
      "################################  1206  ################################\n",
      "Loss:  0.0061035738326609135\n",
      "################################  1207  ################################\n",
      "Loss:  0.006106248125433922\n",
      "################################  1208  ################################\n",
      "Loss:  0.0061088744550943375\n",
      "################################  1209  ################################\n",
      "Loss:  0.00611127307638526\n",
      "################################  1210  ################################\n",
      "Loss:  0.0061137378215789795\n",
      "################################  1211  ################################\n",
      "Loss:  0.006116194650530815\n",
      "################################  1212  ################################\n",
      "Loss:  0.0061189220286905766\n",
      "################################  1213  ################################\n",
      "Loss:  0.006121295969933271\n",
      "################################  1214  ################################\n",
      "Loss:  0.0061226775869727135\n",
      "################################  1215  ################################\n",
      "Loss:  0.006125662941485643\n",
      "################################  1216  ################################\n",
      "Loss:  0.006127359811216593\n",
      "################################  1217  ################################\n",
      "Loss:  0.006129830610007048\n",
      "################################  1218  ################################\n",
      "Loss:  0.006130064371973276\n",
      "################################  1219  ################################\n",
      "Loss:  0.006131801288574934\n",
      "################################  1220  ################################\n",
      "Loss:  0.006134516559541225\n",
      "################################  1221  ################################\n",
      "Loss:  0.006136355455964804\n",
      "################################  1222  ################################\n",
      "Loss:  0.00613866001367569\n",
      "################################  1223  ################################\n",
      "Loss:  0.006139318458735943\n",
      "################################  1224  ################################\n",
      "Loss:  0.006141308229416609\n",
      "################################  1225  ################################\n",
      "Loss:  0.006137268152087927\n",
      "################################  1226  ################################\n",
      "Loss:  0.006137880962342024\n",
      "################################  1227  ################################\n",
      "Loss:  0.006139523349702358\n",
      "################################  1228  ################################\n",
      "Loss:  0.006140821147710085\n",
      "################################  1229  ################################\n",
      "Loss:  0.006142002064734697\n",
      "################################  1230  ################################\n",
      "Loss:  0.006143415346741676\n",
      "################################  1231  ################################\n",
      "Loss:  0.0061448500491678715\n",
      "################################  1232  ################################\n",
      "Loss:  0.006145601626485586\n",
      "################################  1233  ################################\n",
      "Loss:  0.006146381609141827\n",
      "################################  1234  ################################\n",
      "Loss:  0.006146019324660301\n",
      "################################  1235  ################################\n",
      "Loss:  0.006146883126348257\n",
      "################################  1236  ################################\n",
      "Loss:  0.006146860774606466\n",
      "################################  1237  ################################\n",
      "Loss:  0.006150325294584036\n",
      "################################  1238  ################################\n",
      "Loss:  0.006149814929813147\n",
      "################################  1239  ################################\n",
      "Loss:  0.006165243685245514\n",
      "################################  1240  ################################\n",
      "Loss:  0.006162462756037712\n",
      "################################  1241  ################################\n",
      "Loss:  0.006159413140267134\n",
      "################################  1242  ################################\n",
      "Loss:  0.006156187038868666\n",
      "################################  1243  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.006152985151857138\n",
      "################################  1244  ################################\n",
      "Loss:  0.006149796303361654\n",
      "################################  1245  ################################\n",
      "Loss:  0.006146732252091169\n",
      "################################  1246  ################################\n",
      "Loss:  0.006143912207335234\n",
      "################################  1247  ################################\n",
      "Loss:  0.006141310557723045\n",
      "################################  1248  ################################\n",
      "Loss:  0.006138894706964493\n",
      "################################  1249  ################################\n",
      "Loss:  0.006136738229542971\n",
      "################################  1250  ################################\n",
      "Loss:  0.006134785246104002\n",
      "################################  1251  ################################\n",
      "Loss:  0.006133035756647587\n",
      "################################  1252  ################################\n",
      "Loss:  0.006131452973932028\n",
      "################################  1253  ################################\n",
      "Loss:  0.006130031775683165\n",
      "################################  1254  ################################\n",
      "Loss:  0.006128758192062378\n",
      "################################  1255  ################################\n",
      "Loss:  0.006127515807747841\n",
      "################################  1256  ################################\n",
      "Loss:  0.006126455496996641\n",
      "################################  1257  ################################\n",
      "Loss:  0.006125468295067549\n",
      "################################  1258  ################################\n",
      "Loss:  0.006124763283878565\n",
      "################################  1259  ################################\n",
      "Loss:  0.006123961880803108\n",
      "################################  1260  ################################\n",
      "Loss:  0.006123358383774757\n",
      "################################  1261  ################################\n",
      "Loss:  0.006122645456343889\n",
      "################################  1262  ################################\n",
      "Loss:  0.006122036837041378\n",
      "################################  1263  ################################\n",
      "Loss:  0.006121395621448755\n",
      "################################  1264  ################################\n",
      "Loss:  0.0061219241470098495\n",
      "################################  1265  ################################\n",
      "Loss:  0.0061213127337396145\n",
      "################################  1266  ################################\n",
      "Loss:  0.006120671518146992\n",
      "################################  1267  ################################\n",
      "Loss:  0.006121395155787468\n",
      "################################  1268  ################################\n",
      "Loss:  0.006120876409113407\n",
      "################################  1269  ################################\n",
      "Loss:  0.006119968835264444\n",
      "################################  1270  ################################\n",
      "Loss:  0.00611927080899477\n",
      "################################  1271  ################################\n",
      "Loss:  0.006117858458310366\n",
      "################################  1272  ################################\n",
      "Loss:  0.006116576958447695\n",
      "################################  1273  ################################\n",
      "Loss:  0.006115134805440903\n",
      "################################  1274  ################################\n",
      "Loss:  0.006113455630838871\n",
      "################################  1275  ################################\n",
      "Loss:  0.006111710332334042\n",
      "################################  1276  ################################\n",
      "Loss:  0.0061095827259123325\n",
      "################################  1277  ################################\n",
      "Loss:  0.006107359193265438\n",
      "################################  1278  ################################\n",
      "Loss:  0.0061048236675560474\n",
      "################################  1279  ################################\n",
      "Loss:  0.00610201433300972\n",
      "################################  1280  ################################\n",
      "Loss:  0.0060986909084022045\n",
      "################################  1281  ################################\n",
      "Loss:  0.006094479002058506\n",
      "################################  1282  ################################\n",
      "Loss:  0.006091803312301636\n",
      "################################  1283  ################################\n",
      "Loss:  0.006087433081120253\n",
      "################################  1284  ################################\n",
      "Loss:  0.006084337830543518\n",
      "################################  1285  ################################\n",
      "Loss:  0.006079297978430986\n",
      "################################  1286  ################################\n",
      "Loss:  0.00607310002669692\n",
      "################################  1287  ################################\n",
      "Loss:  0.006066667847335339\n",
      "################################  1288  ################################\n",
      "Loss:  0.006059752777218819\n",
      "################################  1289  ################################\n",
      "Loss:  0.006048066075891256\n",
      "################################  1290  ################################\n",
      "Loss:  0.006040812004357576\n",
      "################################  1291  ################################\n",
      "Loss:  0.006032733246684074\n",
      "################################  1292  ################################\n",
      "Loss:  0.006024483125656843\n",
      "################################  1293  ################################\n",
      "Loss:  0.006014917977154255\n",
      "################################  1294  ################################\n",
      "Loss:  0.006009040400385857\n",
      "################################  1295  ################################\n",
      "Loss:  0.00600044708698988\n",
      "################################  1296  ################################\n",
      "Loss:  0.005995398387312889\n",
      "################################  1297  ################################\n",
      "Loss:  0.005988242570310831\n",
      "################################  1298  ################################\n",
      "Loss:  0.005983883049339056\n",
      "################################  1299  ################################\n",
      "Loss:  0.0059805382043123245\n",
      "################################  1300  ################################\n",
      "Loss:  0.005976860877126455\n",
      "################################  1301  ################################\n",
      "Loss:  0.005971818696707487\n",
      "################################  1302  ################################\n",
      "Loss:  0.0059680272825062275\n",
      "################################  1303  ################################\n",
      "Loss:  0.00596439465880394\n",
      "################################  1304  ################################\n",
      "Loss:  0.005960467271506786\n",
      "################################  1305  ################################\n",
      "Loss:  0.005956859793514013\n",
      "################################  1306  ################################\n",
      "Loss:  0.0059532118029892445\n",
      "################################  1307  ################################\n",
      "Loss:  0.005950193386524916\n",
      "################################  1308  ################################\n",
      "Loss:  0.005946282763034105\n",
      "################################  1309  ################################\n",
      "Loss:  0.005942932330071926\n",
      "################################  1310  ################################\n",
      "Loss:  0.005938931368291378\n",
      "################################  1311  ################################\n",
      "Loss:  0.005935662891715765\n",
      "################################  1312  ################################\n",
      "Loss:  0.0059330398216843605\n",
      "################################  1313  ################################\n",
      "Loss:  0.005928638391196728\n",
      "################################  1314  ################################\n",
      "Loss:  0.005926697514951229\n",
      "################################  1315  ################################\n",
      "Loss:  0.005924887023866177\n",
      "################################  1316  ################################\n",
      "Loss:  0.005923215299844742\n",
      "################################  1317  ################################\n",
      "Loss:  0.005921564064919949\n",
      "################################  1318  ################################\n",
      "Loss:  0.005919917486608028\n",
      "################################  1319  ################################\n",
      "Loss:  0.005920822732150555\n",
      "################################  1320  ################################\n",
      "Loss:  0.00591993285343051\n",
      "################################  1321  ################################\n",
      "Loss:  0.005915899761021137\n",
      "################################  1322  ################################\n",
      "Loss:  0.005914958193898201\n",
      "################################  1323  ################################\n",
      "Loss:  0.005915542598813772\n",
      "################################  1324  ################################\n",
      "Loss:  0.005914445035159588\n",
      "################################  1325  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.005914106499403715\n",
      "################################  1326  ################################\n",
      "Loss:  0.005913378670811653\n",
      "################################  1327  ################################\n",
      "Loss:  0.0059132929891347885\n",
      "################################  1328  ################################\n",
      "Loss:  0.005912606604397297\n",
      "################################  1329  ################################\n",
      "Loss:  0.0059120808728039265\n",
      "################################  1330  ################################\n",
      "Loss:  0.005911799147725105\n",
      "################################  1331  ################################\n",
      "Loss:  0.005911373533308506\n",
      "################################  1332  ################################\n",
      "Loss:  0.005911361426115036\n",
      "################################  1333  ################################\n",
      "Loss:  0.0059107509441673756\n",
      "################################  1334  ################################\n",
      "Loss:  0.005910699255764484\n",
      "################################  1335  ################################\n",
      "Loss:  0.005910203326493502\n",
      "################################  1336  ################################\n",
      "Loss:  0.005909654311835766\n",
      "################################  1337  ################################\n",
      "Loss:  0.005908913444727659\n",
      "################################  1338  ################################\n",
      "Loss:  0.005906542297452688\n",
      "################################  1339  ################################\n",
      "Loss:  0.005905902944505215\n",
      "################################  1340  ################################\n",
      "Loss:  0.005904358346015215\n",
      "################################  1341  ################################\n",
      "Loss:  0.005905833560973406\n",
      "################################  1342  ################################\n",
      "Loss:  0.0059049539268016815\n",
      "################################  1343  ################################\n",
      "Loss:  0.00590440072119236\n",
      "################################  1344  ################################\n",
      "Loss:  0.0059030866250395775\n",
      "################################  1345  ################################\n",
      "Loss:  0.005902374163269997\n",
      "################################  1346  ################################\n",
      "Loss:  0.005901051685214043\n",
      "################################  1347  ################################\n",
      "Loss:  0.005899355746805668\n",
      "################################  1348  ################################\n",
      "Loss:  0.005896709393709898\n",
      "################################  1349  ################################\n",
      "Loss:  0.0058951610699296\n",
      "################################  1350  ################################\n",
      "Loss:  0.005891758948564529\n",
      "################################  1351  ################################\n",
      "Loss:  0.005889463238418102\n",
      "################################  1352  ################################\n",
      "Loss:  0.005886663217097521\n",
      "################################  1353  ################################\n",
      "Loss:  0.00588318333029747\n",
      "################################  1354  ################################\n",
      "Loss:  0.005878614727407694\n",
      "################################  1355  ################################\n",
      "Loss:  0.005874330177903175\n",
      "################################  1356  ################################\n",
      "Loss:  0.005869265645742416\n",
      "################################  1357  ################################\n",
      "Loss:  0.0058647869154810905\n",
      "################################  1358  ################################\n",
      "Loss:  0.0058600096963346004\n",
      "################################  1359  ################################\n",
      "Loss:  0.00585554726421833\n",
      "################################  1360  ################################\n",
      "Loss:  0.005850454792380333\n",
      "################################  1361  ################################\n",
      "Loss:  0.005846013315021992\n",
      "################################  1362  ################################\n",
      "Loss:  0.005841193255037069\n",
      "################################  1363  ################################\n",
      "Loss:  0.005836936179548502\n",
      "################################  1364  ################################\n",
      "Loss:  0.005832288879901171\n",
      "################################  1365  ################################\n",
      "Loss:  0.005827333778142929\n",
      "################################  1366  ################################\n",
      "Loss:  0.0058228494599461555\n",
      "################################  1367  ################################\n",
      "Loss:  0.005818519275635481\n",
      "################################  1368  ################################\n",
      "Loss:  0.005814497359097004\n",
      "################################  1369  ################################\n",
      "Loss:  0.0058090463280677795\n",
      "################################  1370  ################################\n",
      "Loss:  0.005804700311273336\n",
      "################################  1371  ################################\n",
      "Loss:  0.00580021645873785\n",
      "################################  1372  ################################\n",
      "Loss:  0.005796540062874556\n",
      "################################  1373  ################################\n",
      "Loss:  0.00579224294051528\n",
      "################################  1374  ################################\n",
      "Loss:  0.00578819727525115\n",
      "################################  1375  ################################\n",
      "Loss:  0.0057837022468447685\n",
      "################################  1376  ################################\n",
      "Loss:  0.005778482183814049\n",
      "################################  1377  ################################\n",
      "Loss:  0.00577259948477149\n",
      "################################  1378  ################################\n",
      "Loss:  0.0057653626427054405\n",
      "################################  1379  ################################\n",
      "Loss:  0.0057572913356125355\n",
      "################################  1380  ################################\n",
      "Loss:  0.005747188813984394\n",
      "################################  1381  ################################\n",
      "Loss:  0.005737809464335442\n",
      "################################  1382  ################################\n",
      "Loss:  0.005727879703044891\n",
      "################################  1383  ################################\n",
      "Loss:  0.005718722939491272\n",
      "################################  1384  ################################\n",
      "Loss:  0.0057096341624855995\n",
      "################################  1385  ################################\n",
      "Loss:  0.005699428729712963\n",
      "################################  1386  ################################\n",
      "Loss:  0.005691577214747667\n",
      "################################  1387  ################################\n",
      "Loss:  0.005683255381882191\n",
      "################################  1388  ################################\n",
      "Loss:  0.005674963817000389\n",
      "################################  1389  ################################\n",
      "Loss:  0.005665333475917578\n",
      "################################  1390  ################################\n",
      "Loss:  0.005658296402543783\n",
      "################################  1391  ################################\n",
      "Loss:  0.005652077961713076\n",
      "################################  1392  ################################\n",
      "Loss:  0.0056443531066179276\n",
      "################################  1393  ################################\n",
      "Loss:  0.00563873490318656\n",
      "################################  1394  ################################\n",
      "Loss:  0.005633956752717495\n",
      "################################  1395  ################################\n",
      "Loss:  0.005629219114780426\n",
      "################################  1396  ################################\n",
      "Loss:  0.005625467747449875\n",
      "################################  1397  ################################\n",
      "Loss:  0.005621453747153282\n",
      "################################  1398  ################################\n",
      "Loss:  0.005618501920253038\n",
      "################################  1399  ################################\n",
      "Loss:  0.005613852292299271\n",
      "################################  1400  ################################\n",
      "Loss:  0.005612069740891457\n",
      "################################  1401  ################################\n",
      "Loss:  0.005609406623989344\n",
      "################################  1402  ################################\n",
      "Loss:  0.005606954451650381\n",
      "################################  1403  ################################\n",
      "Loss:  0.0056044370867311954\n",
      "################################  1404  ################################\n",
      "Loss:  0.005601790267974138\n",
      "################################  1405  ################################\n",
      "Loss:  0.005599390249699354\n",
      "################################  1406  ################################\n",
      "Loss:  0.005597596522420645\n",
      "################################  1407  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.005595857743173838\n",
      "################################  1408  ################################\n",
      "Loss:  0.0055948421359062195\n",
      "################################  1409  ################################\n",
      "Loss:  0.005594093818217516\n",
      "################################  1410  ################################\n",
      "Loss:  0.005593819078058004\n",
      "################################  1411  ################################\n",
      "Loss:  0.005593101028352976\n",
      "################################  1412  ################################\n",
      "Loss:  0.005592512898147106\n",
      "################################  1413  ################################\n",
      "Loss:  0.005592782516032457\n",
      "################################  1414  ################################\n",
      "Loss:  0.0055930656380951405\n",
      "################################  1415  ################################\n",
      "Loss:  0.005594249349087477\n",
      "################################  1416  ################################\n",
      "Loss:  0.00559561513364315\n",
      "################################  1417  ################################\n",
      "Loss:  0.005597164388746023\n",
      "################################  1418  ################################\n",
      "Loss:  0.005598869640380144\n",
      "################################  1419  ################################\n",
      "Loss:  0.005600673612207174\n",
      "################################  1420  ################################\n",
      "Loss:  0.005602546967566013\n",
      "################################  1421  ################################\n",
      "Loss:  0.005604496691375971\n",
      "################################  1422  ################################\n",
      "Loss:  0.005606251303106546\n",
      "################################  1423  ################################\n",
      "Loss:  0.005609049927443266\n",
      "################################  1424  ################################\n",
      "Loss:  0.005610985215753317\n",
      "################################  1425  ################################\n",
      "Loss:  0.005613288842141628\n",
      "################################  1426  ################################\n",
      "Loss:  0.0056150080636143684\n",
      "################################  1427  ################################\n",
      "Loss:  0.005617893300950527\n",
      "################################  1428  ################################\n",
      "Loss:  0.005619851406663656\n",
      "################################  1429  ################################\n",
      "Loss:  0.005621109623461962\n",
      "################################  1430  ################################\n",
      "Loss:  0.0056228856556117535\n",
      "################################  1431  ################################\n",
      "Loss:  0.005624658893793821\n",
      "################################  1432  ################################\n",
      "Loss:  0.005626515485346317\n",
      "################################  1433  ################################\n",
      "Loss:  0.005628196522593498\n",
      "################################  1434  ################################\n",
      "Loss:  0.005630399566143751\n",
      "################################  1435  ################################\n",
      "Loss:  0.005632233340293169\n",
      "################################  1436  ################################\n",
      "Loss:  0.005634085740894079\n",
      "################################  1437  ################################\n",
      "Loss:  0.005635780282318592\n",
      "################################  1438  ################################\n",
      "Loss:  0.005637511145323515\n",
      "################################  1439  ################################\n",
      "Loss:  0.0056390417739748955\n",
      "################################  1440  ################################\n",
      "Loss:  0.005641434341669083\n",
      "################################  1441  ################################\n",
      "Loss:  0.00564305717125535\n",
      "################################  1442  ################################\n",
      "Loss:  0.005654892418533564\n",
      "################################  1443  ################################\n",
      "Loss:  0.005653487052768469\n",
      "################################  1444  ################################\n",
      "Loss:  0.00565241789445281\n",
      "################################  1445  ################################\n",
      "Loss:  0.005651426501572132\n",
      "################################  1446  ################################\n",
      "Loss:  0.00565054826438427\n",
      "################################  1447  ################################\n",
      "Loss:  0.005649749655276537\n",
      "################################  1448  ################################\n",
      "Loss:  0.0056489817798137665\n",
      "################################  1449  ################################\n",
      "Loss:  0.005648369435220957\n",
      "################################  1450  ################################\n",
      "Loss:  0.005647831596434116\n",
      "################################  1451  ################################\n",
      "Loss:  0.005647383630275726\n",
      "################################  1452  ################################\n",
      "Loss:  0.005647042766213417\n",
      "################################  1453  ################################\n",
      "Loss:  0.0056467922404408455\n",
      "################################  1454  ################################\n",
      "Loss:  0.005646621808409691\n",
      "################################  1455  ################################\n",
      "Loss:  0.0056465002708137035\n",
      "################################  1456  ################################\n",
      "Loss:  0.005646515637636185\n",
      "################################  1457  ################################\n",
      "Loss:  0.005646533332765102\n",
      "################################  1458  ################################\n",
      "Loss:  0.005646637175232172\n",
      "################################  1459  ################################\n",
      "Loss:  0.00564676383510232\n",
      "################################  1460  ################################\n",
      "Loss:  0.005646937061101198\n",
      "################################  1461  ################################\n",
      "Loss:  0.005647127516567707\n",
      "################################  1462  ################################\n",
      "Loss:  0.005647364072501659\n",
      "################################  1463  ################################\n",
      "Loss:  0.005647559650242329\n",
      "################################  1464  ################################\n",
      "Loss:  0.005647802259773016\n",
      "################################  1465  ################################\n",
      "Loss:  0.005648009479045868\n",
      "################################  1466  ################################\n",
      "Loss:  0.00564815616235137\n",
      "################################  1467  ################################\n",
      "Loss:  0.005648170132189989\n",
      "################################  1468  ################################\n",
      "Loss:  0.005648320075124502\n",
      "################################  1469  ################################\n",
      "Loss:  0.0056482888758182526\n",
      "################################  1470  ################################\n",
      "Loss:  0.005648323334753513\n",
      "################################  1471  ################################\n",
      "Loss:  0.005648139864206314\n",
      "################################  1472  ################################\n",
      "Loss:  0.005647292826324701\n",
      "################################  1473  ################################\n",
      "Loss:  0.005647141486406326\n",
      "################################  1474  ################################\n",
      "Loss:  0.005645189434289932\n",
      "################################  1475  ################################\n",
      "Loss:  0.0056446027010679245\n",
      "################################  1476  ################################\n",
      "Loss:  0.005643764510750771\n",
      "################################  1477  ################################\n",
      "Loss:  0.005642755422741175\n",
      "################################  1478  ################################\n",
      "Loss:  0.005641680210828781\n",
      "################################  1479  ################################\n",
      "Loss:  0.005640503950417042\n",
      "################################  1480  ################################\n",
      "Loss:  0.005639292299747467\n",
      "################################  1481  ################################\n",
      "Loss:  0.005638048052787781\n",
      "################################  1482  ################################\n",
      "Loss:  0.00563680287450552\n",
      "################################  1483  ################################\n",
      "Loss:  0.0056355902925133705\n",
      "################################  1484  ################################\n",
      "Loss:  0.005634416826069355\n",
      "################################  1485  ################################\n",
      "Loss:  0.005633326712995768\n",
      "################################  1486  ################################\n",
      "Loss:  0.005632256623357534\n",
      "################################  1487  ################################\n",
      "Loss:  0.005631250329315662\n",
      "################################  1488  ################################\n",
      "Loss:  0.005630250554531813\n",
      "################################  1489  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.005629304796457291\n",
      "################################  1490  ################################\n",
      "Loss:  0.005628317128866911\n",
      "################################  1491  ################################\n",
      "Loss:  0.005627439357340336\n",
      "################################  1492  ################################\n",
      "Loss:  0.005626542028039694\n",
      "################################  1493  ################################\n",
      "Loss:  0.005625685676932335\n",
      "################################  1494  ################################\n",
      "Loss:  0.005624744109809399\n",
      "################################  1495  ################################\n",
      "Loss:  0.005623852834105492\n",
      "################################  1496  ################################\n",
      "Loss:  0.005622875411063433\n",
      "################################  1497  ################################\n",
      "Loss:  0.005621987394988537\n",
      "################################  1498  ################################\n",
      "Loss:  0.005620927549898624\n",
      "################################  1499  ################################\n",
      "Loss:  0.005620061419904232\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m history \u001b[38;5;241m=\u001b[39m fit(my_network, training_set, interior, n_epochs, optimizer_, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[1;32m      5\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m total_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m \u001b[43mstart_time\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start_time' is not defined"
     ]
    }
   ],
   "source": [
    "# n_epochs = 10000\n",
    "# start_time = time.time()\n",
    "n_epochs = 1500\n",
    "history = fit(my_network, training_set, interior, n_epochs, optimizer_, p=2, verbose=True )\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# print(\"Training time: {:.2f} seconds\".format(total_time))\n",
    "\n",
    "# with open('p_ic.pkl', 'wb') as f:\n",
    "#     pickle.dump(history, f)\n",
    "\n",
    "# f.close()\n",
    "\n",
    "# model_state_dict = my_network.state_dict()\n",
    "\n",
    "# # Save the model state dictionary to a file\n",
    "# torch.save(model_state_dict, 'p_ic.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the history from the pickle file\n",
    "# with open('p_ic_fixed_param.pkl', 'rb') as f:\n",
    "#     history = pickle.load(f)\n",
    "\n",
    "# # Load the model architecture\n",
    "# my_network = your_model_module.YourModelClass()  # Instantiate your model class\n",
    "\n",
    "# # Load the saved model state dictionary\n",
    "# model_state_dict = torch.load('p_ic_fixed_param.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# # Load the model weights\n",
    "# my_network.load_state_dict(model_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56a0f31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Error Test:  114.07986879348755 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAD4CAYAAADIBWPsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAiVElEQVR4nO3de3Rd5Xnn8e9zdJcsyZIt22DZ2BgDMeAEYhxamJQ0N5JmcGf1EpikzZ22GabpZU0nnc6knbRr1qTttNOu0lCapEk6SUiapq3TktA0IbCalIu5m4uNwfgGxsLy/abbO3+cLftIlm3ZOtKWzvl+1jrrnP3u9+z9nJdt6cfe79mKlBKSJEk6N4W8C5AkSZrJDFOSJEkTYJiSJEmaAMOUJEnSBBimJEmSJqA2rx3PnTs3LVmyJK/dS5IkjdvDDz/8akqpa6x1uYWpJUuWsG7durx2L0mSNG4RseVU67zMJ0mSNAGGKUmSpAkwTEmSJE2AYUqSJGkCDFOSJEkTYJiSJEmaAMOUJEnSBFRsmHpl/1H+6J83sGnXwbxLkSRJFaxiw1TvoT7+9HubeO6VA3mXIkmSKljFhqnm+hoADvcN5lyJJEmqZBUbppqGw1S/YUqSJE2eyg1TdcUwdaRvIOdKJElSJavYMNVcX/wbzl7mkyRJk6liw1RNIaivLXDEMCVJkiZRxYYpKE5C98yUJEmaTGcMUxHxuYjYFRHrT7H+PRHxREQ8GRE/jIjXlr/Mc9NcZ5iSJEmTazxnpj4P3HCa9ZuBH0spXQH8LnBHGeoqi7amOvYd6c+7DEmSVMFqz9QhpXRfRCw5zfoflizeD3SXoa6y6GptoOfA0bzLkCRJFazcc6Y+BHzrVCsj4paIWBcR63p6esq865PNa21k14Fjk74fSZJUvcoWpiLiTRTD1H89VZ+U0h0ppVUppVVdXV3l2vUpzW9roOfAMYaG0qTvS5IkVaeyhKmIWAl8BliTUtpdjm2Ww7zWBgaGEr2H+/IuRZIkVagJh6mIWAx8A/i5lNLGiZdUPvPbGgF4Zb/zpiRJ0uQ44wT0iPgKcD0wNyK2A78N1AGklG4HPgHMAf48IgAGUkqrJqvgszGvrQGAXfuPcdn5ORcjSZIq0ni+zXfzGdZ/GPhw2Soqo/NnNwGwY++RnCuRJEmVqqLvgD6/tZH62gLbeg/nXYokSapQFR2mCoVgUUcTWw1TkiRpklR0mAJY3NlsmJIkSZOmOsLU7sOk5L2mJElS+VV8mFo6t4UDxwbo8U7okiRpElR8mLr0vDYAntl5IOdKJElSJar8MLWgFYANO/fnXIkkSapEFR+mZjfXs6CtkWde9syUJEkqv4oPUwAru9t5dOuevMuQJEkVqCrC1NVLOnlx92F2HfBv9EmSpPKqijC1akkHAA9u7s25EkmSVGmqIkxdsbCdjuY6vvP0K3mXIkmSKkxVhKnamgJvW7GA7z2zi2MDg3mXI0mSKkhVhCmAGy5fwIFjA9y38dW8S5EkSRWkasLUtRfNZX5bA5//4ea8S5EkSRWkasJUfW2BD1y7lB9s2s2T2/flXY4kSaoQVROmAP7jGxYzu7mO3/2np/3Dx5IkqSyqKky1NdbxG2+/lAc397L28ZfyLkeSJFWAqgpTAO++ehEru9v5vX96hn1H+vMuR5IkzXBVF6ZqCsH/+g9XsPvgMT717WfzLkeSJM1wVRemAC5f2M4Hr13Klx/YysNbvCu6JEk6d1UZpgB+9a0XM6+1gd//9oa8S5EkSTPYGcNURHwuInZFxPpTrI+I+NOI2BQRT0TEVeUvs/xaGmr56PXLeGBzLz983ht5SpKkczOeM1OfB244zfp3AMuzxy3Apyde1tS4afViOlvq+X/3b8m7FEmSNEOdMUyllO4DTjexaA3wxVR0PzA7Is4rV4GTqbGuhp983UK+8/Qr9B7qy7scSZI0A5VjztRCYFvJ8vas7SQRcUtErIuIdT09PWXY9cT97NXd9A8mvul9pyRJ0jmY0gnoKaU7UkqrUkqrurq6pnLXp3TpgjaWdbXwL8+8kncpkiRpBipHmNoBLCpZ7s7aZoy3vGY+97+wmwNHvYmnJEk6O+UIU2uBn8++1XcNsC+l9HIZtjtl3nhxF/2DiXVb9uRdiiRJmmFqz9QhIr4CXA/MjYjtwG8DdQAppduBu4B3ApuAw8AHJqvYyXLl4tnUFoIHN/fypkvm5V2OJEmaQc4YplJKN59hfQL+U9kqykFzfS1XdLfz0Gbvhi5Jks5O1d4BfbTVSzt5fPtejvYP5l2KJEmaQQxTmdVLOukfTDy6dW/epUiSpBnEMJVZdUEnEfCgl/okSdJZMExl2pvruHRBGw+9aJiSJEnjZ5gq8YalnTy8ZQ/9g0N5lyJJkmYIw1SJq5d0cqR/kPU79uVdiiRJmiEMUyWuXtoB4KU+SZI0boapEvNaG+nuaOLxbZ6ZkiRJ42OYGmVldztPeplPkiSNk2FqlMsXtrO19zB7D/flXYokSZoBDFOjrFw4G4D1O/bnW4gkSZoRDFOjXL6wDYAnduzNtxBJkjQjGKZGmd1cz+LOZp7c7rwpSZJ0ZoapMVzR3c4ThilJkjQOhqkxrFzYzo69R+g95CR0SZJ0eoapMVzR3Q7gLRIkSdIZGabGcPnCLExt35tvIZIkadozTI2hrbGOC+e28LjzpiRJ0hkYpk7hiu52v9EnSZLOyDB1ClcsbGfn/qPs2n8071IkSdI0Zpg6hdcumg04CV2SJJ2eYeoUVpzXRiFw3pQkSTotw9QptDTUcsmCNh7e0pt3KZIkaRobV5iKiBsiYkNEbIqIj4+xfnFE3BMRj0bEExHxzvKXOvXesLSTR7bspX9wKO9SJEnSNHXGMBURNcBtwDuAFcDNEbFiVLf/DnwtpXQlcBPw5+UuNA+rl3ZypH+Q9c6bkiRJpzCeM1OrgU0ppRdSSn3AncCaUX0S0Ja9bgdeKl+J+bl6SScAD272Up8kSRrbeMLUQmBbyfL2rK3U7wDvjYjtwF3Afx5rQxFxS0Ssi4h1PT0951Du1OpqbeDCuS2GKUmSdErlmoB+M/D5lFI38E7gryPipG2nlO5IKa1KKa3q6uoq064n1+qlnTz4Yi8DzpuSJEljGE+Y2gEsKlnuztpKfQj4GkBK6d+ARmBuOQrM23XL53Lg6ACP+3f6JEnSGMYTph4ClkfE0oiopzjBfO2oPluBNwNExGsohqnpfx1vHK67aC6FgPs2vpp3KZIkaRo6Y5hKKQ0AtwJ3A89Q/NbeUxHxyYi4Mev268BHIuJx4CvA+1NKabKKnkqzm+tZ2T2b+56riGwoSZLKrHY8nVJKd1GcWF7a9omS108D15a3tOnjjRd38Wffe459h/tpb67LuxxJkjSNeAf0cfixi+cylOBfN3mpT5IkjWSYGofXds+mtbGW+zZ6qU+SJI1kmBqH2poC1y6by70be6iQqWCSJKlMDFPj9ObXzGPn/qM86Z+WkSRJJQxT4/TWFfOpKQTfWr8z71IkSdI0Ypgap9nN9Vy9pIN7nt2VdymSJGkaMUydhWuXzeXZnQfYf7Q/71IkSdI0YZg6CysXzQZgvfOmJElSxjB1Fq5Y2A7Ak9sNU5IkqcgwdRY6W+pZOLvJb/RJkqTjDFNnaWV3u2FKkiQdZ5g6S5cvbGfL7sPsO+wkdEmSZJg6ayu7i/Om1r/k2SlJkmSYOmvDk9CfcBK6JEnCMHXWZjfXs6izydsjSJIkwDB1TlYunM0TO/bmXYYkSZoGDFPn4DXntbKt9wiH+wbyLkWSJOXMMHUOFnU2A7B9z5GcK5EkSXkzTJ2DxVmY2tZ7OOdKJElS3gxT52A4TG01TEmSVPUMU+egs6We5voaw5QkSTJMnYuIYHFnM9t6nTMlSVK1G1eYiogbImJDRGyKiI+fos/PRsTTEfFURHy5vGVOP4s6m50zJUmSqD1Th4ioAW4D3gpsBx6KiLUppadL+iwHfhO4NqW0JyLmTVbB08Wijmb+9blXSSkREXmXI0mScjKeM1OrgU0ppRdSSn3AncCaUX0+AtyWUtoDkFLaVd4yp5/FnU0c6R9k96G+vEuRJEk5Gk+YWghsK1nenrWVuhi4OCJ+EBH3R8QNY20oIm6JiHURsa6np+fcKp4mFs/xG32SJKl8E9BrgeXA9cDNwF9GxOzRnVJKd6SUVqWUVnV1dZVp1/lY1OG9piRJ0vjC1A5gUclyd9ZWajuwNqXUn1LaDGykGK4qVrdhSpIkMb4w9RCwPCKWRkQ9cBOwdlSfv6d4VoqImEvxst8L5Stz+mmqr2Fea4OX+SRJqnJnDFMppQHgVuBu4BngaymlpyLikxFxY9btbmB3RDwN3AP8l5TS7skqerpY1NlsmJIkqcqd8dYIACmlu4C7RrV9ouR1An4te1SNxZ3NPLi5N+8yJElSjrwD+gQs6mzm5X1H6BsYyrsUSZKUE8PUBCzqaGIowUt7/bMykiRVK8PUBCzuzL7Rt8d5U5IkVSvD1AR4405JkmSYmoD5rY3U1xQMU5IkVTHD1AQUCkF3R5M37pQkqYoZpiZo6dwWXug5lHcZkiQpJ4apCVo2bxYvvHqIwaGUdymSJCkHhqkJWtbVQt/AEDv2eHsESZKqkWFqgpZ1zQLg+Z6DOVciSZLyYJiaIMOUJEnVzTA1QR0t9cxpqWfTLsOUJEnVyDBVBsu6ZnlmSpKkKmWYKoNl81p43tsjSJJUlQxTZbCsaxa9h/roPdSXdymSJGmKGabKYNk8J6FLklStDFNlcNHwN/qchC5JUtUxTJXB+bObaKgteGZKkqQqZJgqg5pCcGHXLCehS5JUhQxTZbKsq8V7TUmSVIUMU2WyrGsW2/Yc5mj/YN6lSJKkKWSYKpNLF7SSEmzYeSDvUiRJ0hQyTJXJ5QvbAXjqpf05VyJJkqbSuMJURNwQERsiYlNEfPw0/X4qIlJErCpfiTNDd0cTbY21rH9pX96lSJKkKXTGMBURNcBtwDuAFcDNEbFijH6twMeAB8pd5EwQEVx2frtnpiRJqjLjOTO1GtiUUnohpdQH3AmsGaPf7wKfAo6Wsb4Z5bLz23jm5f30Dw7lXYokSZoi4wlTC4FtJcvbs7bjIuIqYFFK6Z9Ot6GIuCUi1kXEup6enrMudrq7fGE7fQND3rxTkqQqMuEJ6BFRAP4I+PUz9U0p3ZFSWpVSWtXV1TXRXU87l53fBsCT2503JUlStRhPmNoBLCpZ7s7ahrUClwPfj4gXgWuAtdU4CX1Z1yzmtNTzg02v5l2KJEmaIuMJUw8ByyNiaUTUAzcBa4dXppT2pZTmppSWpJSWAPcDN6aU1k1KxdNYoRC88eIu7t3Yw+BQyrscSZI0Bc4YplJKA8CtwN3AM8DXUkpPRcQnI+LGyS5wprn+ki72HO7n8e178y5FkiRNgdrxdEop3QXcNartE6foe/3Ey5q53ri8i0LA9zf0cNXijrzLkSRJk8w7oJdZR0s9Vy7u4PsbduVdiiRJmgKGqUlw/cVdPLF9Hz0HjuVdiiRJmmSGqUnwpkvnAXDfxsq7l5YkSRrJMDUJVpzXRldrA/d4qU+SpIpnmJoEhUJw/cVd3LexhwH/tIwkSRXNMDVJrr9kHvuPDvDYtr15lyJJkiaRYWqSXLd8LjWF8FKfJEkVzjA1Sdqb6nj9BR3c86yT0CVJqmSGqUn0pkvm8fTL+9m572jepUiSpElimJpEb10xH4Bvr38550okSdJkMUxNoovmzeLi+bO468mdeZciSZImiWFqkr3zivN4aEsvL+87kncpkiRpEhimJtlPXdVNAF/44Za8S5EkSZPAMDXJFnU2c8PlC/jyA1s4eGwg73IkSVKZGaamwC/+2DL2Hx3gL+59Pu9SJElSmRmmpsDK7tmsed353H7v86zfsS/vciRJUhkZpqbI7/z7y+hsqeeX73yUA0f78y5HkiSViWFqinS01PPH734dW3cf5oOff8hAJUlShTBMTaEfXTaXP7npSh7Zupefuf3fvF2CJEkVwDA1xX5i5Xn81fuvZvueI/zkbT/gqZecQyVJ0kxmmMrBGy/u4m9+8UcoRPCzt/8bP9z0at4lSZKkc2SYyslrzmvj7z56LQs7mvilLz3Ctt7DeZckSZLOwbjCVETcEBEbImJTRHx8jPW/FhFPR8QTEfHdiLig/KVWngXtjfzlz69iKCVu/cqj9A8O5V2SJEk6S2cMUxFRA9wGvANYAdwcEStGdXsUWJVSWgl8Hfj9chdaqS6Y08Knfmolj2/byx/+84a8y5EkSWdpPGemVgObUkovpJT6gDuBNaUdUkr3pJSGr1PdD3SXt8zK9s4rzuM9b1jMX9z7Avdu7Mm7HEmSdBbGE6YWAttKlrdnbafyIeBbY62IiFsiYl1ErOvpMTSU+h/vWsEl81v51a8+xqZdB/MuR5IkjVNZJ6BHxHuBVcAfjLU+pXRHSmlVSmlVV1dXOXc94zXW1fDp915FIeA9n7mfja8cyLskSZI0DuMJUzuARSXL3VnbCBHxFuC3gBtTSsfKU151ubBrFl/68DUMDsGaP/sBn/7+8xztH8y7LEmSdBrjCVMPAcsjYmlE1AM3AWtLO0TElcBfUAxSu8pfZvW4ZEErd/3ydVx70Rw+9e1nefP/uZe1j79ESinv0iRJ0hjOGKZSSgPArcDdwDPA11JKT0XEJyPixqzbHwCzgL+JiMciYu0pNqdxmNfWyGfedzVf/sgbaG+q45e/8ijvvuN+duz1z89IkjTdRF5nPFatWpXWrVuXy75nksGhxNfWbeP3/vFpagrBX33gal5/QWfeZUmSVFUi4uGU0qqx1nkH9GmuphDcvHoxd33s3zFnVgM/99kH+YF/fkaSpGnDMDVDXDCnha/+wjUs6mjmA59/iO8+80reJUmSJAxTM8q81ka++gvXcOmCVn7pS4/wwAu78y5JkqSqZ5iaYWY31/OFD6xmUUcTH/niOu9HJUlSzgxTM1BHSz1f+OBqGutquPmO+/nX55xDJUlSXgxTM1R3RzN33nINHS31vPezD3Drlx/xz9BIkpQDw9QMdmHXLL5563X8yluW891ndvHWP76Xj37pYR7fttebfEqSNEW8z1SF2H3wGJ/7wWa++MMtHDg2wMXzZ/GuleezakkHr+2eTUtDbd4lSpI0Y53uPlOGqQqz/2g/33z8Jf724e08snUvAIWASxa0sXJhO5cvbOPyhe285rw2Gutq8i1WkqQZwjBVpfYe7uOxbXt5ZOteHt26h/U79rHncD9QvBno8nmzuHxhO1dkIeuy89sNWJIkjcEwJQBSSry07yhPbt/H+h37WP9S8fnVg30A1NcUeO2idlYv7eTqJZ28/oIOWhvrcq5akqT8GaZ0Sikldu4/yhPb9/Hwlj08sLmX9Tv2MTiUKARcuqCNFee3ccn8Vi5ZUHzMa20gIvIuXZKkKXO6MOWs5CoXEZzX3sR57U28/bIFABw6NsCjW/fy4Iu9PLJlD9/f0MPXH95+/D2zm+u4eH4rly5oPfG8oJU2z2JJkqqQYUonaWmo5brlc7lu+dzjbbsPHmPDKwfYuPMAG145wIadB/jGIzs4eGzgeJ/5bQ0s65rFsq5ZLOpsorujmUUdzXR3NDG7uc6zWZKkimSY0rjMmdXAj85q4EeXnQhYKSV27D3CxlcO8OzOAzy/6xCbeg7yD4/tYP/RgRHvn9VQy9K5LSzrauGiecXAtWzeLJbMaaG+1tudSZJmLsOUzllE0N3RTHdHMz9+6fwR6/Yd6Wf7nsNs6z2SPR/mhVcP8eDmXv7+sZeO96spBIs7m7Nw1cKyrlnHw1Z7k5cNJUnTn2FKk6K9qY72pnYuO7/9pHWHjg3wQs8hNvUUz2Y933OQ53sOcu/GXfQPnvhCRFdrA8u6Wo5fOrxgTjMXzCmGN2/hIEmaLgxTmnItDbVc0d3OFd0jg9bA4BDb9hzh+V0H2dRzkOd3FUPWNx9/6aTLhgvaGlk8p5nl82Zx5eIOrlw8mwvntjgvS5I05bw1gqa9lBK9h/rY0nuYrbsPs7X3MFt2H2Zr7yGeffkAB7JJ8HNnNXDl4tm8trudld2zuXRBK3NmNVBTMGBJkibGWyNoRosI5sxqYM6sBq5a3DFi3dBQ4vmegzy8ZQ8Pbu7lsW17+c7TrxxfX1MI5rTU09lST1tTHe1NdbQ11mWXIetob6qlvfnk9ramOi8lSpLGxTClGa1QCJbPb2X5/FZuWr0YKE5+X79jH8/3HKTnwDF27T/GnsN97DvSz7bew+w/0s/+owMjbuswlobawvEAVvpoa6yltbGOxroCDbU1xee6GhrramioLdBYV0Nj9txQV6CxduS6htoCBc+WSVLFMEyp4rQ31XHtRXO59qK5p+03MDjE/qMD7DvSf9Jjf/YobXtl/1E2vnKAfUf6OXhsgIlcIa+vLZwIXlngKg1ejXU1NNXX0Jw9N9bVUF9boL4mqKspUFtz4nVdTYG62lHLNQXqa7O+hROvj6+rKVB3fH0410ySJsAwpapVW1OgM7sEeLZSSvQPJo4ODHKsf4ij/YMcGxjkaP/Q8eej/WMsD/fPnkf2HeTYwBCH+wboPdTHkf5BjvQNFp/7B+kbGJqEUSiqrylQVxPUHg9bQV1tafg6Ecaa62tobqilua6G5oaa4nJ9bfH9hRPbqS0E9bXFMFdbE8fX19acCHHFYJhtO1tXWwhqRj0KcaLd4CdpuhlXmIqIG4A/AWqAz6SU/veo9Q3AF4HXA7uBd6eUXixvqdL0ERHU1xbDAo1Ts8+UEoNDxRDXNzhE//BjYNTy4BD9g+n4676BNGJd32Cif2DU8uDQ8bbh5YFsO6XbPtY/xM79/RzuG+Rw3wCHjw1yqG+AoSn8HksE1JYErEIWsobbRgexmhg7mBUKJwLaiG1FUFNTfB6rbaztl25ruC2ieJxESd1B1pa9JltfXI6SPsVlSt8DFAol/SIoZP0KwYj9FbJ9FLINFka1x4j3neH9o5aLz8Do93Ni2yP6F7uOUcPIPsPtlH7+UeOWber4+6Xp4oxhKiJqgNuAtwLbgYciYm1K6emSbh8C9qSULoqIm4BPAe+ejIKlahURxTM3NdDE9Jkcn1Li2MAQA0PpeAAbGCqGvP6hIQaGw9mo9QPHA1/Wf7C4fmAoMZQSA4PZ81AxRA4NpRPrsuXBkrbBoVGPUW2l2xp+9A8OcaQ/29aofQ63DQ4ObwsGh4ZGbH9oCAaGhqY0TOpkpwxcw2l0dNuo9xyPZaO3M2p50j/H6OWTdhunXX+m98dZv//0n/uk95d5f2f4+CMWb3ztQj72luWnKnXSjefM1GpgU0rpBYCIuBNYA5SGqTXA72Svvw78WUREyuu+C5KmTERU/TcfU0oMpSxYlQas7CdgIpFScTGllD0X2znePka/4feXrBs6/v7h5eK6oaGS95f0G0rDbdny0Il2St+fTmzzxPbGeH86UQcjlke9f1QdqWQ7Q6M+4+haS8d1eDxGj0M60en4+J081ifaGLGdsfuctO+S9ZMtMXJHo/c7uoyT6zrD+0/a3sT2N/r9Z1hkdBw48/bP7v3z2xrI03jC1EJgW8nyduANp+qTUhqIiH3AHODV0k4RcQtwC8DixYvPsWRJml4igpqAmsJwqKzucClVmyn9C7MppTtSSqtSSqu6urqmcteSJEmTYjxhagewqGS5O2sbs09E1ALtFCeiS5IkVbTxhKmHgOURsTQi6oGbgLWj+qwF3pe9/mnge86XkiRJ1eCMc6ayOVC3AndTnAjwuZTSUxHxSWBdSmkt8FngryNiE9BLMXBJkiRVvHHdZyqldBdw16i2T5S8Pgr8THlLkyRJmv6mdAK6JElSpTFMSZIkTYBhSpIkaQIiry/dRUQPsGUKdjWXUTcPrWKOxUiOx0iOxwmOxUiOx0iOxwnVNBYXpJTGvElmbmFqqkTEupTSqrzrmA4ci5Ecj5EcjxMci5Ecj5EcjxMciyIv80mSJE2AYUqSJGkCqiFM3ZF3AdOIYzGS4zGS43GCYzGS4zGS43GCY0EVzJmSJEmaTNVwZkqSJGnSGKYkSZImoGLDVETcEBEbImJTRHw873qmQkQsioh7IuLpiHgqIj6WtXdGxHci4rnsuSNrj4j402yMnoiIq/L9BOUXETUR8WhE/GO2vDQiHsg+81cjoj5rb8iWN2Xrl+Ra+CSIiNkR8fWIeDYinomIH6nyY+NXs38n6yPiKxHRWE3HR0R8LiJ2RcT6krazPh4i4n1Z/+ci4n15fJaJOsVY/EH2b+WJiPi7iJhdsu43s7HYEBFvL2mviN87Y41Hybpfj4gUEXOz5Yo+NsYtpVRxD6AGeB64EKgHHgdW5F3XFHzu84CrstetwEZgBfD7wMez9o8Dn8pevxP4FhDANcADeX+GSRiTXwO+DPxjtvw14Kbs9e3AL2WvPwrcnr2+Cfhq3rVPwlh8Afhw9roemF2txwawENgMNJUcF++vpuMDeCNwFbC+pO2sjgegE3ghe+7IXnfk/dnKNBZvA2qz158qGYsV2e+UBmBp9rumppJ+74w1Hln7IuBuijfcnlsNx8Z4H5V6Zmo1sCml9EJKqQ+4E1iTc02TLqX0ckrpkez1AeAZir801lD8RUr2/JPZ6zXAF1PR/cDsiDhvaquePBHRDfwE8JlsOYAfB76edRk9FsNj9HXgzVn/ihAR7RR/QH4WIKXUl1LaS5UeG5laoCkiaoFm4GWq6PhIKd0H9I5qPtvj4e3Ad1JKvSmlPcB3gBsmvfgyG2ssUkr/nFIayBbvB7qz12uAO1NKx1JKm4FNFH/nVMzvnVMcGwB/DPwGUPrNtYo+NsarUsPUQmBbyfL2rK1qZJchrgQeAOanlF7OVu0E5mevK32c/i/Ff/hD2fIcYG/JD8jSz3t8LLL1+7L+lWIp0AP8VXbZ8zMR0UKVHhsppR3AHwJbKYaofcDDVO/xMexsj4eKPk5KfJDi2Reo0rGIiDXAjpTS46NWVeV4jFapYaqqRcQs4G+BX0kp7S9dl4rnXyv+fhgR8S5gV0rp4bxrmSZqKZ62/3RK6UrgEMXLOMdVy7EBkM0FWkMxZJ4PtFDB/9d8LqrpeDidiPgtYAD4Ut615CUimoH/Bnwi71qmq0oNUzsoXtsd1p21VbyIqKMYpL6UUvpG1vzK8CWa7HlX1l7J43QtcGNEvEjxdPuPA39C8RR0bdan9PMeH4tsfTuweyoLnmTbge0ppQey5a9TDFfVeGwAvAXYnFLqSSn1A9+geMxU6/Ex7GyPh4o+TiLi/cC7gPdk4RKqcyyWUfwfj8ezn6ndwCMRsYDqHI+TVGqYeghYnn0zp57ihNG1Odc06bI5HJ8Fnkkp/VHJqrXA8Dcp3gf8Q0n7z2ffxrgG2Fdyin9GSyn9ZkqpO6W0hOJ//++llN4D3AP8dNZt9FgMj9FPZ/0r5v/KU0o7gW0RcUnW9Gbgaarw2MhsBa6JiObs383weFTl8VHibI+Hu4G3RURHdrbvbVnbjBcRN1CcJnBjSulwyaq1wE3ZNzyXAsuBB6ng3zsppSdTSvNSSkuyn6nbKX7ZaSdVeGyMKe8Z8JP1oPgNg40Uv13xW3nXM0Wf+TqKp+WfAB7LHu+kOLfju8BzwL8AnVn/AG7LxuhJYFXen2GSxuV6Tnyb70KKP/g2AX8DNGTtjdnypmz9hXnXPQnj8DpgXXZ8/D3Fb9hU7bEB/E/gWWA98NcUv51VNccH8BWK88X6Kf5y/NC5HA8U5xNtyh4fyPtzlXEsNlGc8zP8s/T2kv6/lY3FBuAdJe0V8XtnrPEYtf5FTnybr6KPjfE+/HMykiRJE1Cpl/kkSZKmhGFKkiRpAgxTkiRJE2CYkiRJmgDDlCRJ0gQYpiRJkibAMCVJkjQB/x/mpKrXtDM5CgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test = torch.linspace(0, 8*pi, 10000).reshape(-1,1)\n",
    "t_test = torch.ones((10000,1))\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution(x_test, t_test).reshape(-1,1)\n",
    "\n",
    "my_network = my_network.cpu()\n",
    "u_test_pred = my_network(test).reshape(-1,1)\n",
    "\n",
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((u_test_pred - u_test)**2)/torch.mean(u_test**2)\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")\n",
    "\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.plot(x_test, u_test, label=\"Ground Truth\",lw=2)\n",
    "plt.plot(x_test, u_test_pred.detach(), label=\"Network Prediction\",lw=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"u\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced5acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b5006ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Error Test:  114.07986879348755 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_test = torch.linspace(0, 8*pi, 10000).reshape(-1,1)\n",
    "t_test = torch.ones((10000,1))\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution(x_test, t_test).reshape(-1,1)\n",
    "\n",
    "my_network = my_network.cpu()\n",
    "u_test_pred = my_network(test).reshape(-1,1)\n",
    "\n",
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((u_test_pred - u_test)**2)/torch.mean(u_test**2)\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")\n",
    "\n",
    "#plt.grid(True, which=\"both\", ls=\":\")\n",
    "\n",
    "# Set Times New Roman font for axes labels and ticks\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "font_path = 'times-new-roman.ttf'\n",
    "ticks_font = FontProperties(fname=font_path, size=14)\n",
    "plt.xlabel(\"x\", fontsize=20, fontproperties=ticks_font)\n",
    "plt.ylabel(\"u\", fontsize=20, fontproperties=ticks_font)\n",
    "\n",
    "# Set Times New Roman font for ticks\n",
    "plt.xticks(fontsize=20, fontproperties=ticks_font)\n",
    "plt.yticks(fontsize=20, fontproperties=ticks_font)\n",
    "#plt.plot(x_test, u_test, label=\"Ground Truth\",lw=2)\n",
    "plt.scatter(x_test[::100], u_test[::100], label=\"Ground Truth\", color='blue', lw=2)\n",
    "plt.plot(x_test, u_test_pred.detach(), label=\"Prediction\",color='red', lw=2)\n",
    "legend_font = FontProperties(family='Times New Roman', style='normal', size=30)\n",
    "\n",
    "#\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.savefig('diff_ic_dotted.pdf', dpi = 500, bbox_inches = \"tight\", format='pdf', backend='cairo')\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(history)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5649c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('input_layer.weight',\n",
       "              tensor([[-2.8867e-01,  2.1617e-01],\n",
       "                      [ 2.1635e-02, -1.7316e-01],\n",
       "                      [ 1.3589e-01, -4.0220e-01],\n",
       "                      [ 3.3085e-01, -3.0147e-01],\n",
       "                      [ 9.4485e-02,  6.7596e-01],\n",
       "                      [ 4.8037e-02,  1.5047e-01],\n",
       "                      [ 1.7420e-02, -1.2656e-02],\n",
       "                      [ 2.6476e-01, -2.8099e-01],\n",
       "                      [-1.6313e-01,  3.2822e-01],\n",
       "                      [ 1.5153e-01,  2.0016e-02],\n",
       "                      [ 3.1241e-01,  1.2560e-01],\n",
       "                      [-1.2882e-03,  1.3924e-02],\n",
       "                      [-2.9341e-02, -1.0869e-01],\n",
       "                      [ 4.2156e-01, -3.3039e-02],\n",
       "                      [-3.7150e-02,  1.6494e-01],\n",
       "                      [-2.8273e-01,  1.9943e-01],\n",
       "                      [-9.0187e-02, -1.0217e-01],\n",
       "                      [ 1.6243e-01, -2.4843e-01],\n",
       "                      [-2.6768e-02,  2.3992e-02],\n",
       "                      [-3.5832e-03, -3.6459e-01],\n",
       "                      [ 4.6498e-01,  1.9718e-01],\n",
       "                      [ 7.2692e-02,  2.9311e-01],\n",
       "                      [-3.1350e-01, -8.2516e-03],\n",
       "                      [-6.3186e-02, -2.4927e-01],\n",
       "                      [-2.8184e-01,  8.1370e-02],\n",
       "                      [ 3.0844e-02, -3.1635e-01],\n",
       "                      [ 3.0929e-02,  2.1060e-01],\n",
       "                      [ 7.5540e-02,  3.3052e-01],\n",
       "                      [-4.0240e-02, -1.2438e-01],\n",
       "                      [-2.0686e-01,  2.6395e-02],\n",
       "                      [ 1.4512e-01, -3.6718e-01],\n",
       "                      [ 2.8406e-01,  1.7301e-01],\n",
       "                      [-1.9123e-02,  3.0728e-01],\n",
       "                      [-2.2202e-02, -6.4514e-03],\n",
       "                      [-1.3281e-01, -7.4291e-02],\n",
       "                      [-2.6175e-01, -1.1558e-01],\n",
       "                      [-3.2532e-02, -2.1378e-01],\n",
       "                      [-5.6794e-02,  6.6944e-02],\n",
       "                      [-1.4606e-01, -2.7259e-01],\n",
       "                      [ 2.7718e-02, -2.4937e-01],\n",
       "                      [ 4.7628e-01,  4.2576e-03],\n",
       "                      [-1.6305e-02,  1.7546e-01],\n",
       "                      [ 2.6238e-02, -5.5113e-01],\n",
       "                      [-3.4372e-02,  5.9709e-02],\n",
       "                      [-9.8802e-02, -6.2963e-02],\n",
       "                      [-1.0513e-01,  1.5829e-01],\n",
       "                      [-3.0646e-01, -1.8775e-01],\n",
       "                      [-2.3739e-01,  1.2387e-01],\n",
       "                      [-1.1416e-01,  1.7131e-01],\n",
       "                      [-5.9335e-02,  4.7420e-03],\n",
       "                      [-3.6005e-02, -2.0754e-01],\n",
       "                      [-9.1216e-02,  1.2741e-01],\n",
       "                      [-1.0939e-01, -6.3282e-01],\n",
       "                      [-9.5100e-02, -3.0893e-01],\n",
       "                      [ 3.3333e-01, -3.4208e-02],\n",
       "                      [ 1.1731e-01,  1.7086e-01],\n",
       "                      [-6.8881e-02, -8.2503e-02],\n",
       "                      [-2.3585e-01, -1.5801e-01],\n",
       "                      [ 1.5790e-02, -3.9694e-01],\n",
       "                      [ 2.5868e-01,  1.1258e-01],\n",
       "                      [-2.8829e-02,  2.9623e-01],\n",
       "                      [ 4.4064e-01,  2.0890e-01],\n",
       "                      [-1.4628e-01, -1.8194e-01],\n",
       "                      [-1.6768e-02, -5.4282e-01],\n",
       "                      [-4.4631e-02, -1.3465e-01],\n",
       "                      [ 5.8991e-02, -4.5756e-01],\n",
       "                      [ 1.1417e-01,  4.9885e-01],\n",
       "                      [-4.1410e-01, -2.0217e-01],\n",
       "                      [-6.1487e-02, -2.3772e-01],\n",
       "                      [ 4.7831e-02,  6.6133e-02],\n",
       "                      [ 4.6623e-01,  2.2866e-01],\n",
       "                      [ 3.2589e-02, -1.7600e-01],\n",
       "                      [ 8.7284e-03,  2.4548e-01],\n",
       "                      [-3.9253e-02,  2.1957e-01],\n",
       "                      [ 2.3322e-01,  1.6981e-01],\n",
       "                      [ 1.0354e-01, -4.3028e-01],\n",
       "                      [ 4.3934e-02,  4.2741e-01],\n",
       "                      [-4.6327e-01, -2.5477e-01],\n",
       "                      [ 2.4621e-02,  1.4408e-01],\n",
       "                      [-1.3146e-01, -1.9079e-01],\n",
       "                      [-9.1755e-02,  6.2319e-01],\n",
       "                      [-4.5664e-03, -2.3589e-02],\n",
       "                      [-9.4396e-03,  3.2891e-01],\n",
       "                      [-7.4041e-03, -3.6519e-01],\n",
       "                      [ 1.4929e-01, -1.6883e-01],\n",
       "                      [ 3.1501e-01,  3.5829e-01],\n",
       "                      [-2.5014e-01, -3.0920e-01],\n",
       "                      [-1.3405e-01, -1.8314e-01],\n",
       "                      [-1.1270e-01,  3.3037e-01],\n",
       "                      [-9.4502e-02,  2.8110e-01],\n",
       "                      [ 7.6105e-02, -3.3334e-01],\n",
       "                      [ 3.2275e-01,  1.1120e-01],\n",
       "                      [-3.6575e-01, -2.2008e-01],\n",
       "                      [-9.6025e-02, -1.8639e-01],\n",
       "                      [-8.3378e-03, -2.9692e-01],\n",
       "                      [ 3.8765e-01,  2.9695e-01],\n",
       "                      [-1.9882e-01, -3.0480e-01],\n",
       "                      [-8.6165e-03,  1.5818e-01],\n",
       "                      [-1.2888e-01,  2.0233e-01],\n",
       "                      [-7.8402e-02,  6.3929e-02],\n",
       "                      [-1.1444e-01, -1.8224e-01],\n",
       "                      [ 1.5261e-01,  1.5353e-01],\n",
       "                      [ 4.0841e-02, -3.0157e-02],\n",
       "                      [-2.6280e-02, -1.8822e-01],\n",
       "                      [-9.4362e-02, -1.7506e-01],\n",
       "                      [ 2.3348e-02, -4.5616e-02],\n",
       "                      [ 2.4157e-01,  2.8048e-01],\n",
       "                      [ 2.4905e-01, -2.1689e-01],\n",
       "                      [-2.3419e-01,  1.2454e-01],\n",
       "                      [ 2.8305e-02, -2.9048e-01],\n",
       "                      [-4.1754e-02,  1.9034e-01],\n",
       "                      [-2.2627e-01, -2.3418e-01],\n",
       "                      [-3.7607e-02,  6.4252e-02],\n",
       "                      [ 4.4075e-02, -2.5236e-01],\n",
       "                      [-4.8613e-02,  3.0980e-01],\n",
       "                      [-3.5833e-02,  9.9740e-02],\n",
       "                      [-1.8814e-02, -1.4784e-01],\n",
       "                      [ 9.9843e-02, -3.1998e-01],\n",
       "                      [ 1.1875e-01, -2.5084e-01],\n",
       "                      [-3.7998e-01, -1.4078e-01],\n",
       "                      [ 2.2392e-02,  3.2690e-01],\n",
       "                      [ 6.4514e-02, -2.5892e-01],\n",
       "                      [-2.2460e-01,  5.0526e-02],\n",
       "                      [ 4.9397e-02, -2.9837e-03],\n",
       "                      [-6.7973e-02,  1.6549e-01],\n",
       "                      [ 4.8300e-01,  2.8938e-01],\n",
       "                      [ 4.5276e-01,  3.3805e-01],\n",
       "                      [-1.7848e-01,  2.4119e-01],\n",
       "                      [-3.1160e-01, -8.3120e-02],\n",
       "                      [ 4.1978e-01,  7.2025e-02],\n",
       "                      [ 2.4269e-02,  4.2806e-02],\n",
       "                      [ 1.6262e-02,  3.3993e-01],\n",
       "                      [-3.9217e-02, -2.1929e-02],\n",
       "                      [-7.2582e-02, -3.6168e-01],\n",
       "                      [ 2.8581e-02,  3.6118e-02],\n",
       "                      [ 2.8575e-01,  2.4000e-01],\n",
       "                      [ 3.2825e-02, -1.2068e-01],\n",
       "                      [ 1.7134e-01,  8.0932e-02],\n",
       "                      [-1.3659e-01, -3.7583e-02],\n",
       "                      [ 1.6367e-01,  1.6337e-01],\n",
       "                      [-2.4537e-01, -2.6936e-01],\n",
       "                      [-1.4744e-01,  6.3424e-03],\n",
       "                      [ 3.8359e-02,  9.5480e-02],\n",
       "                      [-3.7942e-02,  1.2213e-01],\n",
       "                      [-4.2206e-02, -2.2864e-01],\n",
       "                      [ 1.0954e-01, -2.4449e-01],\n",
       "                      [-5.8617e-02, -1.0413e-03],\n",
       "                      [-2.2031e-01, -2.2482e-02],\n",
       "                      [-1.9972e-02, -1.9440e-02],\n",
       "                      [ 8.4749e-02,  3.4601e-02],\n",
       "                      [ 2.8551e-02,  1.1509e-01],\n",
       "                      [ 1.1073e-01, -4.6713e-01],\n",
       "                      [ 1.9196e-02, -3.0658e-01],\n",
       "                      [-1.3548e-01, -2.4312e-01],\n",
       "                      [ 1.9419e-03,  1.5938e-01],\n",
       "                      [-1.5016e-01,  8.5101e-02],\n",
       "                      [-3.9010e-02,  1.3909e-01],\n",
       "                      [ 2.6556e-01,  2.2898e-01],\n",
       "                      [ 3.2736e-01, -5.6556e-02],\n",
       "                      [-2.4682e-02, -2.5014e-01],\n",
       "                      [-1.1585e-01,  1.7662e-01],\n",
       "                      [ 1.3892e-01, -6.5572e-01],\n",
       "                      [ 6.2874e-02, -4.6900e-02],\n",
       "                      [-5.1693e-02, -1.4331e-01],\n",
       "                      [-5.4871e-04,  3.1490e-01],\n",
       "                      [ 8.7286e-02, -7.0365e-02],\n",
       "                      [-2.2731e-01, -1.1048e-02],\n",
       "                      [-2.4272e-02, -4.9468e-01],\n",
       "                      [-1.3934e-01, -3.4035e-01],\n",
       "                      [-4.2900e-02, -3.6755e-01],\n",
       "                      [-3.7508e-02, -2.1726e-01],\n",
       "                      [-2.5034e-01,  2.5950e-01],\n",
       "                      [-3.4280e-01, -1.4179e-02],\n",
       "                      [-4.6941e-02,  2.0036e-02],\n",
       "                      [ 2.3305e-01,  7.5469e-04],\n",
       "                      [ 1.2994e-03, -1.4716e-01],\n",
       "                      [-2.2574e-01,  2.1557e-01],\n",
       "                      [-1.8665e-01,  3.0595e-01],\n",
       "                      [-3.4345e-02,  2.2665e-01],\n",
       "                      [ 8.6815e-02,  2.4025e-01],\n",
       "                      [-3.7667e-02, -3.7456e-01],\n",
       "                      [ 2.2601e-02,  9.9289e-02],\n",
       "                      [-1.2413e-02, -3.3107e-01],\n",
       "                      [-2.6962e-02,  1.7774e-01],\n",
       "                      [ 4.8206e-01,  1.7412e-01],\n",
       "                      [-1.2586e-01,  2.7618e-01],\n",
       "                      [-1.9242e-01,  7.8400e-03],\n",
       "                      [ 1.5393e-01,  2.1164e-01],\n",
       "                      [-1.2238e-01, -2.1279e-01],\n",
       "                      [ 3.0598e-01,  1.6220e-01],\n",
       "                      [-3.2858e-01, -2.3856e-01],\n",
       "                      [-9.6563e-02, -1.4897e-01],\n",
       "                      [-9.4273e-02,  3.9008e-01],\n",
       "                      [-5.4740e-01,  2.2137e-01],\n",
       "                      [-2.1388e-02, -2.0954e-01],\n",
       "                      [-1.4646e-01,  5.1353e-01],\n",
       "                      [ 2.1484e-01,  1.1449e-01],\n",
       "                      [-3.6185e-01, -2.5896e-01],\n",
       "                      [-3.9697e-01, -4.4289e-01],\n",
       "                      [-7.0913e-02, -3.3404e-01]])),\n",
       "             ('input_layer.bias',\n",
       "              tensor([ 1.7865e-01, -1.3582e-01,  1.0910e-01,  1.4094e-02,  1.6340e-01,\n",
       "                      -6.1595e-01, -2.4404e-01,  2.5568e-02,  2.2112e-01,  2.9918e-01,\n",
       "                      -5.7333e-02,  1.4877e-01,  1.8980e-01, -2.5915e-02,  7.0527e-02,\n",
       "                      -1.3364e-01,  1.2773e-01, -1.0795e-01,  8.7504e-02,  2.6548e-01,\n",
       "                       3.5656e-01, -1.4186e-01, -3.3384e-01,  3.0341e-01, -2.4277e-02,\n",
       "                      -1.9546e-01, -2.5239e-01,  9.4159e-02, -8.0113e-03, -2.6509e-01,\n",
       "                      -3.9228e-02,  7.7332e-02,  1.9758e-01,  2.8012e-01,  1.5099e-01,\n",
       "                       3.3415e-01,  3.1721e-01,  3.4243e-01, -1.1051e-02, -2.8050e-01,\n",
       "                       2.0468e-01,  2.5668e-02, -1.5741e-01,  4.5349e-01, -1.3699e-01,\n",
       "                      -4.2639e-02,  7.2242e-02, -1.6164e-01,  3.1212e-01, -9.0125e-05,\n",
       "                       1.9362e-01,  2.4849e-01,  1.0267e-01, -1.9240e-01,  3.5278e-01,\n",
       "                      -3.8800e-02,  3.9612e-01, -1.8817e-01,  4.7500e-02,  7.8041e-02,\n",
       "                       3.6491e-01,  2.7550e-01, -1.5948e-01,  1.1486e-02, -8.9751e-02,\n",
       "                      -3.1276e-02,  2.2311e-01, -9.2226e-02,  1.1458e-01,  5.2599e-03,\n",
       "                       1.7837e-01, -1.7141e-01, -3.7418e-01,  1.1103e-01,  1.5804e-01,\n",
       "                      -8.8020e-02, -3.2649e-01, -1.6711e-01, -3.1972e-01, -4.7763e-02,\n",
       "                      -5.3402e-03,  1.1397e-01,  1.3593e-01, -9.4066e-02,  6.2868e-02,\n",
       "                       2.4105e-01,  5.9373e-03, -2.1994e-01,  1.5597e-01,  2.1739e-02,\n",
       "                      -2.8740e-02,  3.1439e-01, -6.1330e-02,  2.3962e-01,  2.0131e-01,\n",
       "                       2.4778e-01, -1.4108e-01,  1.5528e-01,  5.3251e-02,  7.7225e-02,\n",
       "                      -2.4427e-01,  4.4594e-01, -4.3802e-01, -6.5870e-02,  2.3842e-01,\n",
       "                      -1.0189e-01, -1.4025e-01,  6.0503e-02, -2.2616e-01, -2.8282e-01,\n",
       "                       4.3077e-01, -1.3487e-01,  1.8337e-01, -7.6649e-02, -5.6110e-02,\n",
       "                       2.8684e-01,  1.8145e-01, -4.7870e-02, -7.8137e-02, -3.5739e-02,\n",
       "                       1.1891e-02, -1.6022e-01, -1.1705e-01, -2.1470e-01,  7.5430e-02,\n",
       "                       2.3036e-01,  2.4640e-01, -1.6375e-01, -2.4163e-01,  1.6846e-01,\n",
       "                      -1.7199e-01, -1.0501e-01,  1.9979e-01,  2.2916e-01, -3.5246e-01,\n",
       "                       1.9121e-02, -2.9237e-01, -4.2143e-04, -2.2514e-02,  7.7313e-02,\n",
       "                       5.7393e-02,  3.7209e-02, -4.5615e-02,  4.4217e-01,  3.1279e-01,\n",
       "                      -1.2092e-01, -8.7360e-02, -2.2571e-01,  1.4126e-01, -7.2321e-02,\n",
       "                      -1.0718e-02, -2.4311e-01,  2.1807e-01, -7.5316e-02, -1.6076e-01,\n",
       "                      -1.1726e-01,  3.8473e-01,  2.6947e-01,  4.5999e-02,  2.9634e-01,\n",
       "                       8.2713e-02, -1.1341e-01, -1.9112e-01,  4.1138e-01,  2.4260e-02,\n",
       "                       1.7922e-01, -7.7698e-02, -1.4781e-01, -1.9067e-01, -3.2039e-02,\n",
       "                       1.0850e-01, -1.5416e-01, -1.3515e-01,  1.5715e-01,  1.9230e-01,\n",
       "                       1.9343e-01, -9.8800e-02,  1.0731e-01,  1.3396e-01, -1.1533e-01,\n",
       "                      -2.1377e-02, -1.9465e-01,  2.9517e-01,  1.9366e-01,  2.1007e-01,\n",
       "                      -2.9370e-02,  1.0709e-01, -9.8048e-03, -2.4496e-01,  3.8732e-01,\n",
       "                       1.1967e-01,  1.9349e-01, -1.7347e-01, -1.9724e-01,  2.5820e-01,\n",
       "                       5.1397e-01,  1.2624e-01, -1.6281e-01, -9.8929e-02,  4.2574e-01])),\n",
       "             ('hidden_layers.0.weight',\n",
       "              tensor([[ 0.1265,  0.1227, -0.1929,  ...,  0.1885,  0.0444, -0.1090],\n",
       "                      [ 0.0221, -0.1102, -0.0909,  ...,  0.1240, -0.0052,  0.0170],\n",
       "                      [-0.0672, -0.0348, -0.2703,  ...,  0.1172,  0.0941,  0.0295],\n",
       "                      ...,\n",
       "                      [ 0.1308, -0.0410,  0.0716,  ..., -0.1626,  0.0851, -0.0541],\n",
       "                      [ 0.0997, -0.1588, -0.1841,  ..., -0.0861, -0.0950, -0.1568],\n",
       "                      [ 0.1129,  0.1186,  0.1057,  ..., -0.0216, -0.1126, -0.0824]])),\n",
       "             ('hidden_layers.0.bias',\n",
       "              tensor([-0.0389, -0.0100,  0.0947, -0.0612,  0.1698,  0.0749,  0.0163,  0.0399,\n",
       "                      -0.0089,  0.0881,  0.0534,  0.1441,  0.0526, -0.0513, -0.0234,  0.0628,\n",
       "                       0.1137,  0.1148,  0.0849, -0.0377,  0.0394,  0.1466,  0.0605,  0.1050,\n",
       "                       0.0859,  0.0899, -0.1088, -0.0221, -0.0968, -0.0232, -0.0091, -0.1621,\n",
       "                      -0.0662, -0.0730, -0.0592,  0.0295, -0.0587, -0.0435, -0.0417, -0.1510,\n",
       "                      -0.0234,  0.0055, -0.2798,  0.0245,  0.0736, -0.1010, -0.0052,  0.1130,\n",
       "                       0.2030, -0.0449,  0.0713,  0.0793,  0.1596,  0.0625, -0.0234, -0.1926,\n",
       "                       0.0238,  0.0734,  0.0819, -0.0219,  0.1944, -0.1027,  0.0106,  0.0205,\n",
       "                       0.0722,  0.0969, -0.0494,  0.0183, -0.1068, -0.0794, -0.0326,  0.0495,\n",
       "                      -0.2363, -0.0126,  0.0430,  0.1529,  0.1091,  0.0853, -0.1025, -0.0450,\n",
       "                      -0.1481,  0.1441,  0.2197, -0.0794,  0.1391,  0.1237, -0.0865, -0.1204,\n",
       "                       0.0413, -0.0929, -0.0702, -0.0457,  0.0263, -0.0058,  0.0549,  0.0199,\n",
       "                       0.1167,  0.0811, -0.0427, -0.1881,  0.0299, -0.1833, -0.0232, -0.1121,\n",
       "                      -0.0080, -0.1118,  0.0507,  0.1023,  0.1077,  0.0322, -0.0115, -0.0389,\n",
       "                       0.0500, -0.0470,  0.0847, -0.0704,  0.1194,  0.1049, -0.0252,  0.0799,\n",
       "                      -0.0115, -0.0345, -0.0190, -0.1124, -0.0761, -0.0989, -0.0338,  0.1182,\n",
       "                      -0.2132,  0.0330, -0.2862,  0.0178,  0.0990,  0.1622,  0.1396,  0.0764,\n",
       "                       0.0054,  0.1066, -0.1285,  0.0295, -0.0915, -0.0454,  0.2065, -0.0308,\n",
       "                       0.0567,  0.0235, -0.0309,  0.0830, -0.0837,  0.0034,  0.0159, -0.0154,\n",
       "                      -0.0296, -0.0880, -0.0680,  0.1278,  0.1694, -0.0390,  0.0412, -0.1267,\n",
       "                      -0.0738,  0.0335, -0.0972,  0.1376, -0.0586, -0.0083, -0.1895,  0.0520,\n",
       "                      -0.0740,  0.0251, -0.2985,  0.1414,  0.0213,  0.0290,  0.0456, -0.0845,\n",
       "                      -0.1708,  0.1847,  0.0048, -0.0313,  0.0008,  0.1420, -0.0638,  0.1201,\n",
       "                       0.0301, -0.1779, -0.0060,  0.0171, -0.1405, -0.0261, -0.0153, -0.0396,\n",
       "                      -0.0189, -0.0733, -0.0415, -0.0492, -0.0004, -0.0397, -0.0174,  0.0070])),\n",
       "             ('hidden_layers.1.weight',\n",
       "              tensor([[ 0.0319,  0.1165,  0.1666,  ...,  0.1257, -0.0897,  0.0685],\n",
       "                      [ 0.1030,  0.0224,  0.0698,  ..., -0.0433,  0.1303,  0.1616],\n",
       "                      [-0.1187, -0.1816, -0.0368,  ..., -0.1331,  0.0741,  0.0183],\n",
       "                      ...,\n",
       "                      [-0.0615,  0.1647, -0.0291,  ...,  0.1093, -0.0349,  0.1559],\n",
       "                      [ 0.1800, -0.0951,  0.1104,  ...,  0.1046, -0.1066, -0.0054],\n",
       "                      [ 0.1733, -0.1770,  0.1962,  ...,  0.1441, -0.2002,  0.1210]])),\n",
       "             ('hidden_layers.1.bias',\n",
       "              tensor([-9.2005e-02, -6.8525e-03, -1.3605e-02,  1.3041e-02, -1.1620e-02,\n",
       "                       3.4170e-02,  7.7764e-02, -2.4565e-02,  7.4586e-03,  1.8436e-03,\n",
       "                       1.0111e-01, -3.8147e-02, -1.5146e-02,  1.5505e-02, -4.5265e-02,\n",
       "                      -3.4314e-02, -1.9612e-02, -3.6232e-02,  6.0463e-02, -4.2225e-02,\n",
       "                       9.7863e-02, -5.8740e-02,  3.1256e-02, -2.9566e-02,  7.1166e-03,\n",
       "                       4.4672e-02, -8.4681e-04,  6.9028e-02,  2.2048e-02, -1.0849e-02,\n",
       "                       9.1921e-02,  3.3752e-02,  2.4321e-02,  2.3555e-03,  3.8460e-03,\n",
       "                      -6.9002e-02, -3.5883e-02, -5.8713e-02,  3.7651e-03, -6.8418e-02,\n",
       "                       7.6514e-02,  4.7924e-02, -4.0272e-02,  5.6952e-03,  1.2606e-02,\n",
       "                      -1.5191e-02,  3.9763e-03, -1.0004e-02, -1.1087e-01, -7.1620e-03,\n",
       "                       4.7191e-02,  5.5824e-02,  1.9906e-03, -3.9571e-03,  2.3883e-02,\n",
       "                      -1.1270e-01,  1.0560e-02, -7.6186e-04, -4.6156e-03,  2.6117e-02,\n",
       "                       2.3961e-02,  1.1881e-02, -9.3483e-02, -2.3355e-03, -9.3038e-02,\n",
       "                      -1.5222e-02, -4.3540e-02, -1.8476e-02,  3.8773e-02, -4.9075e-02,\n",
       "                       1.8162e-02, -4.3883e-02,  6.7645e-02, -8.3244e-04,  7.8041e-02,\n",
       "                      -1.4054e-02,  1.1240e-03, -1.1071e-02, -7.0124e-02, -2.4229e-02,\n",
       "                      -4.0805e-02,  1.6779e-04, -1.3788e-02, -4.1798e-02, -1.1476e-01,\n",
       "                      -5.1049e-02,  5.7818e-02, -3.6208e-02,  4.8388e-02,  9.5092e-02,\n",
       "                      -1.1649e-02, -7.8919e-02, -3.4285e-04,  4.9565e-03,  2.8996e-02,\n",
       "                       1.2340e-01, -1.6707e-02, -1.1830e-02,  1.9981e-02, -5.7411e-02,\n",
       "                       3.8017e-02, -7.6480e-02, -1.1388e-03, -3.9003e-02, -5.3492e-02,\n",
       "                       8.5682e-02,  6.2694e-02,  6.4785e-03,  4.8055e-02, -9.2648e-03,\n",
       "                       9.3060e-03, -1.1842e-02,  2.0468e-02,  5.2330e-02, -1.0923e-02,\n",
       "                       4.5515e-02, -1.6362e-02,  4.3036e-02,  4.8938e-02,  3.0985e-03,\n",
       "                       4.2580e-02, -2.0130e-02,  8.3514e-02,  5.3974e-02, -3.1842e-02,\n",
       "                       3.3168e-02, -1.5112e-02,  1.3681e-02, -1.0684e-01, -1.9587e-02,\n",
       "                      -5.5738e-02, -1.0026e-01, -5.7387e-02, -8.8272e-02, -1.6689e-02,\n",
       "                      -8.4638e-02,  1.2481e-01,  3.8259e-03,  1.2582e-02,  6.1121e-03,\n",
       "                      -5.3541e-02, -5.6621e-02, -2.0887e-02,  4.1111e-02,  2.0444e-02,\n",
       "                       1.7083e-01,  1.8408e-02,  1.8268e-02, -3.5137e-02,  2.2455e-02,\n",
       "                      -2.4932e-02,  5.8843e-02, -3.9862e-03,  2.6537e-02, -2.1278e-02,\n",
       "                       1.8729e-02, -2.0552e-02, -1.0746e-02, -6.5146e-02,  1.0228e-01,\n",
       "                       3.1373e-02,  9.9140e-02, -4.3340e-02,  3.3327e-02, -2.6557e-02,\n",
       "                       3.6975e-02, -6.7320e-02,  4.9787e-02,  6.7457e-02,  2.2444e-02,\n",
       "                       9.5716e-04, -1.5590e-02, -3.8374e-02, -1.9070e-02,  2.0167e-02,\n",
       "                      -2.3330e-02, -5.6446e-02, -1.2366e-02,  1.2389e-02, -4.2932e-02,\n",
       "                       2.9304e-02,  3.0664e-02, -8.9437e-02, -3.9482e-02, -5.3686e-02,\n",
       "                       3.0849e-02,  1.1812e-02, -3.3165e-02, -2.0127e-02,  1.9985e-03,\n",
       "                      -6.3346e-02, -2.8954e-02, -5.1022e-02, -6.9843e-02,  6.9846e-03,\n",
       "                       1.9541e-02,  4.1417e-02, -2.9363e-02, -4.5334e-02,  3.5026e-02])),\n",
       "             ('hidden_layers.2.weight',\n",
       "              tensor([[ 0.1648,  0.0056, -0.2074,  ...,  0.0556,  0.1134,  0.2036],\n",
       "                      [ 0.1931, -0.0179, -0.0217,  ...,  0.0770,  0.1302, -0.1741],\n",
       "                      [ 0.0267,  0.1492,  0.1055,  ..., -0.0870,  0.0670,  0.1784],\n",
       "                      ...,\n",
       "                      [-0.1676,  0.1195,  0.0958,  ..., -0.0332, -0.1921,  0.1934],\n",
       "                      [ 0.0472,  0.1726,  0.1525,  ..., -0.1397,  0.0080, -0.1714],\n",
       "                      [ 0.0948, -0.0720, -0.0990,  ...,  0.1799, -0.0042, -0.0530]])),\n",
       "             ('hidden_layers.2.bias',\n",
       "              tensor([-0.0020,  0.0323, -0.0133, -0.0236, -0.0267, -0.0174,  0.0832, -0.0855,\n",
       "                       0.0561, -0.0072, -0.0151,  0.0017, -0.0403,  0.0121, -0.0647,  0.0305,\n",
       "                       0.0014, -0.0184, -0.0440, -0.0276,  0.0034,  0.0295,  0.0045,  0.0321,\n",
       "                      -0.0198,  0.0363,  0.0343,  0.0111,  0.0285, -0.0039,  0.0086, -0.0561,\n",
       "                       0.0463, -0.0007, -0.0058,  0.0312, -0.0015, -0.0084, -0.0507,  0.0192,\n",
       "                       0.0176,  0.0221, -0.0176,  0.0790, -0.0007,  0.0316,  0.0314, -0.0136,\n",
       "                      -0.0116, -0.0115, -0.0454,  0.0009, -0.0137,  0.0388,  0.0042,  0.0054,\n",
       "                       0.0368,  0.0032, -0.0151, -0.0187, -0.0306, -0.0027,  0.0071, -0.0004,\n",
       "                      -0.0443,  0.0145,  0.0525, -0.0312,  0.0104,  0.0128,  0.0085, -0.0053,\n",
       "                       0.0837,  0.0446,  0.0462,  0.0014, -0.0151, -0.0074, -0.0080,  0.0326,\n",
       "                       0.0163, -0.0045,  0.0050,  0.0098, -0.0525,  0.0221, -0.0609, -0.0674,\n",
       "                      -0.0036, -0.0307, -0.0235, -0.0005, -0.0328, -0.0704, -0.0303, -0.0163,\n",
       "                      -0.0076, -0.0275,  0.0306, -0.0366, -0.0352, -0.0513,  0.0103,  0.0114,\n",
       "                      -0.0042, -0.0333,  0.0088,  0.0317,  0.0219, -0.0228,  0.0024, -0.0901,\n",
       "                       0.0147,  0.0070,  0.0113, -0.0590, -0.0504, -0.0181,  0.0083,  0.0367,\n",
       "                      -0.0069,  0.0020, -0.0775,  0.0166,  0.0207, -0.0017, -0.0351, -0.0080,\n",
       "                       0.0090, -0.0020,  0.0191,  0.0383, -0.0469,  0.0096,  0.0202, -0.0162,\n",
       "                      -0.0312,  0.0373, -0.0019, -0.0217, -0.0052,  0.0142, -0.0065, -0.0103,\n",
       "                      -0.0253,  0.0112,  0.0065,  0.0090,  0.0283, -0.0087,  0.0048,  0.0169,\n",
       "                      -0.0297, -0.0458, -0.0008, -0.0062,  0.0364, -0.0001,  0.0052,  0.0064,\n",
       "                       0.0234,  0.0277, -0.0409, -0.0101,  0.0625, -0.0148,  0.0085,  0.0318,\n",
       "                       0.0062,  0.0562,  0.0075, -0.0368, -0.0015,  0.0061, -0.0224,  0.0150,\n",
       "                       0.0067, -0.0260, -0.0129,  0.0152, -0.0182,  0.0231,  0.0426,  0.0458,\n",
       "                      -0.0077,  0.0496,  0.0427, -0.0230,  0.0241, -0.0312,  0.0283,  0.0038,\n",
       "                       0.0140, -0.0047, -0.0134, -0.0056, -0.0295,  0.0298,  0.0049, -0.0480])),\n",
       "             ('hidden_layers.3.weight',\n",
       "              tensor([[-0.0019,  0.0119, -0.0400,  ...,  0.0454, -0.0304,  0.0182],\n",
       "                      [ 0.0409, -0.0643, -0.0169,  ...,  0.0127, -0.0460,  0.0447],\n",
       "                      [ 0.0035,  0.0143,  0.0246,  ..., -0.0341,  0.0113,  0.0226],\n",
       "                      ...,\n",
       "                      [-0.0472, -0.0190, -0.0185,  ...,  0.0615, -0.0640, -0.0020],\n",
       "                      [ 0.0476,  0.0050,  0.0323,  ...,  0.0044,  0.0432,  0.0384],\n",
       "                      [ 0.0508,  0.0573, -0.0656,  ...,  0.0689,  0.0126,  0.0396]])),\n",
       "             ('hidden_layers.3.bias',\n",
       "              tensor([ 0.0335, -0.0268,  0.0275,  0.0176,  0.0507,  0.0651,  0.0631, -0.0107,\n",
       "                      -0.0025, -0.0019,  0.0565,  0.0315, -0.0243,  0.0445, -0.0549,  0.0180,\n",
       "                       0.0157,  0.0520, -0.0170, -0.0415, -0.0206, -0.0680, -0.0101, -0.0010,\n",
       "                       0.0078,  0.0519,  0.0661, -0.0293, -0.0554,  0.0517,  0.0082, -0.0335,\n",
       "                      -0.0491, -0.0372,  0.0374,  0.0596, -0.0042,  0.0531,  0.0531, -0.0291,\n",
       "                      -0.0244, -0.0379, -0.0277,  0.0620,  0.0492, -0.0140, -0.0203,  0.0564,\n",
       "                       0.0402,  0.0163, -0.0495, -0.0591, -0.0423, -0.0027,  0.0425, -0.0139,\n",
       "                      -0.0227, -0.0533, -0.0185,  0.0584, -0.0597,  0.0437, -0.0351, -0.0240,\n",
       "                       0.0445,  0.0506, -0.0143, -0.0076,  0.0705, -0.0553,  0.0532,  0.0343,\n",
       "                      -0.0561, -0.0226, -0.0388, -0.0621,  0.0698,  0.0607, -0.0344, -0.0614,\n",
       "                      -0.0661, -0.0009,  0.0661, -0.0442,  0.0097,  0.0034, -0.0229,  0.0567,\n",
       "                      -0.0310, -0.0522,  0.0705,  0.0274, -0.0182,  0.0326,  0.0007, -0.0465,\n",
       "                      -0.0145, -0.0499, -0.0670, -0.0334, -0.0236,  0.0580, -0.0265,  0.0461,\n",
       "                       0.0631,  0.0053,  0.0110,  0.0484,  0.0149, -0.0217, -0.0009,  0.0389,\n",
       "                      -0.0399,  0.0086,  0.0452, -0.0529,  0.0031, -0.0065, -0.0264, -0.0410,\n",
       "                      -0.0007,  0.0213,  0.0621,  0.0451,  0.0084,  0.0278,  0.0549, -0.0271,\n",
       "                       0.0352,  0.0687,  0.0275,  0.0384,  0.0581,  0.0548,  0.0038,  0.0391,\n",
       "                       0.0337, -0.0470, -0.0356, -0.0170,  0.0405, -0.0493,  0.0125, -0.0375,\n",
       "                      -0.0220,  0.0159, -0.0293,  0.0019, -0.0145, -0.0700, -0.0447,  0.0696,\n",
       "                       0.0425, -0.0549, -0.0693,  0.0571,  0.0627, -0.0671, -0.0283,  0.0622,\n",
       "                       0.0181,  0.0628,  0.0055,  0.0074,  0.0463, -0.0043,  0.0264, -0.0107,\n",
       "                      -0.0139,  0.0593,  0.0079,  0.0432,  0.0238,  0.0278, -0.0266,  0.0284,\n",
       "                      -0.0274,  0.0473, -0.0578, -0.0702, -0.0118, -0.0318,  0.0227, -0.0042,\n",
       "                      -0.0190,  0.0251,  0.0281, -0.0469, -0.0546,  0.0273,  0.0318, -0.0334,\n",
       "                       0.0044,  0.0543,  0.0028, -0.0324, -0.0088,  0.0526,  0.0006,  0.0510])),\n",
       "             ('output_layer.weight',\n",
       "              tensor([[ -0.8813,   0.3854,   0.0719,  -4.4318,   6.0536,  -1.2087,  -3.0267,\n",
       "                         8.6752,   3.1029,   3.5936,  -1.2051,  -8.1435,   1.1297,   6.1170,\n",
       "                        -3.1720,  -5.9728,   1.5559,  -6.3940,  -6.1335,  -0.8479,   1.5928,\n",
       "                       -13.1322,   0.3618,  -0.4097,   2.7645,   6.2629,  -2.6803,   4.3756,\n",
       "                         3.0034,  -6.6996,  -5.8668,   2.8764,   3.8923,  -7.4528,   6.0249,\n",
       "                         4.4978,  -0.8020,  -0.8095,  -1.9385,  -0.2793,   3.4401,  -3.7285,\n",
       "                        -5.9046,  -7.3886,  -2.4787,   1.5607,  -2.2902,  -3.8916,   2.9739,\n",
       "                        10.0161,   0.9365,  -2.2155,  -3.7167,   4.1126,  -4.6167,  -2.9992,\n",
       "                         3.8511,   1.1012,   3.1861,   0.3334,  -3.8860,   1.8883,   1.5514,\n",
       "                        -2.1237,  -4.5566,   1.0864,  -6.5170,  -8.4387,  -3.4277,  -1.9137,\n",
       "                         6.4998,  -1.6114,   8.7309,   5.9778,  -8.1116,  -3.8131,   5.8100,\n",
       "                        -1.7654,   1.3565,   5.7299,   1.4990,  -2.4497,  -4.0149, -10.7905,\n",
       "                         7.1007, -12.1822,  -2.0700,  -2.2333,  -0.5977,   0.8522,  -3.5559,\n",
       "                        -4.1308,  -4.4224,  -2.1017,  -1.8988,  -4.8700,   9.4490,   4.5772,\n",
       "                        -2.0640,   2.4980,  -4.2666,  -4.7674,  10.8915,  -6.9384,  -0.2286,\n",
       "                        -7.6933,   3.6979,  -3.3713,  -2.6319,   1.7476,   0.9801,  -0.9818,\n",
       "                        -1.5987,  -4.7813,   1.5468,  -5.2626,  -0.4497,   0.5446,   0.3912,\n",
       "                         0.2915,  -2.5856,  -4.4469,  -5.0638,  -3.8683,   4.6040,  -4.0260,\n",
       "                        -0.4439,   5.0752,   6.7984,   1.4932,  -0.7118,  -1.1762,   7.0852,\n",
       "                         8.3275,   1.9998,  -5.7047,   7.2751,  -8.0536,   6.9424,  -0.9715,\n",
       "                        -0.7511,   1.1348,   4.9170,   1.4300,   3.3334,   4.8192,  -4.5220,\n",
       "                        -1.5386,  -8.5321,   7.1155,  -1.0296,  -9.5689,  -1.8065,   5.5996,\n",
       "                        11.2956, -13.5192,  -3.6245,  -1.4108,  -2.0263,   1.7924,   2.1004,\n",
       "                        -2.7658,  -5.3513,   1.8583,  -0.9125,   6.6776,  -4.7580,   0.2853,\n",
       "                        -7.4718,  -3.7864,  -2.5332,  -6.1379,  -8.3988,   3.2005,  -0.6229,\n",
       "                         0.3300,  -3.8256,  -0.5218,  -1.0569,  -0.0750,  -3.9268,   5.4177,\n",
       "                         7.9966,   8.0758,  -3.4201,  -7.8940,  -2.7099,   3.2714,   6.6719,\n",
       "                        -0.9283,   5.7710,   3.3382,   1.4849,  -3.3640,   2.3579,  10.2917,\n",
       "                        -2.3069,   0.7094,  -4.3867,  -2.0991]])),\n",
       "             ('output_layer.bias', tensor([-1.7581]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37806fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Error Test:  114.07986879348755 %\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.linspace(0, 8*pi, 10000).reshape(-1,1)\n",
    "t_test = torch.ones((10000,1))\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution(x_test, t_test).reshape(-1,1)\n",
    "\n",
    "my_network = my_network.cpu()\n",
    "u_test_pred = my_network(test).reshape(-1,1)\n",
    "\n",
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((u_test_pred - u_test)**2)/torch.mean(u_test**2)\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "026556dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Error Test:  114.07986879348755 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Generate data\n",
    "x_test = torch.linspace(0, 8 * np.pi, 10000).reshape(-1, 1)\n",
    "t_test = torch.ones((10000, 1))\n",
    "test = torch.cat([x_test, t_test], 1)\n",
    "u_test = exact_solution(x_test, t_test).reshape(-1, 1)\n",
    "\n",
    "my_network = my_network.cpu()\n",
    "u_test_pred = my_network(test).reshape(-1, 1)\n",
    "\n",
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((u_test_pred - u_test) ** 2) / torch.mean(u_test ** 2)\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")\n",
    "\n",
    "# Create a plot with a white background\n",
    "plt.figure(figsize=(10, 6), facecolor='white')\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "\n",
    "# Create the plot for Ground Truth\n",
    "plt.scatter(x_test[::200], u_test[::200], label=\"Ground Truth\", color='blue', lw=2)\n",
    "\n",
    "# Create the plot for Network Prediction using dots\n",
    "plt.plot(x_test, u_test_pred.detach(), 'r', label=\"Network Prediction\")\n",
    "\n",
    "# Set Times New Roman font for axes labels and ticks\n",
    "font_path = 'times-new-roman.ttf'\n",
    "ticks_font = FontProperties(fname=font_path, size=14)\n",
    "plt.xlabel(\"x\", fontsize=14, fontproperties=ticks_font)\n",
    "plt.ylabel(\"u\", fontsize=14, fontproperties=ticks_font)\n",
    "\n",
    "# Set Times New Roman font for ticks\n",
    "plt.xticks(fontsize=14, fontproperties=ticks_font)\n",
    "plt.yticks(fontsize=14, fontproperties=ticks_font)\n",
    "\n",
    "# Labels and legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f1beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
